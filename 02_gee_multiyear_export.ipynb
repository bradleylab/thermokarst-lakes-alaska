{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Year Lake Ice Phenology Data Export\n",
    "## Part 2: GEE and xarray Processing and Export (2019-2021)\n",
    "\n",
    "**Goal:** Export S1 + S2 + ERA5 data for North Slope lakes across 3 years\n",
    "\n",
    "**Strategy:**\n",
    "- Process chunks independently (spatial parallelization)\n",
    "- Export one year at a time per chunk\n",
    "- Use efficient spatial filtering (only process S2 images that overlap lakes)\n",
    "- Total exports: ~21 chunks × 3 years x 2 datasets (S1, S2) = ~126 exports\n",
    "- Export ERA5 via xarray (more efficient)\n",
    "\n",
    "**Data sources:**\n",
    "- Sentinel-1 GRD (SAR)\n",
    "- Sentinel-2 SR Harmonized (optical, for NDSI)\n",
    "- ERA5-Land (temperature)\n",
    "\n",
    "**Years:** 2019, 2020, 2021 (match ALPOD temporal coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n",
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n",
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n",
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n",
      "Earth Engine initialized: GEE Initialized\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import xarray as xr\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Initialize Earth Engine\n",
    "ee.Initialize()\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"Earth Engine initialized: {ee.String('GEE Initialized').getInfo()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Years: [2019, 2020, 2021]\n",
      "  Chunks: 21\n",
      "  Output: gs://wustl-eeps-geospatial/thermokarst_lakes/exports\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BUCKET = 'wustl-eeps-geospatial'\n",
    "BASE_PATH = 'thermokarst_lakes'\n",
    "CHUNKS_PATH = f'gs://{BUCKET}/{BASE_PATH}/processed/chunks'\n",
    "OUTPUT_PATH = f'{BASE_PATH}/exports'\n",
    "YEARS = [2019, 2020, 2021]\n",
    "n_chunks = 21\n",
    "\n",
    "# S2 thresholds\n",
    "S2_CLOUD_THRESHOLD = 30\n",
    "S2_CLOUD_PROB_THRESHOLD = 40\n",
    "S2_NDSI_THRESHOLD = 0.4\n",
    "\n",
    "# Buffer distances (meters)\n",
    "INTERIOR_BUFFER = -10  # negative = inward\n",
    "LANDSCAPE_BUFFER = 100  # outward\n",
    "\n",
    "# Configuration for GEE assets\n",
    "GEE_ASSET_PATH = 'projects/eeps-geospatial/assets/lake_geometries'\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Years: {YEARS}\")\n",
    "print(f\"  Chunks: {n_chunks}\")\n",
    "print(f\"  Output: gs://{BUCKET}/{OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload ALPOD data as GEE Asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping ALPOD upload (already done)\n",
      "Asset location: projects/eeps-geospatial/assets/ALPOD_full\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# UPLOAD ALPOD TO GEE ASSET (ONE-TIME ONLY)\n",
    "# =============================================================================\n",
    "# Set to False after asset is uploaded\n",
    "UPLOAD_ALPOD_ASSET = False\n",
    "\n",
    "if UPLOAD_ALPOD_ASSET:\n",
    "    print(\"Uploading ALPOD to GEE Asset...\")\n",
    "    print(\"(Direct from GCS )\\n\")\n",
    "    \n",
    "    !/opt/conda/bin/earthengine upload table \\\n",
    "        --asset_id=projects/eeps-geospatial/assets/ALPOD_full \\\n",
    "        gs://wustl-eeps-geospatial/thermokarst_lakes/ALPODlakes/ALPODlakes.shp\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Check progress: https://code.earthengine.google.com/tasks\")\n",
    "    print(\"Look for an 'Ingestion' task\")\n",
    "    print(\"\\n After upload completes, set UPLOAD_ALPOD_ASSET = False\")\n",
    "else:\n",
    "    print(\"Skipping ALPOD upload (already done)\")\n",
    "    print(\"Asset location: projects/eeps-geospatial/assets/ALPOD_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEE_ASSET_PATH = 'projects/eeps-geospatial/assets/lake_geometries'\n",
    "\n",
    "def load_chunk_from_bucket(chunk_id):\n",
    "    \"\"\"\n",
    "    Load pre-computed interior and landscape geometries from GEE Assets.\n",
    "    No buffer computation needed - already done.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load pre-computed geometry assets (fast!)\n",
    "    interior_fc = ee.FeatureCollection(f'{GEE_ASSET_PATH}/chunk_{chunk_id:02d}_interior')\n",
    "    landscape_fc = ee.FeatureCollection(f'{GEE_ASSET_PATH}/chunk_{chunk_id:02d}_landscape')\n",
    "    \n",
    "    # Get bounds from interior\n",
    "    bounds = interior_fc.geometry().bounds()\n",
    "    \n",
    "    # Load GeoDataFrame for metadata\n",
    "    chunk_file = f'{CHUNKS_PATH}/chunk_{chunk_id:02d}.geojson'\n",
    "    chunk_gdf = gpd.read_file(chunk_file)\n",
    "    \n",
    "    return interior_fc, landscape_fc, bounds, chunk_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lake_geometry_metrics(lakes_fc, region_bounds):\n",
    "    \"\"\"\n",
    "    Add lake interior and landscape ring geometries\n",
    "    Uses efficient spatial filtering - only masks lakes within 100m of each target lake\n",
    "    \"\"\"\n",
    "    # Load ALPOD from GEE Asset\n",
    "    all_alpod = ee.FeatureCollection('projects/eeps-geospatial/assets/ALPOD_full')\n",
    "    \n",
    "    def add_geometries(lake):\n",
    "        lake_geom = lake.geometry()\n",
    "        lake_id = lake.get('lake_id')\n",
    "        \n",
    "        # Lake interior: 10m inward buffer\n",
    "        lake_interior = lake_geom.buffer(-10)\n",
    "        \n",
    "        # Landscape ring: 100m outward buffer\n",
    "        ring_outer = lake_geom.buffer(100)\n",
    "        \n",
    "        # Only find lakes that intersect THIS lake's 100m buffer\n",
    "        nearby_lakes = all_alpod.filterBounds(ring_outer)\n",
    "        nearby_dissolved = nearby_lakes.geometry().dissolve(maxError=10)\n",
    "        \n",
    "        # Subtract only the nearby lakes\n",
    "        landscape_ring = ring_outer.difference(nearby_dissolved)\n",
    "        \n",
    "        return lake.set({\n",
    "            'lake_id': lake_id,\n",
    "            'lake_interior': lake_interior,\n",
    "            'landscape_ring': landscape_ring\n",
    "        })\n",
    "    \n",
    "    return lakes_fc.map(add_geometries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sentinel-1 Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentinel1(interior_fc, landscape_fc, year, region_bounds):\n",
    "    \"\"\"\n",
    "    Extract S1 features using pre-computed interior and landscape FCs.\n",
    "    \"\"\"\n",
    "    \n",
    "    s1 = (ee.ImageCollection('COPERNICUS/S1_GRD')\n",
    "          .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "          .filterBounds(region_bounds)\n",
    "          .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
    "          .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
    "          .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n",
    "          .filter(ee.Filter.eq('resolution_meters', 10)))\n",
    "    \n",
    "    def apply_angle_mask(img):\n",
    "        angle = img.select('angle')\n",
    "        angle_mask = angle.gt(25).And(angle.lt(50))\n",
    "        return img.select(['VV', 'VH']).updateMask(angle_mask).copyProperties(img, img.propertyNames())\n",
    "    \n",
    "    s1 = s1.map(apply_angle_mask)\n",
    "    \n",
    "    def extract_s1_features(img):\n",
    "        vv_img = img.select('VV')\n",
    "        vh_img = img.select('VH')\n",
    "        vv_vh_img = vv_img.subtract(vh_img).rename('VV_VH')\n",
    "        r_band = vv_img.unitScale(-20, -5).multiply(255).rename('R')\n",
    "        g_band = vh_img.unitScale(-28, -12).multiply(255).rename('G')\n",
    "        b_band = vv_vh_img.unitScale(8, 18).multiply(255).rename('B')\n",
    "        all_bands = vv_img.addBands(vh_img).addBands(vv_vh_img).addBands(r_band).addBands(g_band).addBands(b_band)\n",
    "        \n",
    "        date = img.date()\n",
    "        orbit = img.get('orbitProperties_pass')\n",
    "        \n",
    "        # Sample both geometries\n",
    "        lake_samples = all_bands.reduceRegions(\n",
    "            collection=interior_fc,\n",
    "            reducer=ee.Reducer.mean(),\n",
    "            scale=10\n",
    "        )\n",
    "        \n",
    "        land_samples = all_bands.reduceRegions(\n",
    "            collection=landscape_fc,\n",
    "            reducer=ee.Reducer.mean(),\n",
    "            scale=10\n",
    "        )\n",
    "        \n",
    "        # Join lake and landscape samples by lake_id\n",
    "        join_filter = ee.Filter.equals(leftField='lake_id', rightField='lake_id')\n",
    "        joined = ee.Join.inner('lake', 'land').apply(lake_samples, land_samples, join_filter)\n",
    "        \n",
    "        def create_s1_feature(joined_feat):\n",
    "            lake_feat = ee.Feature(joined_feat.get('lake'))\n",
    "            land_feat = ee.Feature(joined_feat.get('land'))\n",
    "            \n",
    "            return ee.Feature(None, {\n",
    "                'lake_id': lake_feat.get('lake_id'),\n",
    "                's1_date': date.format('YYYY-MM-dd'),\n",
    "                's1_doy': date.getRelative('day', 'year'),\n",
    "                's1_orbit': orbit,\n",
    "                'lake_vv_db': lake_feat.get('VV'),\n",
    "                'lake_vh_db': lake_feat.get('VH'),\n",
    "                'lake_vv_vh_db': lake_feat.get('VV_VH'),\n",
    "                'lake_r': lake_feat.get('R'),\n",
    "                'lake_g': lake_feat.get('G'),\n",
    "                'lake_b': lake_feat.get('B'),\n",
    "                'land_vv_db': land_feat.get('VV'),\n",
    "                'land_vh_db': land_feat.get('VH'),\n",
    "                'land_vv_vh_db': land_feat.get('VV_VH'),\n",
    "                'land_r': land_feat.get('R'),\n",
    "                'land_g': land_feat.get('G'),\n",
    "                'land_b': land_feat.get('B')\n",
    "            })\n",
    "        \n",
    "        return joined.map(create_s1_feature)\n",
    "    \n",
    "    s1_features = s1.map(extract_s1_features).flatten()\n",
    "    s1_features = s1_features.filter(ee.Filter.notNull(['lake_vv_db']))\n",
    "    \n",
    "    return s1_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sentinel-2 Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ndsi(img):\n",
    "    \"\"\"\n",
    "    Compute Normalized Difference Snow Index (NDSI)\n",
    "    NDSI = (Green - SWIR1) / (Green + SWIR1)\n",
    "    \"\"\"\n",
    "    green = img.select('B3')\n",
    "    swir1 = img.select('B11')\n",
    "    \n",
    "    ndsi = green.subtract(swir1).divide(green.add(swir1)).rename('ndsi')\n",
    "    \n",
    "    return img.addBands(ndsi)\n",
    "\n",
    "def mask_s2_clouds(img):\n",
    "    \"\"\"\n",
    "    Mask clouds using QA60 band (basic cloud mask)\n",
    "    \"\"\"\n",
    "    qa = img.select('QA60')\n",
    "    \n",
    "    # Bits 10 and 11 are clouds and cirrus\n",
    "    cloud_bit_mask = 1 << 10\n",
    "    cirrus_bit_mask = 1 << 11\n",
    "    \n",
    "    # Both should be zero (clear conditions)\n",
    "    mask = qa.bitwiseAnd(cloud_bit_mask).eq(0).And(\n",
    "           qa.bitwiseAnd(cirrus_bit_mask).eq(0))\n",
    "    \n",
    "    return img.updateMask(mask)\n",
    "\n",
    "def add_s2cloudless_mask(img):\n",
    "    s2_cloudless = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "    cloud_prob_collection = s2_cloudless.filter(\n",
    "        ee.Filter.eq('system:index', img.get('system:index'))\n",
    "    )\n",
    "    \n",
    "    has_cloud_data = cloud_prob_collection.size().gt(0)\n",
    "    \n",
    "    def apply_s2cloudless_mask():\n",
    "        cloud_prob = cloud_prob_collection.first().select('probability')\n",
    "        is_clear = cloud_prob.lt(S2_CLOUD_PROB_THRESHOLD)\n",
    "        return img.updateMask(is_clear)\n",
    "    \n",
    "    def use_qa60_only():\n",
    "        return img  # Already has QA60 mask\n",
    "    \n",
    "    return ee.Image(ee.Algorithms.If(\n",
    "        has_cloud_data,\n",
    "        apply_s2cloudless_mask(),\n",
    "        use_qa60_only()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentinel2(interior_fc, year, region_bounds):\n",
    "    \"\"\"\n",
    "    Extract S2 features using pre-computed interior FC.\n",
    "    \"\"\"\n",
    "    \n",
    "    s2 = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "          .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "          .filterBounds(region_bounds)\n",
    "          .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', S2_CLOUD_THRESHOLD)))\n",
    "    \n",
    "    s2_cloudless = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "                    .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "                    .filterBounds(region_bounds))\n",
    "    \n",
    "    def add_cloud_and_ndsi_bands(img):\n",
    "        img_date = img.date()\n",
    "        cloud_prob_img = s2_cloudless.filterDate(\n",
    "            img_date, img_date.advance(1, 'day')\n",
    "        ).first()\n",
    "        \n",
    "        cloud_prob = ee.Image(ee.Algorithms.If(\n",
    "            cloud_prob_img,\n",
    "            ee.Image(cloud_prob_img).select('probability'),\n",
    "            ee.Image.constant(0).rename('probability')\n",
    "        ))\n",
    "        \n",
    "        qa = img.select('QA60')\n",
    "        cloud_bit_mask = 1 << 10\n",
    "        cirrus_bit_mask = 1 << 11\n",
    "        qa_clear = qa.bitwiseAnd(cloud_bit_mask).eq(0).And(\n",
    "                   qa.bitwiseAnd(cirrus_bit_mask).eq(0))\n",
    "        \n",
    "        s2cloudless_clear = cloud_prob.lt(S2_CLOUD_PROB_THRESHOLD)\n",
    "        combined_clear = qa_clear.And(s2cloudless_clear)\n",
    "        \n",
    "        ndsi = img.normalizedDifference(['B3', 'B11']).rename('ndsi')\n",
    "        ndsi_masked = ndsi.updateMask(combined_clear)\n",
    "        ice_binary = ndsi_masked.gt(S2_NDSI_THRESHOLD).rename('ice_binary')\n",
    "        cloud_mask = combined_clear.Not().rename('is_cloud')\n",
    "        \n",
    "        return img.addBands(ndsi_masked).addBands(ice_binary).addBands(cloud_mask)\n",
    "    \n",
    "    s2 = s2.map(add_cloud_and_ndsi_bands)\n",
    "    \n",
    "    def extract_s2_features(img):\n",
    "        date = img.date()\n",
    "        \n",
    "        # Stack bands for single reduceRegions call\n",
    "        bands = img.select(['ndsi', 'ice_binary', 'is_cloud'])\n",
    "        \n",
    "        samples = bands.reduceRegions(\n",
    "            collection=interior_fc,\n",
    "            reducer=ee.Reducer.mean(),\n",
    "            scale=20\n",
    "        )\n",
    "        \n",
    "        def create_s2_feature(feat):\n",
    "            ndsi_mean = feat.get('ndsi')\n",
    "            ice_fraction = feat.get('ice_binary')\n",
    "            cloud_fraction = feat.get('is_cloud')\n",
    "            \n",
    "            cloud_pct = ee.Algorithms.If(\n",
    "                cloud_fraction,\n",
    "                ee.Number(cloud_fraction).multiply(100),\n",
    "                ee.Number(-1)\n",
    "            )\n",
    "            \n",
    "            ice_fraction_safe = ee.Algorithms.If(\n",
    "                ndsi_mean,\n",
    "                ice_fraction,\n",
    "                None\n",
    "            )\n",
    "            \n",
    "            return ee.Feature(None, {\n",
    "                'lake_id': feat.get('lake_id'),\n",
    "                's2_date': date.format('YYYY-MM-dd'),\n",
    "                's2_doy': date.getRelative('day', 'year'),\n",
    "                's2_ndsi_mean': ndsi_mean,\n",
    "                's2_ice_fraction': ice_fraction_safe,\n",
    "                's2_cloud_pct': cloud_pct\n",
    "            })\n",
    "        \n",
    "        return samples.map(create_s2_feature)\n",
    "    \n",
    "    s2_features = s2.map(extract_s2_features).flatten()\n",
    "    s2_features = s2_features.filter(ee.Filter.notNull(['s2_ndsi_mean']))\n",
    "    \n",
    "    return s2_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ERA5 Temperature Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_era5_temperature(lakes_fc, year):\n",
    "    \"\"\"\n",
    "    Process ERA5-Land temperature data - OPTIMIZED VERSION\n",
    "    Pre-computes daily means once, then samples all lakes in batch\n",
    "    Much faster than computing daily mean separately for each lake\n",
    "    \"\"\"\n",
    "    print(f\"  Loading ERA5 hourly data for {year}...\")\n",
    "    \n",
    "    # Load ERA5-Land hourly data\n",
    "    era5_hourly = (ee.ImageCollection('ECMWF/ERA5_LAND/HOURLY')\n",
    "                   .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "                   .select('temperature_2m'))\n",
    "    \n",
    "    # Convert to Celsius\n",
    "    def to_celsius(img):\n",
    "        temp_c = img.subtract(273.15).rename('temp_c')\n",
    "        return temp_c.copyProperties(img, ['system:time_start'])\n",
    "    \n",
    "    era5_hourly = era5_hourly.map(to_celsius)\n",
    "    \n",
    "    print(f\"  Pre-computing daily means...\")\n",
    "    \n",
    "    # Determine number of days in year (handle leap years)\n",
    "    is_leap = ee.Number(year).mod(4).eq(0).And(\n",
    "        ee.Number(year).mod(100).neq(0).Or(\n",
    "            ee.Number(year).mod(400).eq(0)\n",
    "        )\n",
    "    )\n",
    "    n_days = ee.Number(ee.Algorithms.If(is_leap, 366, 365))\n",
    "    \n",
    "    # Pre-compute daily means for entire year (365 or 366 images)\n",
    "    def compute_daily_mean(day):\n",
    "        day = ee.Number(day)\n",
    "        date = ee.Date.fromYMD(year, 1, 1).advance(day.subtract(1), 'day')\n",
    "        next_date = date.advance(1, 'day')\n",
    "        \n",
    "        # Get all hourly images for this day\n",
    "        daily_collection = era5_hourly.filterDate(date, next_date)\n",
    "        \n",
    "        # Check if we have data\n",
    "        has_data = daily_collection.size().gt(0)\n",
    "        \n",
    "        # Compute mean if data exists, otherwise use missing flag\n",
    "        daily_mean = ee.Image(ee.Algorithms.If(\n",
    "            has_data,\n",
    "            daily_collection.mean(),\n",
    "            ee.Image.constant(-9999).rename('temp_c')\n",
    "        ))\n",
    "        \n",
    "        return daily_mean.set({\n",
    "            'system:time_start': date.millis(),\n",
    "            'doy': day,\n",
    "            'date': date.format('YYYY-MM-dd')\n",
    "        })\n",
    "    \n",
    "    days = ee.List.sequence(1, n_days)\n",
    "    era5_daily = ee.ImageCollection.fromImages(days.map(compute_daily_mean))\n",
    "    \n",
    "    print(f\"  Sampling all lakes from daily means...\")\n",
    "    \n",
    "    # Now sample ALL lakes from each daily mean (batch operation)\n",
    "    def sample_all_lakes(daily_img):\n",
    "        doy = daily_img.get('doy')\n",
    "        date = daily_img.get('date')\n",
    "        \n",
    "        # Sample ALL lakes at once using reduceRegions\n",
    "        samples = daily_img.reduceRegions(\n",
    "            collection=lakes_fc,\n",
    "            reducer=ee.Reducer.first(),  # Get pixel value at centroid\n",
    "            scale=11000  # ERA5-Land resolution\n",
    "        )\n",
    "        \n",
    "        # Add date info to each sampled feature\n",
    "        def add_date_info(feat):\n",
    "            # Get the temperature value (from 'first' property created by reducer)\n",
    "            # Use ee.Algorithms.If to provide default if missing\n",
    "            temp_value = ee.Algorithms.If(\n",
    "                feat.propertyNames().contains('first'),\n",
    "                feat.get('first'),\n",
    "                -9999\n",
    "            )\n",
    "            \n",
    "            return feat.set({\n",
    "                'era5_date': date,\n",
    "                'era5_doy': doy,\n",
    "                'temp_c': temp_value\n",
    "            })\n",
    "        \n",
    "        return samples.map(add_date_info)\n",
    "    \n",
    "    # Process all daily images\n",
    "    era5_features = era5_daily.map(sample_all_lakes).flatten()\n",
    "    \n",
    "    return era5_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Show chunk statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks to process: 21\n",
      "Years to process: [2019, 2020, 2021]\n",
      "Total exports: 189 (chunks × years × 3 datasets)\n",
      "\n",
      "Chunk statistics:\n",
      "    chunk_id  n_lakes    lat_min    lat_max     lon_min     lon_max\n",
      "0          0     1659  69.003753  70.805772 -157.460685 -156.323643\n",
      "1          1     1402  69.003327  70.498668 -150.588202 -149.501159\n",
      "2          2     2172  69.000561  70.506201 -149.575529 -148.460811\n",
      "3          3     1002  69.002549  70.662131 -161.092265 -159.724530\n",
      "4          4     2243  70.139174  71.157997 -155.626734 -154.362639\n",
      "5          5      408  69.061209  70.123854 -144.857630 -141.021667\n",
      "6          6     1726  69.886316  70.909059 -153.572246 -152.259761\n",
      "7          7     1704  69.002644  70.811790 -158.909885 -157.856992\n",
      "8          8      753  69.019065  70.186629 -147.158786 -144.887360\n",
      "9          9      460  69.027426  70.075681 -163.578774 -162.004458\n",
      "10        10     1440  69.013772  70.328129 -153.976923 -152.575637\n",
      "11        11     1392  69.000974  70.218418 -155.661592 -154.104769\n",
      "12        12     1834  69.001176  70.475984 -151.662586 -150.520819\n",
      "13        13     2001  69.468849  70.951464 -156.328499 -155.325830\n",
      "14        14     2095  69.242415  70.875454 -154.562909 -153.490827\n",
      "15        15     1152  69.001200  70.319093 -148.591307 -147.158186\n",
      "16        16     2019  69.000146  70.582641 -152.675190 -151.585260\n",
      "17        17      897  69.008946  70.299875 -162.343774 -160.999169\n",
      "18        18     1493  69.020858  70.841981 -159.930147 -158.880509\n",
      "19        19     1987  69.118177  71.063040 -158.031454 -157.038279\n",
      "20        20     1269  70.430225  71.360948 -157.142680 -155.634957\n"
     ]
    }
   ],
   "source": [
    "# Load chunk statistics to know how many chunks we have\n",
    "chunk_stats = pd.read_csv(f'gs://{BUCKET}/{BASE_PATH}/processed/chunk_statistics.csv')\n",
    "n_chunks = len(chunk_stats)\n",
    "\n",
    "print(f\"Total chunks to process: {n_chunks}\")\n",
    "print(f\"Years to process: {YEARS}\")\n",
    "print(f\"Total exports: {n_chunks * len(YEARS) * 3} (chunks × years × 3 datasets)\")\n",
    "print(\"\\nChunk statistics:\")\n",
    "print(chunk_stats[['chunk_id', 'n_lakes', 'lat_min', 'lat_max', 'lon_min', 'lon_max']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ERA5 Temperature export (via xarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DOWNLOADING ERA5 TEMPERATURE DATA\n",
      "============================================================\n",
      "Step 1: Loading lake locations by chunk...\n",
      "  Loaded 31,108 lakes across 21 chunks\n",
      "\n",
      "Step 2: Accessing ERA5 dataset...\n",
      "  Checking coordinates...\n",
      "  Latitude: -90.0 to 90.0\n",
      "  Longitude: 0.0 to 359.8\n",
      "\n",
      "  Converting to 0-360: Alaska is 196-220E\n",
      "\n",
      "Step 3: Selecting North Slope region...\n",
      "  Selected: 13 lats x 97 lons x 26304 hours\n",
      "\n",
      "Step 4: Computing daily means...\n",
      "  1096 daily means computed\n",
      "\n",
      "Step 5: Processing each chunk...\n",
      "  Chunk 00 (1659 lakes)... Done\n",
      "  Chunk 01 (1402 lakes)... Done\n",
      "  Chunk 02 (2172 lakes)... Done\n",
      "  Chunk 03 (1002 lakes)... Done\n",
      "  Chunk 04 (2243 lakes)... Done\n",
      "  Chunk 05 (408 lakes)... Done\n",
      "  Chunk 06 (1726 lakes)... Done\n",
      "  Chunk 07 (1704 lakes)... Done\n",
      "  Chunk 08 (753 lakes)... Done\n",
      "  Chunk 09 (460 lakes)... Done\n",
      "  Chunk 10 (1440 lakes)... Done\n",
      "  Chunk 11 (1392 lakes)... Done\n",
      "  Chunk 12 (1834 lakes)... Done\n",
      "  Chunk 13 (2001 lakes)... Done\n",
      "  Chunk 14 (2095 lakes)... Done\n",
      "  Chunk 15 (1152 lakes)... Done\n",
      "  Chunk 16 (2019 lakes)... Done\n",
      "  Chunk 17 (897 lakes)... Done\n",
      "  Chunk 18 (1493 lakes)... Done\n",
      "  Chunk 19 (1987 lakes)... Done\n",
      "  Chunk 20 (1269 lakes)... Done\n",
      "\n",
      "============================================================\n",
      "ERA5 COMPLETE!\n",
      "============================================================\n",
      "Time: 7.0 minutes\n",
      "Files: 63 CSV files\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ERA5 Temperature Export (via xarray) - ROBUST VERSION\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DOWNLOADING ERA5 TEMPERATURE DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "era5_start = time.time()\n",
    "\n",
    "# Load chunk info\n",
    "print(\"Step 1: Loading lake locations by chunk...\")\n",
    "chunks_info = []\n",
    "\n",
    "for chunk_id in range(n_chunks):\n",
    "    chunk_file = f'{CHUNKS_PATH}/chunk_{chunk_id:02d}.geojson'\n",
    "    chunk_gdf = gpd.read_file(chunk_file)\n",
    "    chunk_gdf['lake_id'] = range(len(chunk_gdf))\n",
    "    \n",
    "    # Keep original -180:180 coordinates\n",
    "    chunks_info.append({\n",
    "        'chunk_id': chunk_id,\n",
    "        'gdf': chunk_gdf[['lake_id', 'centroid_lon', 'centroid_lat']],\n",
    "        'n_lakes': len(chunk_gdf)\n",
    "    })\n",
    "\n",
    "total_lakes = sum(c['n_lakes'] for c in chunks_info)\n",
    "print(f\"  Loaded {total_lakes:,} lakes across {len(chunks_info)} chunks\\n\")\n",
    "\n",
    "# Download ERA5\n",
    "print(\"Step 2: Accessing ERA5 dataset...\")\n",
    "ERA5_URL = 'gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3'\n",
    "ds = xr.open_zarr(ERA5_URL, chunks={'time': 100}, consolidated=True)\n",
    "\n",
    "# Check coordinate system\n",
    "print(\"  Checking coordinates...\")\n",
    "lat_min, lat_max = float(ds.latitude.min()), float(ds.latitude.max())\n",
    "lon_min, lon_max = float(ds.longitude.min()), float(ds.longitude.max())\n",
    "print(f\"  Latitude: {lat_min:.1f} to {lat_max:.1f}\")\n",
    "print(f\"  Longitude: {lon_min:.1f} to {lon_max:.1f}\\n\")\n",
    "\n",
    "# Convert Alaska longitudes if needed\n",
    "alaska_lon_min, alaska_lon_max = -164, -140\n",
    "if lon_min >= 0:  # Dataset uses 0-360\n",
    "    alaska_lon_min = alaska_lon_min % 360  # 196\n",
    "    alaska_lon_max = alaska_lon_max % 360  # 220\n",
    "    print(f\"  Converting to 0-360: Alaska is {alaska_lon_min}-{alaska_lon_max}E\\n\")\n",
    "\n",
    "print(\"Step 3: Selecting North Slope region...\")\n",
    "# Select with proper ordering\n",
    "if ds.latitude[0] > ds.latitude[-1]:  # Descending\n",
    "    lat_slice = slice(72, 69)\n",
    "else:  # Ascending\n",
    "    lat_slice = slice(69, 72)\n",
    "\n",
    "ds_subset = ds['2m_temperature'].sel(\n",
    "    latitude=lat_slice,\n",
    "    longitude=slice(alaska_lon_min, alaska_lon_max),\n",
    "    time=slice(f'{YEARS[0]}-01-01', f'{YEARS[-1]}-12-31')\n",
    ")\n",
    "\n",
    "print(f\"  Selected: {len(ds_subset.latitude)} lats x {len(ds_subset.longitude)} lons x {len(ds_subset.time)} hours\\n\")\n",
    "\n",
    "if len(ds_subset.latitude) == 0 or len(ds_subset.longitude) == 0:\n",
    "    raise ValueError(\"Selection returned empty dataset - check coordinate ranges\")\n",
    "\n",
    "print(\"Step 4: Computing daily means...\")\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    # Compute and load into memory (avoids dask chunking issues)\n",
    "    ds_daily = ds_subset.resample(time='1D').mean().compute()\n",
    "    ds_daily_celsius = ds_daily - 273.15\n",
    "\n",
    "print(f\"  {len(ds_daily_celsius.time)} daily means computed\\n\")\n",
    "\n",
    "print(\"Step 5: Processing each chunk...\")\n",
    "\n",
    "for chunk_info in chunks_info:\n",
    "    chunk_id = chunk_info['chunk_id']\n",
    "    chunk_gdf = chunk_info['gdf']\n",
    "    n_lakes = chunk_info['n_lakes']\n",
    "    \n",
    "    print(f\"  Chunk {chunk_id:02d} ({n_lakes} lakes)...\", end=' ')\n",
    "    \n",
    "    # Convert lake coordinates if needed\n",
    "    lake_lons = chunk_gdf['centroid_lon'].values\n",
    "    if alaska_lon_min > 180:  # Dataset uses 0-360\n",
    "        lake_lons = lake_lons % 360\n",
    "    \n",
    "    lake_lats = xr.DataArray(chunk_gdf['centroid_lat'].values, dims=['lake'])\n",
    "    lake_lons_da = xr.DataArray(lake_lons, dims=['lake'])\n",
    "    \n",
    "    # Interpolate\n",
    "    lake_temps = ds_daily_celsius.interp(\n",
    "        latitude=lake_lats,\n",
    "        longitude=lake_lons_da,\n",
    "        method='linear'\n",
    "    )\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_chunk = lake_temps.to_dataframe(name='temp_c').reset_index()\n",
    "    df_chunk['lake_id'] = df_chunk['lake']\n",
    "    df_chunk['era5_date'] = df_chunk['time'].dt.strftime('%Y-%m-%d')\n",
    "    df_chunk['era5_doy'] = df_chunk['time'].dt.dayofyear\n",
    "    df_chunk['year'] = df_chunk['time'].dt.year\n",
    "    df_chunk = df_chunk[['lake_id', 'era5_date', 'era5_doy', 'temp_c', 'year']]\n",
    "    \n",
    "    # Export by year\n",
    "    for year in YEARS:\n",
    "        df_year = df_chunk[df_chunk['year'] == year].copy()\n",
    "        df_year = df_year.drop(columns=['year']).sort_values(['lake_id', 'era5_doy'])\n",
    "        output_file = f'gs://{BUCKET}/{OUTPUT_PATH}/{year}/chunk_{chunk_id:02d}/era5_data.csv'\n",
    "        df_year.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(\"Done\")\n",
    "\n",
    "era5_time = time.time() - era5_start\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ERA5 COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Time: {era5_time/60:.1f} minutes\")\n",
    "print(f\"Files: {len(chunks_info) * len(YEARS)} CSV files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## S1/S2 exports (via GEE)\n",
    "\n",
    "**WARNING:** This will prep ~126 export tasks (21 chunks × 3 years × 2 datasets each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing Chunk 0\n",
      "============================================================\n",
      "  1659 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Chunk 0 complete (1.5s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 1\n",
      "============================================================\n",
      "  1402 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Chunk 1 complete (1.0s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 2\n",
      "============================================================\n",
      "  2172 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Chunk 2 complete (2.9s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 3\n",
      "============================================================\n",
      "  1002 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Chunk 3 complete (0.7s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 4\n",
      "============================================================\n",
      "  2243 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Chunk 4 complete (2.5s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 5\n",
      "============================================================\n",
      "  408 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Chunk 5 complete (0.5s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 6\n",
      "============================================================\n",
      "  1726 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Chunk 6 complete (3.5s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 7\n",
      "============================================================\n",
      "  1704 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Chunk 7 complete (1.0s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 8\n",
      "============================================================\n",
      "  753 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Chunk 8 complete (0.6s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 9\n",
      "============================================================\n",
      "  460 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Chunk 9 complete (0.6s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 10\n",
      "============================================================\n",
      "  1440 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Chunk 10 complete (0.9s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 11\n",
      "============================================================\n",
      "  1392 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Chunk 11 complete (1.1s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 12\n",
      "============================================================\n",
      "  1834 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Chunk 12 complete (2.8s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 13\n",
      "============================================================\n",
      "  2001 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Chunk 13 complete (2.6s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 14\n",
      "============================================================\n",
      "  2095 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Chunk 14 complete (2.1s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 15\n",
      "============================================================\n",
      "  1152 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Chunk 15 complete (0.8s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 16\n",
      "============================================================\n",
      "  2019 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Chunk 16 complete (5.4s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 17\n",
      "============================================================\n",
      "  897 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Chunk 17 complete (0.8s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 18\n",
      "============================================================\n",
      "  1493 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Chunk 18 complete (1.0s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 19\n",
      "============================================================\n",
      "  1987 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Chunk 19 complete (7.4s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 20\n",
      "============================================================\n",
      "  1269 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Chunk 20 complete (1.1s)\n",
      "\n",
      "============================================================\n",
      "ALL EXPORTS PREPARED: 126 tasks\n",
      "============================================================\n",
      "Total preparation time: 0.7 minutes\n",
      "\n",
      "Ready to start 126 S1/S2 exports.\n",
      "ERA5 already complete (63 files).\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Prep S1/S2 exports\n",
    "# ERA5 already exported above\n",
    "# ============================================================\n",
    "\n",
    "all_exports = []\n",
    "total_start = time.time()\n",
    "\n",
    "for chunk_id in range(n_chunks):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing Chunk {chunk_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    chunk_start = time.time()\n",
    "    \n",
    "    # Load chunk with pre-computed geometries\n",
    "    interior_fc, landscape_fc, chunk_bounds, chunk_gdf = load_chunk_from_bucket(chunk_id)\n",
    "    \n",
    "    print(f\"  {len(chunk_gdf)} lakes (geometries pre-computed)\")\n",
    "    \n",
    "    for year in YEARS:\n",
    "        print(f\"  Preparing {year}...\", end=' ')\n",
    "        \n",
    "        # Process S1 and S2 with pre-computed FCs\n",
    "        s1_features = process_sentinel1(interior_fc, landscape_fc, year, chunk_bounds)\n",
    "        s2_features = process_sentinel2(interior_fc, year, chunk_bounds)\n",
    "        \n",
    "        # Create export tasks\n",
    "        s1_task = ee.batch.Export.table.toCloudStorage(\n",
    "            collection=s1_features,\n",
    "            description=f'S1_chunk{chunk_id:02d}_{year}',\n",
    "            bucket=BUCKET,\n",
    "            fileNamePrefix=f'{OUTPUT_PATH}/{year}/chunk_{chunk_id:02d}/s1_data',\n",
    "            fileFormat='CSV'\n",
    "        )\n",
    "        \n",
    "        s2_task = ee.batch.Export.table.toCloudStorage(\n",
    "            collection=s2_features,\n",
    "            description=f'S2_chunk{chunk_id:02d}_{year}',\n",
    "            bucket=BUCKET,\n",
    "            fileNamePrefix=f'{OUTPUT_PATH}/{year}/chunk_{chunk_id:02d}/s2_data',\n",
    "            fileFormat='CSV'\n",
    "        )\n",
    "        \n",
    "        all_exports.append({\n",
    "            'chunk_id': chunk_id,\n",
    "            'year': year,\n",
    "            'type': 'S1',\n",
    "            'task': s1_task\n",
    "        })\n",
    "        all_exports.append({\n",
    "            'chunk_id': chunk_id,\n",
    "            'year': year,\n",
    "            'type': 'S2',\n",
    "            'task': s2_task\n",
    "        })\n",
    "        \n",
    "        print(\"Done\")\n",
    "    \n",
    "    chunk_time = time.time() - chunk_start\n",
    "    print(f\"  Chunk {chunk_id} complete ({chunk_time:.1f}s)\")\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ALL EXPORTS PREPARED: {len(all_exports)} tasks\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total preparation time: {total_time/60:.1f} minutes\")\n",
    "print(f\"\\nReady to start {len(all_exports)} S1/S2 exports.\")\n",
    "print(f\"ERA5 already complete (63 files).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 126 tasks...\n",
      "  Started 10 tasks...\n",
      "  Started 20 tasks...\n",
      "  Started 30 tasks...\n",
      "  Started 40 tasks...\n",
      "  Started 50 tasks...\n",
      "  Started 60 tasks...\n",
      "  Started 70 tasks...\n",
      "  Started 80 tasks...\n",
      "  Started 90 tasks...\n",
      "  Started 100 tasks...\n",
      "  Started 110 tasks...\n",
      "  Started 120 tasks...\n",
      "\n",
      "Started 126/126 tasks\n",
      "Monitor at: https://code.earthengine.google.com/tasks\n"
     ]
    }
   ],
   "source": [
    "# Start all exports\n",
    "print(f\"Starting {len(all_exports)} tasks...\")\n",
    "started = 0\n",
    "\n",
    "for i, export in enumerate(all_exports):\n",
    "    try:\n",
    "        export['task'].start()\n",
    "        started += 1\n",
    "        \n",
    "        if started % 10 == 0:\n",
    "            print(f\"  Started {started} tasks...\")\n",
    "            time.sleep(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting task {i} ({export['type']} chunk {export['chunk_id']} {export['year']}): {e}\")\n",
    "\n",
    "print(f\"\\nStarted {started}/{len(all_exports)} tasks\")\n",
    "print(\"Monitor at: https://code.earthengine.google.com/tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Monitor GEE Export Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export Status Summary:\n",
      "  Total tasks: 126\n",
      "    READY: 124\n",
      "    RUNNING: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'READY': 124, 'RUNNING': 2, 'COMPLETED': 0, 'FAILED': 0, 'CANCELLED': 0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check status of GEE exports\n",
    "def check_export_status():\n",
    "    status_summary = {\n",
    "        'READY': 0,\n",
    "        'RUNNING': 0,\n",
    "        'COMPLETED': 0,\n",
    "        'FAILED': 0,\n",
    "        'CANCELLED': 0\n",
    "    }\n",
    "    \n",
    "    for exp in all_exports:\n",
    "        status = exp['task'].status()['state']\n",
    "        status_summary[status] = status_summary.get(status, 0) + 1\n",
    "    \n",
    "    print(f\"Export Status Summary:\")\n",
    "    print(f\"  Total tasks: {len(all_exports)}\")\n",
    "    for state, count in status_summary.items():\n",
    "        if count > 0:\n",
    "            print(f\"    {state}: {count}\")\n",
    "    \n",
    "    return status_summary\n",
    "\n",
    "# Run this cell periodically to check progress\n",
    "check_export_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook exports:\n",
    "\n",
    "**Sentinel-1**: \n",
    "- Lake interior: VV, VH backscatter (dB), VV-VH ratio, RGB features (normalized 0-255)\n",
    "- Landscape ring: VV, VH backscatter (dB), VV-VH ratio, RGB features (normalized 0-255)\n",
    "- Both ASCENDING and DESCENDING orbits\n",
    "- Angle mask applied (25-50 degrees)\n",
    "\n",
    "**Sentinel-2**: \n",
    "- s2_ndsi_mean: Mean NDSI value across lake pixels (continuous, -1 to 1)\n",
    "- s2_ice_fraction: Fraction of lake pixels with NDSI > 0.4 (continuous, 0 to 1)\n",
    "- s2_cloud_pct: Percentage of lake masked by clouds (0-100, for QC filtering)\n",
    "- Dual cloud masking: QA60 + s2cloudless\n",
    "\n",
    "**ERA5**: \n",
    "- Daily mean 2m air temperature at lake centroids (Celsius)\n",
    "\n",
    "**For:**\n",
    "- 21 spatial chunks\n",
    "- 3 years (2019, 2020, 2021)\n",
    "- ~31,000 North Slope lakes (>0.02 km²)\n",
    "\n",
    "**Output structure:**\n",
    "```\n",
    "gs://wustl-eeps-geospatial/thermokarst_lakes/exports/\n",
    "├── 2019/\n",
    "│   ├── chunk_00/\n",
    "│   │   ├── s1_data.csv\n",
    "│   │   ├── s2_data.csv\n",
    "│   │   └── era5_data.csv\n",
    "│   ├── chunk_01/\n",
    "│   └── ...\n",
    "├── 2020/\n",
    "└── 2021/\n",
    "```\n",
    "\n",
    "**Next step:** Combine CSVs and run ice detection algorithm (Notebook 03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "gee",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "GEE Geospatial (Local)",
   "language": "python",
   "name": "gee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

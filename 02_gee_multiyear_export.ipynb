{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Year Lake Ice Phenology Data Export\n",
    "## Part 2: xarray and GEE Processing and Export (2019-2021)\n",
    "\n",
    "**Goal:** Export S1 + S2 + ERA5 data for North Slope lakes across 3 years\n",
    "\n",
    "**Strategy:**\n",
    "- Process chunks independently (spatial parallelization)\n",
    "- Export one year at a time per chunk\n",
    "- Use efficient spatial filtering (only process S2 images that overlap lakes)\n",
    "- Total exports: ~21 chunks × 3 years x 2 datasets (S1, S2) = ~126 exports\n",
    "- Export ERA5 via xarray (more efficient)\n",
    "\n",
    "**Data sources:**\n",
    "- Sentinel-1 GRD (SAR)\n",
    "- Sentinel-2 SR Harmonized (optical, for NDSI)\n",
    "- ERA5-Land (temperature)\n",
    "\n",
    "**Years:** 2019, 2020, 2021 (match ALPOD temporal coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n",
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n",
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n",
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n",
      "Earth Engine initialized: GEE Initialized\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import xarray as xr\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Initialize Earth Engine\n",
    "ee.Initialize()\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"Earth Engine initialized: {ee.String('GEE Initialized').getInfo()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Years: [2019, 2020, 2021]\n",
      "  Chunks path: gs://wustl-eeps-geospatial/thermokarst_lakes/processed/chunks\n",
      "  Output: gs://wustl-eeps-geospatial/thermokarst_lakes/exports\n",
      "  S2 scene cloud threshold: 30%\n",
      "  S2 pixel cloud probability threshold: 40%\n",
      "  S2 time window: ±3 days\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BUCKET = 'wustl-eeps-geospatial'\n",
    "BASE_PATH = 'thermokarst_lakes'\n",
    "CHUNKS_PATH = f'gs://{BUCKET}/{BASE_PATH}/processed/chunks'\n",
    "OUTPUT_PATH = f'{BASE_PATH}/exports'  # No gs:// prefix for GEE exports\n",
    "\n",
    "# Years to process (match ALPOD coverage)\n",
    "YEARS = [2019, 2020, 2021]\n",
    "\n",
    "# Processing parameters\n",
    "SCALE = 10  # Sentinel-1 resolution\n",
    "S2_NDSI_THRESHOLD = 0.4  # NDSI > 0.4 = ice\n",
    "S2_CLOUD_THRESHOLD = 30  # Maximum cloud cover for S2 images (scene-level)\n",
    "S2_CLOUD_PROB_THRESHOLD = 40  # s2cloudless probability threshold (pixel-level)\n",
    "S2_TIME_WINDOW = 3  # Days before/after S1 acquisition to look for S2\n",
    "\n",
    "# Projection\n",
    "ALASKA_ALBERS = 'EPSG:3338'\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Years: {YEARS}\")\n",
    "print(f\"  Chunks path: {CHUNKS_PATH}\")\n",
    "print(f\"  Output: gs://{BUCKET}/{OUTPUT_PATH}\")\n",
    "print(f\"  S2 scene cloud threshold: {S2_CLOUD_THRESHOLD}%\")\n",
    "print(f\"  S2 pixel cloud probability threshold: {S2_CLOUD_PROB_THRESHOLD}%\")\n",
    "print(f\"  S2 time window: ±{S2_TIME_WINDOW} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload ALPOD data as GEE Asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping ALPOD upload (already done)\n",
      "Asset location: projects/eeps-geospatial/assets/ALPOD_full\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# UPLOAD ALPOD TO GEE ASSET (ONE-TIME ONLY)\n",
    "# =============================================================================\n",
    "# Set to False after asset is uploaded\n",
    "UPLOAD_ALPOD_ASSET = False\n",
    "\n",
    "if UPLOAD_ALPOD_ASSET:\n",
    "    print(\"Uploading ALPOD to GEE Asset...\")\n",
    "    print(\"(Direct from GCS )\\n\")\n",
    "    \n",
    "    !/opt/conda/bin/earthengine upload table \\\n",
    "        --asset_id=projects/eeps-geospatial/assets/ALPOD_full \\\n",
    "        gs://wustl-eeps-geospatial/thermokarst_lakes/ALPODlakes/ALPODlakes.shp\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Check progress: https://code.earthengine.google.com/tasks\")\n",
    "    print(\"Look for an 'Ingestion' task\")\n",
    "    print(\"\\n After upload completes, set UPLOAD_ALPOD_ASSET = False\")\n",
    "else:\n",
    "    print(\"Skipping ALPOD upload (already done)\")\n",
    "    print(\"Asset location: projects/eeps-geospatial/assets/ALPOD_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunk_from_bucket(chunk_id):\n",
    "    \"\"\"\n",
    "    Load a chunk and return ee.FeatureCollection from GEE Asset\n",
    "    Only sends lake IDs to GEE, not geometries (avoids payload limits)\n",
    "    \"\"\"\n",
    "    chunk_file = f'{CHUNKS_PATH}/chunk_{chunk_id:02d}.geojson'\n",
    "    \n",
    "    # Load locally to get ALPOD IDs and metadata\n",
    "    chunk_gdf = gpd.read_file(chunk_file)\n",
    "    chunk_gdf_wgs84 = chunk_gdf.to_crs('EPSG:4326')\n",
    "    \n",
    "    # Get ALPOD IDs\n",
    "    alpod_ids = chunk_gdf['id'].tolist()\n",
    "    \n",
    "    # Load from GEE Asset and filter to these lakes\n",
    "    all_alpod = ee.FeatureCollection('projects/eeps-geospatial/assets/ALPOD_full')\n",
    "    chunk_fc = all_alpod.filter(ee.Filter.inList('id', alpod_ids))\n",
    "    \n",
    "    # Add lake_id field (use ALPOD id)\n",
    "    chunk_fc = chunk_fc.map(lambda f: f.set('lake_id', f.get('id')))\n",
    "    \n",
    "    return chunk_fc, chunk_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lake_geometry_metrics(lakes_fc, region_bounds):\n",
    "    \"\"\"\n",
    "    Add lake interior and landscape ring geometries\n",
    "    Uses efficient spatial filtering - only masks lakes within 100m of each target lake\n",
    "    \"\"\"\n",
    "    # Load ALPOD from GEE Asset\n",
    "    all_alpod = ee.FeatureCollection('projects/eeps-geospatial/assets/ALPOD_full')\n",
    "    \n",
    "    def add_geometries(lake):\n",
    "        lake_geom = lake.geometry()\n",
    "        lake_id = lake.get('lake_id')\n",
    "        \n",
    "        # Lake interior: 10m inward buffer\n",
    "        lake_interior = lake_geom.buffer(-10)\n",
    "        \n",
    "        # Landscape ring: 100m outward buffer\n",
    "        ring_outer = lake_geom.buffer(100)\n",
    "        \n",
    "        # Only find lakes that intersect THIS lake's 100m buffer\n",
    "        nearby_lakes = all_alpod.filterBounds(ring_outer)\n",
    "        nearby_dissolved = nearby_lakes.geometry().dissolve(maxError=10)\n",
    "        \n",
    "        # Subtract only the nearby lakes\n",
    "        landscape_ring = ring_outer.difference(nearby_dissolved)\n",
    "        \n",
    "        return lake.set({\n",
    "            'lake_id': lake_id,\n",
    "            'lake_interior': lake_interior,\n",
    "            'landscape_ring': landscape_ring\n",
    "        })\n",
    "    \n",
    "    return lakes_fc.map(add_geometries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sentinel-1 Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentinel1(lakes_fc, year, region_bounds):\n",
    "    \"\"\"\n",
    "    Extract S1 features for both lake interiors and landscape rings.\n",
    "    \n",
    "    For each zone (lake and landscape), exports:\n",
    "    - VV, VH backscatter (raw dB)\n",
    "    - VV-VH ratio (dB)\n",
    "    - RGB scaled features (normalized 0-255)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load S1 collection\n",
    "    s1 = (ee.ImageCollection('COPERNICUS/S1_GRD')\n",
    "          .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "          .filterBounds(region_bounds)\n",
    "          .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
    "          .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
    "          .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n",
    "          .filter(ee.Filter.eq('resolution_meters', 10)))\n",
    "    \n",
    "    def apply_angle_mask(img):\n",
    "        \"\"\"Mask pixels outside 25-50 degree incidence angle range\"\"\"\n",
    "        angle = img.select('angle')\n",
    "        angle_mask = angle.gt(25).And(angle.lt(50))\n",
    "        return img.select(['VV', 'VH']).updateMask(angle_mask).copyProperties(img, img.propertyNames())\n",
    "    \n",
    "    s1 = s1.map(apply_angle_mask)\n",
    "    \n",
    "    def extract_s1_features(img):\n",
    "        \"\"\"Extract lake and landscape statistics from each S1 image\"\"\"\n",
    "        \n",
    "        vv_img = img.select('VV')\n",
    "        vh_img = img.select('VH')\n",
    "        \n",
    "        # Compute VV-VH at image level (before reduction)\n",
    "        vv_vh_img = vv_img.subtract(vh_img).rename('VV_VH')\n",
    "        \n",
    "        # Compute RGB bands (scaled/normalized features)\n",
    "        r_band = vv_img.unitScale(-20, -5).multiply(255).rename('R')\n",
    "        g_band = vh_img.unitScale(-28, -12).multiply(255).rename('G')\n",
    "        b_band = vv_vh_img.unitScale(8, 18).multiply(255).rename('B')\n",
    "        \n",
    "        # Stack all bands for efficient extraction\n",
    "        all_bands = vv_img.addBands(vh_img).addBands(vv_vh_img).addBands(r_band).addBands(g_band).addBands(b_band)\n",
    "        \n",
    "        date = img.date()\n",
    "        orbit = img.get('orbitProperties_pass')\n",
    "        \n",
    "        # Sample lake interiors\n",
    "        lake_samples = all_bands.reduceRegions(\n",
    "            collection=lakes_fc.select(['lake_id', 'lake_interior']),\n",
    "            reducer=ee.Reducer.mean(),\n",
    "            scale=10\n",
    "        )\n",
    "        \n",
    "        # Sample landscape rings\n",
    "        land_samples = all_bands.reduceRegions(\n",
    "            collection=lakes_fc.select(['lake_id', 'landscape_ring']),\n",
    "            reducer=ee.Reducer.mean(),\n",
    "            scale=10\n",
    "        )\n",
    "        \n",
    "        def create_s1_feature(lake_feat):\n",
    "            lake_id = lake_feat.get('lake_id')\n",
    "            \n",
    "            # Find matching landscape sample\n",
    "            land_feat = land_samples.filter(\n",
    "                ee.Filter.eq('lake_id', lake_id)\n",
    "            ).first()\n",
    "            \n",
    "            # Lake values - VV-VH already computed at image level\n",
    "            lake_vv = lake_feat.get('VV')\n",
    "            lake_vh = lake_feat.get('VH')\n",
    "            lake_vv_vh = lake_feat.get('VV_VH')\n",
    "            \n",
    "            # Landscape values\n",
    "            land_vv = land_feat.get('VV')\n",
    "            land_vh = land_feat.get('VH')\n",
    "            land_vv_vh = land_feat.get('VV_VH')\n",
    "            \n",
    "            # RGB values (already computed by reducer)\n",
    "            lake_r = lake_feat.get('R')\n",
    "            lake_g = lake_feat.get('G')\n",
    "            lake_b = lake_feat.get('B')\n",
    "            land_r = land_feat.get('R')\n",
    "            land_g = land_feat.get('G')\n",
    "            land_b = land_feat.get('B')\n",
    "            \n",
    "            return ee.Feature(None, {\n",
    "                'lake_id': lake_id,\n",
    "                's1_date': date.format('YYYY-MM-dd'),\n",
    "                's1_doy': date.getRelative('day', 'year'),\n",
    "                's1_orbit': orbit,\n",
    "                # Lake features\n",
    "                'lake_vv_db': lake_vv,\n",
    "                'lake_vh_db': lake_vh,\n",
    "                'lake_vv_vh_db': lake_vv_vh,\n",
    "                'lake_r': lake_r,\n",
    "                'lake_g': lake_g,\n",
    "                'lake_b': lake_b,\n",
    "                # Landscape features\n",
    "                'land_vv_db': land_vv,\n",
    "                'land_vh_db': land_vh,\n",
    "                'land_vv_vh_db': land_vv_vh,\n",
    "                'land_r': land_r,\n",
    "                'land_g': land_g,\n",
    "                'land_b': land_b\n",
    "            })\n",
    "        \n",
    "        return lake_samples.map(create_s1_feature)\n",
    "    \n",
    "    s1_features = s1.map(extract_s1_features).flatten()\n",
    "    \n",
    "    # Filter out observations where lake had no valid pixels\n",
    "    s1_features = s1_features.filter(ee.Filter.notNull(['lake_vv_db']))\n",
    "    \n",
    "    return s1_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sentinel-2 Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ndsi(img):\n",
    "    \"\"\"\n",
    "    Compute Normalized Difference Snow Index (NDSI)\n",
    "    NDSI = (Green - SWIR1) / (Green + SWIR1)\n",
    "    \"\"\"\n",
    "    green = img.select('B3')\n",
    "    swir1 = img.select('B11')\n",
    "    \n",
    "    ndsi = green.subtract(swir1).divide(green.add(swir1)).rename('ndsi')\n",
    "    \n",
    "    return img.addBands(ndsi)\n",
    "\n",
    "def mask_s2_clouds(img):\n",
    "    \"\"\"\n",
    "    Mask clouds using QA60 band (basic cloud mask)\n",
    "    \"\"\"\n",
    "    qa = img.select('QA60')\n",
    "    \n",
    "    # Bits 10 and 11 are clouds and cirrus\n",
    "    cloud_bit_mask = 1 << 10\n",
    "    cirrus_bit_mask = 1 << 11\n",
    "    \n",
    "    # Both should be zero (clear conditions)\n",
    "    mask = qa.bitwiseAnd(cloud_bit_mask).eq(0).And(\n",
    "           qa.bitwiseAnd(cirrus_bit_mask).eq(0))\n",
    "    \n",
    "    return img.updateMask(mask)\n",
    "\n",
    "def add_s2cloudless_mask(img):\n",
    "    s2_cloudless = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "    cloud_prob_collection = s2_cloudless.filter(\n",
    "        ee.Filter.eq('system:index', img.get('system:index'))\n",
    "    )\n",
    "    \n",
    "    has_cloud_data = cloud_prob_collection.size().gt(0)\n",
    "    \n",
    "    def apply_s2cloudless_mask():\n",
    "        cloud_prob = cloud_prob_collection.first().select('probability')\n",
    "        is_clear = cloud_prob.lt(S2_CLOUD_PROB_THRESHOLD)\n",
    "        return img.updateMask(is_clear)\n",
    "    \n",
    "    def use_qa60_only():\n",
    "        return img  # Already has QA60 mask\n",
    "    \n",
    "    return ee.Image(ee.Algorithms.If(\n",
    "        has_cloud_data,\n",
    "        apply_s2cloudless_mask(),\n",
    "        use_qa60_only()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel-2 Processing\n",
    "\n",
    "def process_sentinel2(lakes_fc, year, region_bounds):\n",
    "    \"\"\"\n",
    "    Extract S2 features - FIXED VERSION\n",
    "    \n",
    "    Exports:\n",
    "    - s2_ice_fraction: fraction of lake pixels with NDSI > threshold (0-1 continuous)\n",
    "    - s2_ndsi_mean: mean NDSI value across lake pixels (continuous)\n",
    "    - s2_cloud_pct: percentage of lake pixels masked by clouds (for QC)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load S2 collection with scene-level cloud filter\n",
    "    s2 = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "          .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "          .filterBounds(region_bounds)\n",
    "          .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', S2_CLOUD_THRESHOLD)))\n",
    "    \n",
    "    # Load s2cloudless for pixel-level cloud masking\n",
    "    s2_cloudless = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "                    .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "                    .filterBounds(region_bounds))\n",
    "    \n",
    "    def add_cloud_and_ndsi_bands(img):\n",
    "        \"\"\"Add NDSI, binary ice mask, and cloud probability bands\"\"\"\n",
    "        \n",
    "        # Get matching cloud probability image\n",
    "        img_date = img.date()\n",
    "        cloud_prob_img = s2_cloudless.filterDate(\n",
    "            img_date, img_date.advance(1, 'day')\n",
    "        ).first()\n",
    "        \n",
    "        # Cloud probability (0-100)\n",
    "        cloud_prob = ee.Image(ee.Algorithms.If(\n",
    "            cloud_prob_img,\n",
    "            ee.Image(cloud_prob_img).select('probability'),\n",
    "            ee.Image.constant(0).rename('probability')\n",
    "        ))\n",
    "        \n",
    "        # QA60 cloud mask\n",
    "        qa = img.select('QA60')\n",
    "        cloud_bit_mask = 1 << 10\n",
    "        cirrus_bit_mask = 1 << 11\n",
    "        qa_clear = qa.bitwiseAnd(cloud_bit_mask).eq(0).And(\n",
    "                   qa.bitwiseAnd(cirrus_bit_mask).eq(0))\n",
    "        \n",
    "        # Combined cloud mask: QA60 clear AND s2cloudless < threshold\n",
    "        s2cloudless_clear = cloud_prob.lt(S2_CLOUD_PROB_THRESHOLD)\n",
    "        combined_clear = qa_clear.And(s2cloudless_clear)\n",
    "        \n",
    "        # NDSI with cloud mask applied\n",
    "        ndsi = img.normalizedDifference(['B3', 'B11']).rename('ndsi')\n",
    "        ndsi_masked = ndsi.updateMask(combined_clear)\n",
    "        \n",
    "        # Binary ice mask (1 where NDSI > threshold, 0 otherwise)\n",
    "        # This gets averaged to give ice_fraction\n",
    "        ice_binary = ndsi_masked.gt(S2_NDSI_THRESHOLD).rename('ice_binary')\n",
    "        \n",
    "        # Cloud mask as 0/1 for computing cloud percentage\n",
    "        cloud_mask = combined_clear.Not().rename('is_cloud')\n",
    "        \n",
    "        return img.addBands(ndsi_masked).addBands(ice_binary).addBands(cloud_mask)\n",
    "    \n",
    "    s2 = s2.map(add_cloud_and_ndsi_bands)\n",
    "    \n",
    "    def extract_s2_features(img):\n",
    "        \"\"\"Extract lake-level statistics from each S2 image\"\"\"\n",
    "        \n",
    "        date = img.date()\n",
    "        \n",
    "        # Reduce NDSI over lake interiors (mean of continuous values)\n",
    "        ndsi_stats = img.select('ndsi').reduceRegions(\n",
    "            collection=lakes_fc.select(['lake_id', 'lake_interior']),\n",
    "            reducer=ee.Reducer.mean().setOutputs(['ndsi_mean']),\n",
    "            scale=20\n",
    "        )\n",
    "        \n",
    "        # Reduce ice_binary over lake interiors (mean = fraction of ice pixels)\n",
    "        ice_stats = img.select('ice_binary').reduceRegions(\n",
    "            collection=lakes_fc.select(['lake_id', 'lake_interior']),\n",
    "            reducer=ee.Reducer.mean().setOutputs(['ice_fraction']),\n",
    "            scale=20\n",
    "        )\n",
    "        \n",
    "        # Reduce cloud mask to get cloud percentage\n",
    "        cloud_stats = img.select('is_cloud').reduceRegions(\n",
    "            collection=lakes_fc.select(['lake_id', 'lake_interior']),\n",
    "            reducer=ee.Reducer.mean().setOutputs(['cloud_fraction']),\n",
    "            scale=20\n",
    "        )\n",
    "        \n",
    "        def create_s2_feature(ndsi_feat):\n",
    "            lake_id = ndsi_feat.get('lake_id')\n",
    "            \n",
    "            # Get matching ice and cloud stats\n",
    "            ice_feat = ice_stats.filter(ee.Filter.eq('lake_id', lake_id)).first()\n",
    "            cloud_feat = cloud_stats.filter(ee.Filter.eq('lake_id', lake_id)).first()\n",
    "            \n",
    "            # Get values with null handling\n",
    "            ndsi_mean = ndsi_feat.get('ndsi_mean')\n",
    "            ice_fraction = ice_feat.get('ice_fraction')\n",
    "            cloud_fraction = cloud_feat.get('cloud_fraction')\n",
    "            \n",
    "            # Convert cloud fraction to percentage (0-100)\n",
    "            cloud_pct = ee.Algorithms.If(\n",
    "                cloud_fraction,\n",
    "                ee.Number(cloud_fraction).multiply(100),\n",
    "                ee.Number(-1)  # Flag for missing data\n",
    "            )\n",
    "            \n",
    "            # Handle nulls: if ndsi_mean is null, set ice_fraction to null too\n",
    "            # This happens when all pixels are masked by clouds\n",
    "            ice_fraction_safe = ee.Algorithms.If(\n",
    "                ndsi_mean,\n",
    "                ice_fraction,\n",
    "                None\n",
    "            )\n",
    "            \n",
    "            return ee.Feature(None, {\n",
    "                'lake_id': lake_id,\n",
    "                's2_date': date.format('YYYY-MM-dd'),\n",
    "                's2_doy': date.getRelative('day', 'year'),\n",
    "                's2_ndsi_mean': ndsi_mean,\n",
    "                's2_ice_fraction': ice_fraction_safe,\n",
    "                's2_cloud_pct': cloud_pct\n",
    "            })\n",
    "        \n",
    "        return ndsi_stats.map(create_s2_feature)\n",
    "    \n",
    "    s2_features = s2.map(extract_s2_features).flatten()\n",
    "    \n",
    "    # Filter out features where all data is null (completely cloudy)\n",
    "    # This reduces export size and avoids null-heavy CSVs\n",
    "    s2_features = s2_features.filter(ee.Filter.notNull(['s2_ndsi_mean']))\n",
    "    \n",
    "    return s2_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ERA5 Temperature Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_era5_temperature(lakes_fc, year):\n",
    "    \"\"\"\n",
    "    Process ERA5-Land temperature data - OPTIMIZED VERSION\n",
    "    Pre-computes daily means once, then samples all lakes in batch\n",
    "    Much faster than computing daily mean separately for each lake\n",
    "    \"\"\"\n",
    "    print(f\"  Loading ERA5 hourly data for {year}...\")\n",
    "    \n",
    "    # Load ERA5-Land hourly data\n",
    "    era5_hourly = (ee.ImageCollection('ECMWF/ERA5_LAND/HOURLY')\n",
    "                   .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "                   .select('temperature_2m'))\n",
    "    \n",
    "    # Convert to Celsius\n",
    "    def to_celsius(img):\n",
    "        temp_c = img.subtract(273.15).rename('temp_c')\n",
    "        return temp_c.copyProperties(img, ['system:time_start'])\n",
    "    \n",
    "    era5_hourly = era5_hourly.map(to_celsius)\n",
    "    \n",
    "    print(f\"  Pre-computing daily means...\")\n",
    "    \n",
    "    # Determine number of days in year (handle leap years)\n",
    "    is_leap = ee.Number(year).mod(4).eq(0).And(\n",
    "        ee.Number(year).mod(100).neq(0).Or(\n",
    "            ee.Number(year).mod(400).eq(0)\n",
    "        )\n",
    "    )\n",
    "    n_days = ee.Number(ee.Algorithms.If(is_leap, 366, 365))\n",
    "    \n",
    "    # Pre-compute daily means for entire year (365 or 366 images)\n",
    "    def compute_daily_mean(day):\n",
    "        day = ee.Number(day)\n",
    "        date = ee.Date.fromYMD(year, 1, 1).advance(day.subtract(1), 'day')\n",
    "        next_date = date.advance(1, 'day')\n",
    "        \n",
    "        # Get all hourly images for this day\n",
    "        daily_collection = era5_hourly.filterDate(date, next_date)\n",
    "        \n",
    "        # Check if we have data\n",
    "        has_data = daily_collection.size().gt(0)\n",
    "        \n",
    "        # Compute mean if data exists, otherwise use missing flag\n",
    "        daily_mean = ee.Image(ee.Algorithms.If(\n",
    "            has_data,\n",
    "            daily_collection.mean(),\n",
    "            ee.Image.constant(-9999).rename('temp_c')\n",
    "        ))\n",
    "        \n",
    "        return daily_mean.set({\n",
    "            'system:time_start': date.millis(),\n",
    "            'doy': day,\n",
    "            'date': date.format('YYYY-MM-dd')\n",
    "        })\n",
    "    \n",
    "    days = ee.List.sequence(1, n_days)\n",
    "    era5_daily = ee.ImageCollection.fromImages(days.map(compute_daily_mean))\n",
    "    \n",
    "    print(f\"  Sampling all lakes from daily means...\")\n",
    "    \n",
    "    # Now sample ALL lakes from each daily mean (batch operation)\n",
    "    def sample_all_lakes(daily_img):\n",
    "        doy = daily_img.get('doy')\n",
    "        date = daily_img.get('date')\n",
    "        \n",
    "        # Sample ALL lakes at once using reduceRegions\n",
    "        samples = daily_img.reduceRegions(\n",
    "            collection=lakes_fc,\n",
    "            reducer=ee.Reducer.first(),  # Get pixel value at centroid\n",
    "            scale=11000  # ERA5-Land resolution\n",
    "        )\n",
    "        \n",
    "        # Add date info to each sampled feature\n",
    "        def add_date_info(feat):\n",
    "            # Get the temperature value (from 'first' property created by reducer)\n",
    "            # Use ee.Algorithms.If to provide default if missing\n",
    "            temp_value = ee.Algorithms.If(\n",
    "                feat.propertyNames().contains('first'),\n",
    "                feat.get('first'),\n",
    "                -9999\n",
    "            )\n",
    "            \n",
    "            return feat.set({\n",
    "                'era5_date': date,\n",
    "                'era5_doy': doy,\n",
    "                'temp_c': temp_value\n",
    "            })\n",
    "        \n",
    "        return samples.map(add_date_info)\n",
    "    \n",
    "    # Process all daily images\n",
    "    era5_features = era5_daily.map(sample_all_lakes).flatten()\n",
    "    \n",
    "    return era5_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Show chunk statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks to process: 21\n",
      "Years to process: [2019, 2020, 2021]\n",
      "Total exports: 189 (chunks × years × 3 datasets)\n",
      "\n",
      "Chunk statistics:\n",
      "    chunk_id  n_lakes    lat_min    lat_max     lon_min     lon_max\n",
      "0          0     1659  69.003753  70.805772 -157.460685 -156.323643\n",
      "1          1     1402  69.003327  70.498668 -150.588202 -149.501159\n",
      "2          2     2172  69.000561  70.506201 -149.575529 -148.460811\n",
      "3          3     1002  69.002549  70.662131 -161.092265 -159.724530\n",
      "4          4     2243  70.139174  71.157997 -155.626734 -154.362639\n",
      "5          5      408  69.061209  70.123854 -144.857630 -141.021667\n",
      "6          6     1726  69.886316  70.909059 -153.572246 -152.259761\n",
      "7          7     1704  69.002644  70.811790 -158.909885 -157.856992\n",
      "8          8      753  69.019065  70.186629 -147.158786 -144.887360\n",
      "9          9      460  69.027426  70.075681 -163.578774 -162.004458\n",
      "10        10     1440  69.013772  70.328129 -153.976923 -152.575637\n",
      "11        11     1392  69.000974  70.218418 -155.661592 -154.104769\n",
      "12        12     1834  69.001176  70.475984 -151.662586 -150.520819\n",
      "13        13     2001  69.468849  70.951464 -156.328499 -155.325830\n",
      "14        14     2095  69.242415  70.875454 -154.562909 -153.490827\n",
      "15        15     1152  69.001200  70.319093 -148.591307 -147.158186\n",
      "16        16     2019  69.000146  70.582641 -152.675190 -151.585260\n",
      "17        17      897  69.008946  70.299875 -162.343774 -160.999169\n",
      "18        18     1493  69.020858  70.841981 -159.930147 -158.880509\n",
      "19        19     1987  69.118177  71.063040 -158.031454 -157.038279\n",
      "20        20     1269  70.430225  71.360948 -157.142680 -155.634957\n"
     ]
    }
   ],
   "source": [
    "# Load chunk statistics to know how many chunks we have\n",
    "chunk_stats = pd.read_csv(f'gs://{BUCKET}/{BASE_PATH}/processed/chunk_statistics.csv')\n",
    "n_chunks = len(chunk_stats)\n",
    "\n",
    "print(f\"Total chunks to process: {n_chunks}\")\n",
    "print(f\"Years to process: {YEARS}\")\n",
    "print(f\"Total exports: {n_chunks * len(YEARS) * 3} (chunks × years × 3 datasets)\")\n",
    "print(\"\\nChunk statistics:\")\n",
    "print(chunk_stats[['chunk_id', 'n_lakes', 'lat_min', 'lat_max', 'lon_min', 'lon_max']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ERA5 Temperature export (via xarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DOWNLOADING ERA5 TEMPERATURE DATA\n",
      "============================================================\n",
      "This will take 5-10 minutes for all chunks and years...\n",
      "\n",
      "Step 1: Loading lake locations by chunk...\n",
      "  Loaded 31,108 lakes across 21 chunks\n",
      "\n",
      "Step 2: Accessing ERA5 dataset...\n",
      "  Dataset opened\n",
      "\n",
      "Step 3: Selecting North Slope region and time period...\n",
      "  Selected: 0 lats x 97 lons, 26304 hours\n",
      "\n",
      "Step 4: Computing daily means (2-3 minutes)...\n",
      "  1096 daily means computed\n",
      "\n",
      "Step 5: Processing each chunk and exporting...\n",
      "  Chunk 00 (1659 lakes)... "
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[1;32m     71\u001b[0m     warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m     lake_temps \u001b[38;5;241m=\u001b[39m \u001b[43mds_daily_celsius\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatitude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlake_lats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlongitude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlake_lons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Convert to DataFrame\u001b[39;00m\n\u001b[1;32m     75\u001b[0m df_chunk \u001b[38;5;241m=\u001b[39m lake_temps\u001b[38;5;241m.\u001b[39mto_dataframe(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp_c\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[0;32m/opt/conda/envs/gee/lib/python3.10/site-packages/xarray/core/dataarray.py:2361\u001b[0m, in \u001b[0;36mDataArray.interp\u001b[0;34m(self, coords, method, assume_sorted, kwargs, **coords_kwargs)\u001b[0m\n\u001b[1;32m   2357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muifc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   2359\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterp only works for a numeric type array. Given \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2360\u001b[0m     )\n\u001b[0;32m-> 2361\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_temp_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2365\u001b[0m \u001b[43m    \u001b[49m\u001b[43massume_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massume_sorted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2366\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcoords_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2367\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2368\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_temp_dataset(ds)\n",
      "File \u001b[0;32m/opt/conda/envs/gee/lib/python3.10/site-packages/xarray/core/dataset.py:3853\u001b[0m, in \u001b[0;36mDataset.interp\u001b[0;34m(self, coords, method, assume_sorted, kwargs, method_non_numeric, **coords_kwargs)\u001b[0m\n\u001b[1;32m   3850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_kind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muifc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3851\u001b[0m     \u001b[38;5;66;03m# For normal number types do the interpolation:\u001b[39;00m\n\u001b[1;32m   3852\u001b[0m     var_indexers \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m use_indexers\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m var\u001b[38;5;241m.\u001b[39mdims}\n\u001b[0;32m-> 3853\u001b[0m     variables[name] \u001b[38;5;241m=\u001b[39m \u001b[43mmissing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype_kind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObU\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (use_indexers\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m&\u001b[39m var\u001b[38;5;241m.\u001b[39mdims):\n\u001b[1;32m   3855\u001b[0m     \u001b[38;5;66;03m# For types that we do not understand do stepwise\u001b[39;00m\n\u001b[1;32m   3856\u001b[0m     \u001b[38;5;66;03m# interpolation to avoid modifying the elements.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3859\u001b[0m     \u001b[38;5;66;03m# this loop there might be some duplicate code that slows it\u001b[39;00m\n\u001b[1;32m   3860\u001b[0m     \u001b[38;5;66;03m# down, therefore collect these signals and run it later:\u001b[39;00m\n\u001b[1;32m   3861\u001b[0m     reindex_vars\u001b[38;5;241m.\u001b[39mappend(name)\n",
      "File \u001b[0;32m/opt/conda/envs/gee/lib/python3.10/site-packages/xarray/core/missing.py:667\u001b[0m, in \u001b[0;36minterp\u001b[0;34m(var, indexes_coords, method, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m broadcast_dims \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m var\u001b[38;5;241m.\u001b[39mdims \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dims]\n\u001b[1;32m    666\u001b[0m original_dims \u001b[38;5;241m=\u001b[39m broadcast_dims \u001b[38;5;241m+\u001b[39m dims\n\u001b[0;32m--> 667\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43minterpolate_variable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moriginal_dims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mindep_indexes_coords\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;66;03m# dimension of the output array\u001b[39;00m\n\u001b[1;32m    675\u001b[0m out_dims: OrderedSet \u001b[38;5;241m=\u001b[39m OrderedSet()\n",
      "File \u001b[0;32m/opt/conda/envs/gee/lib/python3.10/site-packages/xarray/core/missing.py:740\u001b[0m, in \u001b[0;36minterpolate_variable\u001b[0;34m(var, indexes_coords, method, kwargs)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# scipy.interpolate.interp1d always forces to float.\u001b[39;00m\n\u001b[1;32m    739\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(var\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, np\u001b[38;5;241m.\u001b[39minexact) \u001b[38;5;28;01melse\u001b[39;00m var\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m--> 740\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mapply_ufunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_interpnd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43min_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_core_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_core_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_core_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43moutput_core_dims\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_in_core_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallelized\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterp_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterp_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# we leave broadcasting up to dask if possible\u001b[39;49;00m\n\u001b[1;32m    753\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# but we need broadcasted values in _interpnd, so propagate that\u001b[39;49;00m\n\u001b[1;32m    754\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# context (dimension names), and broadcast there\u001b[39;49;00m\n\u001b[1;32m    755\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# This would be unnecessary if we could tell apply_ufunc\u001b[39;49;00m\n\u001b[1;32m    756\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# to insert size-1 broadcast dimensions\u001b[39;49;00m\n\u001b[1;32m    757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult_coord_core_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_core_dims\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult_coords\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TODO: deprecate and have the user rechunk themselves\u001b[39;49;00m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdask_gufunc_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_rechunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvectorize_dims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/envs/gee/lib/python3.10/site-packages/xarray/computation/apply_ufunc.py:1273\u001b[0m, in \u001b[0;36mapply_ufunc\u001b[0;34m(func, input_core_dims, output_core_dims, exclude_dims, vectorize, join, dataset_join, dataset_fill_value, keep_attrs, kwargs, dask, output_dtypes, output_sizes, meta, dask_gufunc_kwargs, on_missing_core_dim, *args)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;66;03m# feed Variables directly through apply_variable_ufunc\u001b[39;00m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(a, Variable) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args):\n\u001b[0;32m-> 1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvariables_vfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1275\u001b[0m     \u001b[38;5;66;03m# feed anything else through apply_array_ufunc\u001b[39;00m\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m apply_array_ufunc(func, \u001b[38;5;241m*\u001b[39margs, dask\u001b[38;5;241m=\u001b[39mdask)\n",
      "File \u001b[0;32m/opt/conda/envs/gee/lib/python3.10/site-packages/xarray/computation/apply_ufunc.py:814\u001b[0m, in \u001b[0;36mapply_variable_ufunc\u001b[0;34m(func, signature, exclude_dims, dask, output_dtypes, vectorize, keep_attrs, dask_gufunc_kwargs, *args)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m vectorize:\n\u001b[1;32m    810\u001b[0m     func \u001b[38;5;241m=\u001b[39m _vectorize(\n\u001b[1;32m    811\u001b[0m         func, signature, output_dtypes\u001b[38;5;241m=\u001b[39moutput_dtypes, exclude_dims\u001b[38;5;241m=\u001b[39mexclude_dims\n\u001b[1;32m    812\u001b[0m     )\n\u001b[0;32m--> 814\u001b[0m result_data \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m signature\u001b[38;5;241m.\u001b[39mnum_outputs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    817\u001b[0m     result_data \u001b[38;5;241m=\u001b[39m (result_data,)\n",
      "File \u001b[0;32m/opt/conda/envs/gee/lib/python3.10/site-packages/xarray/computation/apply_ufunc.py:792\u001b[0m, in \u001b[0;36mapply_variable_ufunc.<locals>.func\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunc\u001b[39m(\u001b[38;5;241m*\u001b[39marrays):\n\u001b[0;32m--> 792\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mchunkmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gufunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumpy_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m        \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_gufunc_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_dims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdask_gufunc_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m/opt/conda/envs/gee/lib/python3.10/site-packages/xarray/namedarray/daskmanager.py:159\u001b[0m, in \u001b[0;36mDaskManager.apply_gufunc\u001b[0;34m(self, func, signature, axes, axis, keepdims, output_dtypes, output_sizes, vectorize, allow_rechunk, meta, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_gufunc\u001b[39m(\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    144\u001b[0m     func: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    156\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgufunc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m apply_gufunc\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_gufunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_rechunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_rechunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/gee/lib/python3.10/site-packages/dask/array/gufunc.py:462\u001b[0m, in \u001b[0;36mapply_gufunc\u001b[0;34m(func, signature, axes, axis, keepdims, output_dtypes, output_sizes, vectorize, allow_rechunk, meta, *args, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dims):\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loop_output_dims:\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;66;03m# This will be rechunked to -1\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m         factor \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(a\u001b[38;5;241m.\u001b[39mchunks[i]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Dimensions that we can reduce to compensate for the increase\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         changeable_dimensions\u001b[38;5;241m.\u001b[39madd(i)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ERA5 Temperature Export (via xarray) - ROBUST VERSION\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DOWNLOADING ERA5 TEMPERATURE DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "era5_start = time.time()\n",
    "\n",
    "# Load chunk info\n",
    "print(\"Step 1: Loading lake locations by chunk...\")\n",
    "chunks_info = []\n",
    "\n",
    "for chunk_id in range(n_chunks):\n",
    "    chunk_file = f'{CHUNKS_PATH}/chunk_{chunk_id:02d}.geojson'\n",
    "    chunk_gdf = gpd.read_file(chunk_file)\n",
    "    chunk_gdf['lake_id'] = range(len(chunk_gdf))\n",
    "    \n",
    "    # Keep original -180:180 coordinates\n",
    "    chunks_info.append({\n",
    "        'chunk_id': chunk_id,\n",
    "        'gdf': chunk_gdf[['lake_id', 'centroid_lon', 'centroid_lat']],\n",
    "        'n_lakes': len(chunk_gdf)\n",
    "    })\n",
    "\n",
    "total_lakes = sum(c['n_lakes'] for c in chunks_info)\n",
    "print(f\"  Loaded {total_lakes:,} lakes across {len(chunks_info)} chunks\\n\")\n",
    "\n",
    "# Download ERA5\n",
    "print(\"Step 2: Accessing ERA5 dataset...\")\n",
    "ERA5_URL = 'gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3'\n",
    "ds = xr.open_zarr(ERA5_URL, chunks={'time': 100}, consolidated=True)\n",
    "\n",
    "# Check coordinate system\n",
    "print(\"  Checking coordinates...\")\n",
    "lat_min, lat_max = float(ds.latitude.min()), float(ds.latitude.max())\n",
    "lon_min, lon_max = float(ds.longitude.min()), float(ds.longitude.max())\n",
    "print(f\"  Latitude: {lat_min:.1f} to {lat_max:.1f}\")\n",
    "print(f\"  Longitude: {lon_min:.1f} to {lon_max:.1f}\\n\")\n",
    "\n",
    "# Convert Alaska longitudes if needed\n",
    "alaska_lon_min, alaska_lon_max = -164, -140\n",
    "if lon_min >= 0:  # Dataset uses 0-360\n",
    "    alaska_lon_min = alaska_lon_min % 360  # 196\n",
    "    alaska_lon_max = alaska_lon_max % 360  # 220\n",
    "    print(f\"  Converting to 0-360: Alaska is {alaska_lon_min}-{alaska_lon_max}E\\n\")\n",
    "\n",
    "print(\"Step 3: Selecting North Slope region...\")\n",
    "# Select with proper ordering\n",
    "if ds.latitude[0] > ds.latitude[-1]:  # Descending\n",
    "    lat_slice = slice(72, 69)\n",
    "else:  # Ascending\n",
    "    lat_slice = slice(69, 72)\n",
    "\n",
    "ds_subset = ds['2m_temperature'].sel(\n",
    "    latitude=lat_slice,\n",
    "    longitude=slice(alaska_lon_min, alaska_lon_max),\n",
    "    time=slice(f'{YEARS[0]}-01-01', f'{YEARS[-1]}-12-31')\n",
    ")\n",
    "\n",
    "print(f\"  Selected: {len(ds_subset.latitude)} lats x {len(ds_subset.longitude)} lons x {len(ds_subset.time)} hours\\n\")\n",
    "\n",
    "if len(ds_subset.latitude) == 0 or len(ds_subset.longitude) == 0:\n",
    "    raise ValueError(\"Selection returned empty dataset - check coordinate ranges\")\n",
    "\n",
    "print(\"Step 4: Computing daily means...\")\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    # Compute and load into memory (avoids dask chunking issues)\n",
    "    ds_daily = ds_subset.resample(time='1D').mean().compute()\n",
    "    ds_daily_celsius = ds_daily - 273.15\n",
    "\n",
    "print(f\"  {len(ds_daily_celsius.time)} daily means computed\\n\")\n",
    "\n",
    "print(\"Step 5: Processing each chunk...\")\n",
    "\n",
    "for chunk_info in chunks_info:\n",
    "    chunk_id = chunk_info['chunk_id']\n",
    "    chunk_gdf = chunk_info['gdf']\n",
    "    n_lakes = chunk_info['n_lakes']\n",
    "    \n",
    "    print(f\"  Chunk {chunk_id:02d} ({n_lakes} lakes)...\", end=' ')\n",
    "    \n",
    "    # Convert lake coordinates if needed\n",
    "    lake_lons = chunk_gdf['centroid_lon'].values\n",
    "    if alaska_lon_min > 180:  # Dataset uses 0-360\n",
    "        lake_lons = lake_lons % 360\n",
    "    \n",
    "    lake_lats = xr.DataArray(chunk_gdf['centroid_lat'].values, dims=['lake'])\n",
    "    lake_lons_da = xr.DataArray(lake_lons, dims=['lake'])\n",
    "    \n",
    "    # Interpolate\n",
    "    lake_temps = ds_daily_celsius.interp(\n",
    "        latitude=lake_lats,\n",
    "        longitude=lake_lons_da,\n",
    "        method='linear'\n",
    "    )\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_chunk = lake_temps.to_dataframe(name='temp_c').reset_index()\n",
    "    df_chunk['lake_id'] = df_chunk['lake']\n",
    "    df_chunk['era5_date'] = df_chunk['time'].dt.strftime('%Y-%m-%d')\n",
    "    df_chunk['era5_doy'] = df_chunk['time'].dt.dayofyear\n",
    "    df_chunk['year'] = df_chunk['time'].dt.year\n",
    "    df_chunk = df_chunk[['lake_id', 'era5_date', 'era5_doy', 'temp_c', 'year']]\n",
    "    \n",
    "    # Export by year\n",
    "    for year in YEARS:\n",
    "        df_year = df_chunk[df_chunk['year'] == year].copy()\n",
    "        df_year = df_year.drop(columns=['year']).sort_values(['lake_id', 'era5_doy'])\n",
    "        output_file = f'gs://{BUCKET}/{OUTPUT_PATH}/{year}/chunk_{chunk_id:02d}/era5_data.csv'\n",
    "        df_year.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(\"Done\")\n",
    "\n",
    "era5_time = time.time() - era5_start\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ERA5 COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Time: {era5_time/60:.1f} minutes\")\n",
    "print(f\"Files: {len(chunks_info) * len(YEARS)} CSV files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## S1/S2 exports (via GEE)\n",
    "\n",
    "**WARNING:** This will prep ~126 export tasks (21 chunks × 3 years × 2 datasets each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing Chunk 0\n",
      "============================================================\n",
      "  1659 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019...   Loading ERA5 hourly data for 2019...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2020...   Loading ERA5 hourly data for 2020...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2021...   Loading ERA5 hourly data for 2021...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Chunk 0 complete (1.3s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 1\n",
      "============================================================\n",
      "  1402 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019...   Loading ERA5 hourly data for 2019...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2020...   Loading ERA5 hourly data for 2020...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2021...   Loading ERA5 hourly data for 2021...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Chunk 1 complete (1.7s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 2\n",
      "============================================================\n",
      "  2172 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019...   Loading ERA5 hourly data for 2019...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2020...   Loading ERA5 hourly data for 2020...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2021...   Loading ERA5 hourly data for 2021...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Chunk 2 complete (3.9s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 3\n",
      "============================================================\n",
      "  1002 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019...   Loading ERA5 hourly data for 2019...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2020...   Loading ERA5 hourly data for 2020...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2021...   Loading ERA5 hourly data for 2021...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Chunk 3 complete (1.2s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 4\n",
      "============================================================\n",
      "  2243 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019...   Loading ERA5 hourly data for 2019...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2020...   Loading ERA5 hourly data for 2020...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2021...   Loading ERA5 hourly data for 2021...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Chunk 4 complete (3.5s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 5\n",
      "============================================================\n",
      "  408 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019...   Loading ERA5 hourly data for 2019...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2020...   Loading ERA5 hourly data for 2020...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2021...   Loading ERA5 hourly data for 2021...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Chunk 5 complete (0.7s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 6\n",
      "============================================================\n",
      "  1726 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019...   Loading ERA5 hourly data for 2019...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2020...   Loading ERA5 hourly data for 2020...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2021...   Loading ERA5 hourly data for 2021...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Chunk 6 complete (4.6s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 7\n",
      "============================================================\n",
      "  1704 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019...   Loading ERA5 hourly data for 2019...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2020...   Loading ERA5 hourly data for 2020...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2021...   Loading ERA5 hourly data for 2021...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Chunk 7 complete (2.1s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 8\n",
      "============================================================\n",
      "  753 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019...   Loading ERA5 hourly data for 2019...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2020...   Loading ERA5 hourly data for 2020...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2021...   Loading ERA5 hourly data for 2021...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Chunk 8 complete (1.2s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 9\n",
      "============================================================\n",
      "  460 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019...   Loading ERA5 hourly data for 2019...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2020...   Loading ERA5 hourly data for 2020...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2021...   Loading ERA5 hourly data for 2021...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Chunk 9 complete (0.9s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 10\n",
      "============================================================\n",
      "  1440 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019...   Loading ERA5 hourly data for 2019...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2020...   Loading ERA5 hourly data for 2020...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2021...   Loading ERA5 hourly data for 2021...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Chunk 10 complete (1.8s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 11\n",
      "============================================================\n",
      "  1392 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019...   Loading ERA5 hourly data for 2019...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2020...   Loading ERA5 hourly data for 2020...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2021...   Loading ERA5 hourly data for 2021...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Chunk 11 complete (1.8s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 12\n",
      "============================================================\n",
      "  1834 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019...   Loading ERA5 hourly data for 2019...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2020...   Loading ERA5 hourly data for 2020...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2021...   Loading ERA5 hourly data for 2021...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Chunk 12 complete (4.0s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 13\n",
      "============================================================\n",
      "  2001 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019...   Loading ERA5 hourly data for 2019...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2020...   Loading ERA5 hourly data for 2020...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2021...   Loading ERA5 hourly data for 2021...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Chunk 13 complete (3.8s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 14\n",
      "============================================================\n",
      "  2095 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019...   Loading ERA5 hourly data for 2019...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2020...   Loading ERA5 hourly data for 2020...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2021...   Loading ERA5 hourly data for 2021...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Chunk 14 complete (3.3s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 15\n",
      "============================================================\n",
      "  1152 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019...   Loading ERA5 hourly data for 2019...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2020...   Loading ERA5 hourly data for 2020...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2021...   Loading ERA5 hourly data for 2021...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Chunk 15 complete (1.4s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 16\n",
      "============================================================\n",
      "  2019 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019...   Loading ERA5 hourly data for 2019...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2020...   Loading ERA5 hourly data for 2020...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2021...   Loading ERA5 hourly data for 2021...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Chunk 16 complete (6.5s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 17\n",
      "============================================================\n",
      "  897 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019...   Loading ERA5 hourly data for 2019...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2020...   Loading ERA5 hourly data for 2020...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2021...   Loading ERA5 hourly data for 2021...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Chunk 17 complete (1.6s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 18\n",
      "============================================================\n",
      "  1493 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019...   Loading ERA5 hourly data for 2019...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2020...   Loading ERA5 hourly data for 2020...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2021...   Loading ERA5 hourly data for 2021...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Chunk 18 complete (2.0s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 19\n",
      "============================================================\n",
      "  1987 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019...   Loading ERA5 hourly data for 2019...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2020...   Loading ERA5 hourly data for 2020...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2021...   Loading ERA5 hourly data for 2021...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Chunk 19 complete (8.1s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 20\n",
      "============================================================\n",
      "  1269 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019...   Loading ERA5 hourly data for 2019...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2020...   Loading ERA5 hourly data for 2020...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Preparing 2021...   Loading ERA5 hourly data for 2021...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "Done\n",
      "  Chunk 20 complete (2.0s)\n",
      "\n",
      "============================================================\n",
      "ALL EXPORTS PREPARED: 189 tasks\n",
      "============================================================\n",
      "Total preparation time: 1.0 minutes\n",
      "\n",
      "Ready to start. Run next cell to begin 189 exports.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Prep S1/S2 exports\n",
    "# NOTE: ERA5 already exported via xarray \n",
    "# ============================================================\n",
    "\n",
    "all_exports = []\n",
    "total_start = time.time()\n",
    "\n",
    "for chunk_id in range(n_chunks):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing Chunk {chunk_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    chunk_start = time.time()\n",
    "    \n",
    "    # Load chunk\n",
    "    chunk_fc, chunk_gdf = load_chunk_from_bucket(chunk_id)\n",
    "    chunk_bounds = chunk_fc.geometry().bounds()\n",
    "    \n",
    "    print(f\"  {len(chunk_gdf)} lakes\")\n",
    "    \n",
    "    # Add lake geometries\n",
    "    print(\"  Adding geometries...\")\n",
    "    fc_with_geom = add_lake_geometry_metrics(chunk_fc, chunk_bounds)\n",
    "    \n",
    "    for year in YEARS:\n",
    "        print(f\"  Preparing {year}...\", end=' ')\n",
    "        \n",
    "        # Process S1 and S2 only (ERA5 already done)\n",
    "        s1_features = process_sentinel1(fc_with_geom, year, chunk_bounds)\n",
    "        s2_features = process_sentinel2(fc_with_geom, year, chunk_bounds)\n",
    "        \n",
    "        # Create export tasks\n",
    "        s1_task = ee.batch.Export.table.toCloudStorage(\n",
    "            collection=s1_features,\n",
    "            description=f'S1_chunk{chunk_id:02d}_{year}',\n",
    "            bucket=BUCKET,\n",
    "            fileNamePrefix=f'{OUTPUT_PATH}/{year}/chunk_{chunk_id:02d}/s1_data',\n",
    "            fileFormat='CSV'\n",
    "        )\n",
    "        \n",
    "        s2_task = ee.batch.Export.table.toCloudStorage(\n",
    "            collection=s2_features,\n",
    "            description=f'S2_chunk{chunk_id:02d}_{year}',\n",
    "            bucket=BUCKET,\n",
    "            fileNamePrefix=f'{OUTPUT_PATH}/{year}/chunk_{chunk_id:02d}/s2_data',\n",
    "            fileFormat='CSV'\n",
    "        )\n",
    "        \n",
    "        all_exports.append({\n",
    "            'chunk_id': chunk_id,\n",
    "            'year': year,\n",
    "            'type': 'S1',\n",
    "            'task': s1_task\n",
    "        })\n",
    "        all_exports.append({\n",
    "            'chunk_id': chunk_id,\n",
    "            'year': year,\n",
    "            'type': 'S2',\n",
    "            'task': s2_task\n",
    "        })\n",
    "        \n",
    "        print(\"Done\")\n",
    "    \n",
    "    chunk_time = time.time() - chunk_start\n",
    "    print(f\"  Chunk {chunk_id} complete ({chunk_time:.1f}s)\")\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ALL EXPORTS PREPARED: {len(all_exports)} tasks\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total preparation time: {total_time/60:.1f} minutes\")\n",
    "print(f\"\\nReady to start. Run next cell to begin {len(all_exports)} exports.\")\n",
    "print(f\"(ERA5 already complete - {len(all_exports)} S1/S2 tasks only)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Started 10 tasks...\n",
      "  Started 20 tasks...\n",
      "  Started 30 tasks...\n",
      "  Started 40 tasks...\n",
      "  Started 50 tasks...\n",
      "  Started 60 tasks...\n",
      "  Started 70 tasks...\n",
      "  Started 80 tasks...\n",
      "  Started 90 tasks...\n",
      "  Started 100 tasks...\n",
      "  Started 110 tasks...\n",
      "  Started 120 tasks...\n",
      "  Started 130 tasks...\n",
      "  Started 140 tasks...\n",
      "  Started 150 tasks...\n",
      "  Started 160 tasks...\n",
      "  Started 170 tasks...\n",
      "  Started 180 tasks...\n",
      "\n",
      "Started 189 tasks\n",
      "Monitor at: https://code.earthengine.google.com/tasks\n"
     ]
    }
   ],
   "source": [
    "# Start all exports\n",
    "started = 0\n",
    "for i, export in enumerate(all_exports):\n",
    "    try:\n",
    "        task = export['task']\n",
    "        if task.status()['state'] == 'UNSUBMITTED':\n",
    "            task.start()\n",
    "            started += 1\n",
    "            \n",
    "            # Small delay every 10 tasks to avoid rate limiting\n",
    "            if started % 10 == 0:\n",
    "                print(f\"  Started {started} tasks...\")\n",
    "                time.sleep(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting task {i} ({export['type']} chunk {export['chunk_id']} {export['year']}): {e}\")\n",
    "\n",
    "print(f\"\\nStarted {started} tasks\")\n",
    "print(\"Monitor at: https://code.earthengine.google.com/tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Monitor GEE Export Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export Status Summary:\n",
      "  Total tasks: 189\n",
      "    READY: 189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'READY': 189, 'RUNNING': 0, 'COMPLETED': 0, 'FAILED': 0, 'CANCELLED': 0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check status of GEE exports\n",
    "def check_export_status():\n",
    "    status_summary = {\n",
    "        'READY': 0,\n",
    "        'RUNNING': 0,\n",
    "        'COMPLETED': 0,\n",
    "        'FAILED': 0,\n",
    "        'CANCELLED': 0\n",
    "    }\n",
    "    \n",
    "    for exp in all_exports:\n",
    "        status = exp['task'].status()['state']\n",
    "        status_summary[status] = status_summary.get(status, 0) + 1\n",
    "    \n",
    "    print(f\"Export Status Summary:\")\n",
    "    print(f\"  Total tasks: {len(all_exports)}\")\n",
    "    for state, count in status_summary.items():\n",
    "        if count > 0:\n",
    "            print(f\"    {state}: {count}\")\n",
    "    \n",
    "    return status_summary\n",
    "\n",
    "# Run this cell periodically to check progress\n",
    "check_export_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook exports:\n",
    "\n",
    "**Sentinel-1**: \n",
    "- Lake interior: VV, VH backscatter (dB), VV-VH ratio, RGB features (normalized 0-255)\n",
    "- Landscape ring: VV, VH backscatter (dB), VV-VH ratio, RGB features (normalized 0-255)\n",
    "- Both ASCENDING and DESCENDING orbits\n",
    "- Angle mask applied (25-50 degrees)\n",
    "\n",
    "**Sentinel-2**: \n",
    "- s2_ndsi_mean: Mean NDSI value across lake pixels (continuous, -1 to 1)\n",
    "- s2_ice_fraction: Fraction of lake pixels with NDSI > 0.4 (continuous, 0 to 1)\n",
    "- s2_cloud_pct: Percentage of lake masked by clouds (0-100, for QC filtering)\n",
    "- Dual cloud masking: QA60 + s2cloudless\n",
    "\n",
    "**ERA5**: \n",
    "- Daily mean 2m air temperature at lake centroids (Celsius)\n",
    "\n",
    "**For:**\n",
    "- 21 spatial chunks\n",
    "- 3 years (2019, 2020, 2021)\n",
    "- ~31,000 North Slope lakes (>0.02 km²)\n",
    "\n",
    "**Output structure:**\n",
    "```\n",
    "gs://wustl-eeps-geospatial/thermokarst_lakes/exports/\n",
    "├── 2019/\n",
    "│   ├── chunk_00/\n",
    "│   │   ├── s1_data.csv\n",
    "│   │   ├── s2_data.csv\n",
    "│   │   └── era5_data.csv\n",
    "│   ├── chunk_01/\n",
    "│   └── ...\n",
    "├── 2020/\n",
    "└── 2021/\n",
    "```\n",
    "\n",
    "**Next step:** Combine CSVs and run ice detection algorithm (Notebook 03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "gee",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "GEE Geospatial (Local)",
   "language": "python",
   "name": "gee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

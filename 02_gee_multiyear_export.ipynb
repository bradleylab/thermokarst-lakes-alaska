{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Year Lake Ice Phenology Data Export\n",
    "## Part 2: GEE and xarray Processing and Export (2019-2023)\n",
    "\n",
    "**Goal:** Export S1 + S2 + ERA5 data for North Slope lakes across 5 years\n",
    "\n",
    "**Strategy:**\n",
    "- Process chunks independently (spatial parallelization)\n",
    "- Export one year at a time per chunk\n",
    "- Use efficient spatial filtering (only process S2 images that overlap lakes)\n",
    "- Export ERA5 via xarray (more efficient)\n",
    "- Total GEE exports: ~21 chunks × 5 years × 2 datasets (S1, S2) = ~210 exports\n",
    "\n",
    "\n",
    "**Data sources:**\n",
    "- ERA5-Land (temperature)\n",
    "- Sentinel-1 GRD (SAR)\n",
    "- Sentinel-2 SR Harmonized (optical, for NDSI)\n",
    "\n",
    "**Years:** 2019, 2020, 2021, 2022, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n",
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n",
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n",
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n",
      "Earth Engine initialized: GEE Initialized\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import xarray as xr\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Initialize Earth Engine\n",
    "ee.Initialize()\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"Earth Engine initialized: {ee.String('GEE Initialized').getInfo()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Years: [2019, 2020, 2021, 2022, 2023]\n",
      "  Chunks: 21\n",
      "  Output: gs://wustl-eeps-geospatial/thermokarst_lakes/exports\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BUCKET = 'wustl-eeps-geospatial'\n",
    "BASE_PATH = 'thermokarst_lakes'\n",
    "CHUNKS_PATH = f'gs://{BUCKET}/{BASE_PATH}/processed/chunks'\n",
    "OUTPUT_PATH = f'{BASE_PATH}/exports'\n",
    "GEE_ASSET_PATH = 'projects/eeps-geospatial/assets/lake_geometries'\n",
    "YEARS = [2019, 2020, 2021, 2022, 2023]\n",
    "n_chunks = 21\n",
    "\n",
    "# S2 thresholds\n",
    "S2_CLOUD_THRESHOLD = 30\n",
    "S2_CLOUD_PROB_THRESHOLD = 40\n",
    "S2_NDSI_THRESHOLD = 0.4\n",
    "\n",
    "# Buffer distances (meters)\n",
    "INTERIOR_BUFFER = -10  # negative = inward\n",
    "LANDSCAPE_BUFFER = 100  # outward\n",
    "\n",
    "# Configuration for GEE assets\n",
    "GEE_ASSET_PATH = 'projects/eeps-geospatial/assets/lake_geometries'\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Years: {YEARS}\")\n",
    "print(f\"  Chunks: {n_chunks}\")\n",
    "print(f\"  Output: gs://{BUCKET}/{OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunk_from_bucket(chunk_id):\n",
    "    \"\"\"\n",
    "    Load pre-computed interior and landscape geometries from GEE Assets.\n",
    "    No buffer computation needed - already done.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load pre-computed geometry assets (fast!)\n",
    "    interior_fc = ee.FeatureCollection(f'{GEE_ASSET_PATH}/chunk_{chunk_id:02d}_interior')\n",
    "    landscape_fc = ee.FeatureCollection(f'{GEE_ASSET_PATH}/chunk_{chunk_id:02d}_landscape')\n",
    "    \n",
    "    # Load GeoDataFrame for metadata\n",
    "    chunk_file = f'{CHUNKS_PATH}/chunk_{chunk_id:02d}.geojson'\n",
    "    chunk_gdf = gpd.read_file(chunk_file)\n",
    "    \n",
    "    # Get bounds in WGS84 for GEE (avoids GEE edge limit!)\n",
    "    chunk_gdf_wgs84 = chunk_gdf.to_crs('EPSG:4326')\n",
    "    bounds_array = chunk_gdf_wgs84.total_bounds  # [minx, miny, maxx, maxy] in lon/lat\n",
    "    bounds = ee.Geometry.Rectangle([\n",
    "        bounds_array[0], bounds_array[1], \n",
    "        bounds_array[2], bounds_array[3]\n",
    "    ])\n",
    "    \n",
    "    return interior_fc, landscape_fc, bounds, chunk_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lake_geometry_metrics(lakes_fc, region_bounds):\n",
    "    \"\"\"\n",
    "    Add lake interior and landscape ring geometries\n",
    "    Uses efficient spatial filtering - only masks lakes within 100m of each target lake\n",
    "    \"\"\"\n",
    "    # Load ALPOD from GEE Asset\n",
    "    all_alpod = ee.FeatureCollection('projects/eeps-geospatial/assets/ALPOD_full')\n",
    "    \n",
    "    def add_geometries(lake):\n",
    "        lake_geom = lake.geometry()\n",
    "        id = lake.get('id')\n",
    "        \n",
    "        # Lake interior: 10m inward buffer\n",
    "        lake_interior = lake_geom.buffer(-10)\n",
    "        \n",
    "        # Landscape ring: 100m outward buffer\n",
    "        ring_outer = lake_geom.buffer(100)\n",
    "        \n",
    "        # Only find lakes that intersect THIS lake's 100m buffer\n",
    "        nearby_lakes = all_alpod.filterBounds(ring_outer)\n",
    "        nearby_dissolved = nearby_lakes.geometry().dissolve(maxError=10)\n",
    "        \n",
    "        # Subtract only the nearby lakes\n",
    "        landscape_ring = ring_outer.difference(nearby_dissolved)\n",
    "        \n",
    "        return lake.set({\n",
    "            'id': id,\n",
    "            'lake_interior': lake_interior,\n",
    "            'landscape_ring': landscape_ring\n",
    "        })\n",
    "    \n",
    "    return lakes_fc.map(add_geometries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sentinel-1 Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentinel1(interior_fc, landscape_fc, year, region_bounds):\n",
    "    \"\"\"\n",
    "    Extract S1 features using pre-computed interior and landscape FCs.\n",
    "    \"\"\"\n",
    "    \n",
    "    s1 = (ee.ImageCollection('COPERNICUS/S1_GRD')\n",
    "          .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "          .filterBounds(region_bounds)\n",
    "          .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
    "          .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
    "          .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n",
    "          .filter(ee.Filter.eq('resolution_meters', 10)))\n",
    "    \n",
    "    def apply_angle_mask(img):\n",
    "        angle = img.select('angle')\n",
    "        angle_mask = angle.gt(25).And(angle.lt(50))\n",
    "        return img.select(['VV', 'VH']).updateMask(angle_mask).copyProperties(img, img.propertyNames())\n",
    "    \n",
    "    s1 = s1.map(apply_angle_mask)\n",
    "    \n",
    "    def extract_s1_features(img):\n",
    "        vv_img = img.select('VV')\n",
    "        vh_img = img.select('VH')\n",
    "        vv_vh_img = vv_img.subtract(vh_img).rename('VV_VH')\n",
    "        r_band = vv_img.unitScale(-20, -5).multiply(255).rename('R')\n",
    "        g_band = vh_img.unitScale(-28, -12).multiply(255).rename('G')\n",
    "        b_band = vv_vh_img.unitScale(8, 18).multiply(255).rename('B')\n",
    "        all_bands = vv_img.addBands(vh_img).addBands(vv_vh_img).addBands(r_band).addBands(g_band).addBands(b_band)\n",
    "        \n",
    "        date = img.date()\n",
    "        orbit = img.get('orbitProperties_pass')\n",
    "        \n",
    "        # Sample both geometries\n",
    "        lake_samples = all_bands.reduceRegions(\n",
    "            collection=interior_fc,\n",
    "            reducer=ee.Reducer.mean(),\n",
    "            scale=10\n",
    "        )\n",
    "        \n",
    "        land_samples = all_bands.reduceRegions(\n",
    "            collection=landscape_fc,\n",
    "            reducer=ee.Reducer.mean(),\n",
    "            scale=10\n",
    "        )\n",
    "        \n",
    "        # Join lake and landscape samples by lake id\n",
    "        join_filter = ee.Filter.equals(leftField='id', rightField='id')\n",
    "        joined = ee.Join.inner('lake', 'land').apply(lake_samples, land_samples, join_filter)\n",
    "        \n",
    "        def create_s1_feature(joined_feat):\n",
    "            lake_feat = ee.Feature(joined_feat.get('lake'))\n",
    "            land_feat = ee.Feature(joined_feat.get('land'))\n",
    "            \n",
    "            return ee.Feature(None, {\n",
    "                'id': lake_feat.get('id'),\n",
    "                's1_date': date.format('YYYY-MM-dd'),\n",
    "                's1_doy': date.getRelative('day', 'year'),\n",
    "                's1_orbit': orbit,\n",
    "                'lake_vv_db': lake_feat.get('VV'),\n",
    "                'lake_vh_db': lake_feat.get('VH'),\n",
    "                'lake_vv_vh_db': lake_feat.get('VV_VH'),\n",
    "                'lake_r': lake_feat.get('R'),\n",
    "                'lake_g': lake_feat.get('G'),\n",
    "                'lake_b': lake_feat.get('B'),\n",
    "                'land_vv_db': land_feat.get('VV'),\n",
    "                'land_vh_db': land_feat.get('VH'),\n",
    "                'land_vv_vh_db': land_feat.get('VV_VH'),\n",
    "                'land_r': land_feat.get('R'),\n",
    "                'land_g': land_feat.get('G'),\n",
    "                'land_b': land_feat.get('B')\n",
    "            })\n",
    "        \n",
    "        return joined.map(create_s1_feature)\n",
    "    \n",
    "    s1_features = s1.map(extract_s1_features).flatten()\n",
    "    s1_features = s1_features.filter(ee.Filter.notNull(['lake_vv_db']))\n",
    "    \n",
    "    return s1_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sentinel-2 Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ndsi(img):\n",
    "    \"\"\"\n",
    "    Compute Normalized Difference Snow Index (NDSI)\n",
    "    NDSI = (Green - SWIR1) / (Green + SWIR1)\n",
    "    \"\"\"\n",
    "    green = img.select('B3')\n",
    "    swir1 = img.select('B11')\n",
    "    \n",
    "    ndsi = green.subtract(swir1).divide(green.add(swir1)).rename('ndsi')\n",
    "    \n",
    "    return img.addBands(ndsi)\n",
    "\n",
    "def mask_s2_clouds(img):\n",
    "    \"\"\"\n",
    "    Mask clouds using QA60 band (basic cloud mask)\n",
    "    \"\"\"\n",
    "    qa = img.select('QA60')\n",
    "    \n",
    "    # Bits 10 and 11 are clouds and cirrus\n",
    "    cloud_bit_mask = 1 << 10\n",
    "    cirrus_bit_mask = 1 << 11\n",
    "    \n",
    "    # Both should be zero (clear conditions)\n",
    "    mask = qa.bitwiseAnd(cloud_bit_mask).eq(0).And(\n",
    "           qa.bitwiseAnd(cirrus_bit_mask).eq(0))\n",
    "    \n",
    "    return img.updateMask(mask)\n",
    "\n",
    "def add_s2cloudless_mask(img):\n",
    "    s2_cloudless = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "    cloud_prob_collection = s2_cloudless.filter(\n",
    "        ee.Filter.eq('system:index', img.get('system:index'))\n",
    "    )\n",
    "    \n",
    "    has_cloud_data = cloud_prob_collection.size().gt(0)\n",
    "    \n",
    "    def apply_s2cloudless_mask():\n",
    "        cloud_prob = cloud_prob_collection.first().select('probability')\n",
    "        is_clear = cloud_prob.lt(S2_CLOUD_PROB_THRESHOLD)\n",
    "        return img.updateMask(is_clear)\n",
    "    \n",
    "    def use_qa60_only():\n",
    "        return img  # Already has QA60 mask\n",
    "    \n",
    "    return ee.Image(ee.Algorithms.If(\n",
    "        has_cloud_data,\n",
    "        apply_s2cloudless_mask(),\n",
    "        use_qa60_only()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentinel2(interior_fc, year, region_bounds):\n",
    "    \"\"\"\n",
    "    Extract S2 features using pre-computed interior FC.\n",
    "    \"\"\"\n",
    "    \n",
    "    s2 = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "          .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "          .filterBounds(region_bounds)\n",
    "          .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', S2_CLOUD_THRESHOLD)))\n",
    "    \n",
    "    s2_cloudless = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "                    .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "                    .filterBounds(region_bounds))\n",
    "    \n",
    "    def add_cloud_and_ndsi_bands(img):\n",
    "        img_date = img.date()\n",
    "        cloud_prob_img = s2_cloudless.filterDate(\n",
    "            img_date, img_date.advance(1, 'day')\n",
    "        ).first()\n",
    "        \n",
    "        cloud_prob = ee.Image(ee.Algorithms.If(\n",
    "            cloud_prob_img,\n",
    "            ee.Image(cloud_prob_img).select('probability'),\n",
    "            ee.Image.constant(0).rename('probability')\n",
    "        ))\n",
    "        \n",
    "        qa = img.select('QA60')\n",
    "        cloud_bit_mask = 1 << 10\n",
    "        cirrus_bit_mask = 1 << 11\n",
    "        qa_clear = qa.bitwiseAnd(cloud_bit_mask).eq(0).And(\n",
    "                   qa.bitwiseAnd(cirrus_bit_mask).eq(0))\n",
    "        \n",
    "        s2cloudless_clear = cloud_prob.lt(S2_CLOUD_PROB_THRESHOLD)\n",
    "        combined_clear = qa_clear.And(s2cloudless_clear)\n",
    "        \n",
    "        ndsi = img.normalizedDifference(['B3', 'B11']).rename('ndsi')\n",
    "        ndsi_masked = ndsi.updateMask(combined_clear)\n",
    "        ice_binary = ndsi_masked.gt(S2_NDSI_THRESHOLD).rename('ice_binary')\n",
    "        cloud_mask = combined_clear.Not().rename('is_cloud')\n",
    "        \n",
    "        return img.addBands(ndsi_masked).addBands(ice_binary).addBands(cloud_mask)\n",
    "    \n",
    "    s2 = s2.map(add_cloud_and_ndsi_bands)\n",
    "    \n",
    "    def extract_s2_features(img):\n",
    "        date = img.date()\n",
    "        \n",
    "        # Stack bands for single reduceRegions call\n",
    "        bands = img.select(['ndsi', 'ice_binary', 'is_cloud'])\n",
    "        \n",
    "        # Combined reducer: mean + count + stdDev\n",
    "        reducer = ee.Reducer.mean().combine(\n",
    "            ee.Reducer.count(), '', True\n",
    "        ).combine(\n",
    "            ee.Reducer.stdDev(), '', True\n",
    "        )\n",
    "        \n",
    "        samples = bands.reduceRegions(\n",
    "            collection=interior_fc,\n",
    "            reducer=reducer,\n",
    "            scale=20\n",
    "        )\n",
    "        \n",
    "        def create_s2_feature(feat):\n",
    "            # Get all metrics with safe null handling\n",
    "            ndsi_mean = feat.get('ndsi')\n",
    "            ndsi_count = feat.get('ndsi_count')\n",
    "            ndsi_std = feat.get('ndsi_stdDev')\n",
    "            ice_fraction = feat.get('ice_binary')\n",
    "            cloud_fraction = feat.get('is_cloud')\n",
    "            \n",
    "            # Safe conversions with null checks\n",
    "            cloud_pct = ee.Algorithms.If(\n",
    "                cloud_fraction,\n",
    "                ee.Number(cloud_fraction).multiply(100),\n",
    "                None\n",
    "            )\n",
    "            \n",
    "            ice_fraction_safe = ee.Algorithms.If(\n",
    "                ndsi_mean,\n",
    "                ice_fraction,\n",
    "                None\n",
    "            )\n",
    "            \n",
    "            pixel_count_safe = ee.Algorithms.If(\n",
    "                ndsi_count,\n",
    "                ee.Number(ndsi_count).toInt(),\n",
    "                None\n",
    "            )\n",
    "            \n",
    "            ndsi_std_safe = ee.Algorithms.If(\n",
    "                ndsi_std,\n",
    "                ndsi_std,\n",
    "                None\n",
    "            )\n",
    "            \n",
    "            return ee.Feature(None, {\n",
    "                'id': feat.get('id'),\n",
    "                's2_date': date.format('YYYY-MM-dd'),\n",
    "                's2_doy': date.getRelative('day', 'year'),\n",
    "                's2_ndsi_mean': ndsi_mean,\n",
    "                's2_ndsi_std': ndsi_std_safe,\n",
    "                's2_ice_fraction': ice_fraction_safe,\n",
    "                's2_pixel_count': pixel_count_safe,\n",
    "                's2_cloud_pct': cloud_pct\n",
    "            })\n",
    "        \n",
    "        return samples.map(create_s2_feature)\n",
    "    \n",
    "    s2_features = s2.map(extract_s2_features).flatten()\n",
    "    s2_features = s2_features.filter(ee.Filter.notNull(['s2_ndsi_mean']))\n",
    "    \n",
    "    return s2_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ERA5 Temperature Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_era5_temperature(lakes_fc, year):\n",
    "    \"\"\"\n",
    "    Process ERA5-Land temperature data - OPTIMIZED VERSION\n",
    "    Pre-computes daily means once, then samples all lakes in batch\n",
    "    Much faster than computing daily mean separately for each lake\n",
    "    \"\"\"\n",
    "    print(f\"  Loading ERA5 hourly data for {year}...\")\n",
    "    \n",
    "    # Load ERA5-Land hourly data\n",
    "    era5_hourly = (ee.ImageCollection('ECMWF/ERA5_LAND/HOURLY')\n",
    "                   .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "                   .select('temperature_2m'))\n",
    "    \n",
    "    # Convert to Celsius\n",
    "    def to_celsius(img):\n",
    "        temp_c = img.subtract(273.15).rename('temp_c')\n",
    "        return temp_c.copyProperties(img, ['system:time_start'])\n",
    "    \n",
    "    era5_hourly = era5_hourly.map(to_celsius)\n",
    "    \n",
    "    print(f\"  Pre-computing daily means...\")\n",
    "    \n",
    "    # Determine number of days in year (handle leap years)\n",
    "    is_leap = ee.Number(year).mod(4).eq(0).And(\n",
    "        ee.Number(year).mod(100).neq(0).Or(\n",
    "            ee.Number(year).mod(400).eq(0)\n",
    "        )\n",
    "    )\n",
    "    n_days = ee.Number(ee.Algorithms.If(is_leap, 366, 365))\n",
    "    \n",
    "    # Pre-compute daily means for entire year (365 or 366 images)\n",
    "    def compute_daily_mean(day):\n",
    "        day = ee.Number(day)\n",
    "        date = ee.Date.fromYMD(year, 1, 1).advance(day.subtract(1), 'day')\n",
    "        next_date = date.advance(1, 'day')\n",
    "        \n",
    "        # Get all hourly images for this day\n",
    "        daily_collection = era5_hourly.filterDate(date, next_date)\n",
    "        \n",
    "        # Check if we have data\n",
    "        has_data = daily_collection.size().gt(0)\n",
    "        \n",
    "        # Compute mean if data exists, otherwise use missing flag\n",
    "        daily_mean = ee.Image(ee.Algorithms.If(\n",
    "            has_data,\n",
    "            daily_collection.mean(),\n",
    "            ee.Image.constant(-9999).rename('temp_c')\n",
    "        ))\n",
    "        \n",
    "        return daily_mean.set({\n",
    "            'system:time_start': date.millis(),\n",
    "            'doy': day,\n",
    "            'date': date.format('YYYY-MM-dd')\n",
    "        })\n",
    "    \n",
    "    days = ee.List.sequence(1, n_days)\n",
    "    era5_daily = ee.ImageCollection.fromImages(days.map(compute_daily_mean))\n",
    "    \n",
    "    print(f\"  Sampling all lakes from daily means...\")\n",
    "    \n",
    "    # Now sample ALL lakes from each daily mean (batch operation)\n",
    "    def sample_all_lakes(daily_img):\n",
    "        doy = daily_img.get('doy')\n",
    "        date = daily_img.get('date')\n",
    "        \n",
    "        # Sample ALL lakes at once using reduceRegions\n",
    "        samples = daily_img.reduceRegions(\n",
    "            collection=lakes_fc,\n",
    "            reducer=ee.Reducer.first(),  # Get pixel value at centroid\n",
    "            scale=11000  # ERA5-Land resolution\n",
    "        )\n",
    "        \n",
    "        # Add date info to each sampled feature\n",
    "        def add_date_info(feat):\n",
    "            # Get the temperature value (from 'first' property created by reducer)\n",
    "            # Use ee.Algorithms.If to provide default if missing\n",
    "            temp_value = ee.Algorithms.If(\n",
    "                feat.propertyNames().contains('first'),\n",
    "                feat.get('first'),\n",
    "                -9999\n",
    "            )\n",
    "            \n",
    "            return feat.set({\n",
    "                'era5_date': date,\n",
    "                'era5_doy': doy,\n",
    "                'temp_c': temp_value\n",
    "            })\n",
    "        \n",
    "        return samples.map(add_date_info)\n",
    "    \n",
    "    # Process all daily images\n",
    "    era5_features = era5_daily.map(sample_all_lakes).flatten()\n",
    "    \n",
    "    return era5_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Show chunk statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks to process: 21\n",
      "Years to process: [2019, 2020, 2021, 2022, 2023]\n",
      "Total exports: 315 (chunks × years × 3 datasets)\n",
      "\n",
      "Chunk statistics:\n",
      "    chunk_id  n_lakes    lat_min    lat_max     lon_min     lon_max\n",
      "0          0     1659  69.003753  70.805772 -157.460685 -156.323643\n",
      "1          1     1402  69.003327  70.498668 -150.588202 -149.501159\n",
      "2          2     2172  69.000561  70.506201 -149.575529 -148.460811\n",
      "3          3     1002  69.002549  70.662131 -161.092265 -159.724530\n",
      "4          4     2243  70.139174  71.157997 -155.626734 -154.362639\n",
      "5          5      408  69.061209  70.123854 -144.857630 -141.021667\n",
      "6          6     1726  69.886316  70.909059 -153.572246 -152.259761\n",
      "7          7     1704  69.002644  70.811790 -158.909885 -157.856992\n",
      "8          8      753  69.019065  70.186629 -147.158786 -144.887360\n",
      "9          9      460  69.027426  70.075681 -163.578774 -162.004458\n",
      "10        10     1440  69.013772  70.328129 -153.976923 -152.575637\n",
      "11        11     1392  69.000974  70.218418 -155.661592 -154.104769\n",
      "12        12     1834  69.001176  70.475984 -151.662586 -150.520819\n",
      "13        13     2001  69.468849  70.951464 -156.328499 -155.325830\n",
      "14        14     2095  69.242415  70.875454 -154.562909 -153.490827\n",
      "15        15     1152  69.001200  70.319093 -148.591307 -147.158186\n",
      "16        16     2019  69.000146  70.582641 -152.675190 -151.585260\n",
      "17        17      897  69.008946  70.299875 -162.343774 -160.999169\n",
      "18        18     1493  69.020858  70.841981 -159.930147 -158.880509\n",
      "19        19     1987  69.118177  71.063040 -158.031454 -157.038279\n",
      "20        20     1269  70.430225  71.360948 -157.142680 -155.634957\n"
     ]
    }
   ],
   "source": [
    "# Load chunk statistics to know how many chunks we have\n",
    "chunk_stats = pd.read_csv(f'gs://{BUCKET}/{BASE_PATH}/processed/chunk_statistics.csv')\n",
    "n_chunks = len(chunk_stats)\n",
    "\n",
    "print(f\"Total chunks to process: {n_chunks}\")\n",
    "print(f\"Years to process: {YEARS}\")\n",
    "print(f\"Total exports: {n_chunks * len(YEARS) * 3} (chunks × years × 3 datasets)\")\n",
    "print(\"\\nChunk statistics:\")\n",
    "print(chunk_stats[['chunk_id', 'n_lakes', 'lat_min', 'lat_max', 'lon_min', 'lon_max']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ERA5 Temperature export (via xarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after reading back:\n",
      "['id', 'chunk_id', 'lake_area_m2', 'lake_area_km2', 'lake_perim_m', 'circularity', 'shoreline_dev', 'convexity', 'centroid_lon', 'centroid_lat', 'geometry']\n",
      "\n",
      "First row:\n",
      "id                                                          278001\n",
      "chunk_id                                                         0\n",
      "lake_area_m2                                          28192.123629\n",
      "lake_area_km2                                             0.028192\n",
      "lake_perim_m                                            829.785235\n",
      "circularity                                               0.514525\n",
      "shoreline_dev                                             1.394109\n",
      "convexity                                                 0.910838\n",
      "centroid_lon                                           -157.027101\n",
      "centroid_lat                                             70.805772\n",
      "geometry         POLYGON ((-113496.97826215002 2315713.90602798...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "chunk_gdf = gpd.read_file('gs://wustl-eeps-geospatial/thermokarst_lakes/processed/chunks/chunk_00.geojson')\n",
    "\n",
    "print(\"Columns after reading back:\")\n",
    "print(chunk_gdf.columns.tolist())\n",
    "print(\"\\nFirst row:\")\n",
    "print(chunk_gdf.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping ERA5 export (RUN_ERA5_EXPORT = False)\n",
      "ERA5 data already exported - see gs://wustl-eeps-geospatial/thermokarst_lakes/exports/YYYY/chunk_XX/era5_data.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ERA5 Export Control\n",
    "# ============================================================\n",
    "RUN_ERA5_EXPORT = False  # Set to True to run ERA5 export\n",
    "\n",
    "if not RUN_ERA5_EXPORT:\n",
    "    print(\"Skipping ERA5 export (RUN_ERA5_EXPORT = False)\")\n",
    "    print(\"ERA5 data already exported - see gs://wustl-eeps-geospatial/thermokarst_lakes/exports/YYYY/chunk_XX/era5_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping ERA5 export (RUN_ERA5_EXPORT = False)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ERA5 Temperature Export (via xarray) \n",
    "# ============================================================\n",
    "if RUN_ERA5_EXPORT:\n",
    "    print(\"=\"*60)\n",
    "    print(\"DOWNLOADING ERA5 TEMPERATURE DATA\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    era5_start = time.time()\n",
    "\n",
    "    # Load chunk info\n",
    "    print(\"Step 1: Loading lake locations by chunk...\")\n",
    "    chunks_info = []\n",
    "\n",
    "    for chunk_id in range(n_chunks):\n",
    "        chunk_file = f'{CHUNKS_PATH}/chunk_{chunk_id:02d}.geojson'\n",
    "        chunk_gdf = gpd.read_file(chunk_file)\n",
    "       \n",
    "        \n",
    "        # Keep original -180:180 coordinates\n",
    "        chunks_info.append({\n",
    "            'chunk_id': chunk_id,\n",
    "            'gdf': chunk_gdf[['id', 'centroid_lon', 'centroid_lat']],\n",
    "            'n_lakes': len(chunk_gdf)\n",
    "        })\n",
    "\n",
    "    total_lakes = sum(c['n_lakes'] for c in chunks_info)\n",
    "    print(f\"  Loaded {total_lakes:,} lakes across {len(chunks_info)} chunks\\n\")\n",
    "\n",
    "    # Download ERA5\n",
    "    print(\"Step 2: Accessing ERA5 dataset...\")\n",
    "    ERA5_URL = 'gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3'\n",
    "    ds = xr.open_zarr(ERA5_URL, chunks={'time': 100}, consolidated=True)\n",
    "\n",
    "    # Check coordinate system\n",
    "    print(\"  Checking coordinates...\")\n",
    "    lat_min, lat_max = float(ds.latitude.min()), float(ds.latitude.max())\n",
    "    lon_min, lon_max = float(ds.longitude.min()), float(ds.longitude.max())\n",
    "    print(f\"  Latitude: {lat_min:.1f} to {lat_max:.1f}\")\n",
    "    print(f\"  Longitude: {lon_min:.1f} to {lon_max:.1f}\\n\")\n",
    "\n",
    "    # Convert Alaska longitudes if needed\n",
    "    alaska_lon_min, alaska_lon_max = -164, -140\n",
    "    if lon_min >= 0:  # Dataset uses 0-360\n",
    "        alaska_lon_min = alaska_lon_min % 360  # 196\n",
    "        alaska_lon_max = alaska_lon_max % 360  # 220\n",
    "        print(f\"  Converting to 0-360: Alaska is {alaska_lon_min}-{alaska_lon_max}E\\n\")\n",
    "\n",
    "    print(\"Step 3: Selecting North Slope region...\")\n",
    "    # Select with proper ordering\n",
    "    if ds.latitude[0] > ds.latitude[-1]:  # Descending\n",
    "        lat_slice = slice(72, 69)\n",
    "    else:  # Ascending\n",
    "        lat_slice = slice(69, 72)\n",
    "\n",
    "    ds_subset = ds['2m_temperature'].sel(\n",
    "        latitude=lat_slice,\n",
    "        longitude=slice(alaska_lon_min, alaska_lon_max),\n",
    "        time=slice(f'{YEARS[0]}-01-01', f'{YEARS[-1]}-12-31')\n",
    "    )\n",
    "\n",
    "    print(f\"  Selected: {len(ds_subset.latitude)} lats x {len(ds_subset.longitude)} lons x {len(ds_subset.time)} hours\\n\")\n",
    "\n",
    "    if len(ds_subset.latitude) == 0 or len(ds_subset.longitude) == 0:\n",
    "        raise ValueError(\"Selection returned empty dataset - check coordinate ranges\")\n",
    "\n",
    "    print(\"Step 4: Computing daily means...\")\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        # Compute and load into memory (avoids dask chunking issues)\n",
    "        ds_daily = ds_subset.resample(time='1D').mean().compute()\n",
    "        ds_daily_celsius = ds_daily - 273.15\n",
    "\n",
    "    print(f\"  {len(ds_daily_celsius.time)} daily means computed\\n\")\n",
    "\n",
    "    print(\"Step 5: Processing each chunk...\")\n",
    "\n",
    "    for chunk_info in chunks_info:\n",
    "        chunk_id = chunk_info['chunk_id']\n",
    "        chunk_gdf = chunk_info['gdf']\n",
    "        n_lakes = chunk_info['n_lakes']\n",
    "        \n",
    "        print(f\"  Chunk {chunk_id:02d} ({n_lakes} lakes)...\", end=' ')\n",
    "        \n",
    "        # Convert lake coordinates if needed\n",
    "        lake_lons = chunk_gdf['centroid_lon'].values\n",
    "        if alaska_lon_min > 180:  # Dataset uses 0-360\n",
    "            lake_lons = lake_lons % 360\n",
    "        \n",
    "        lake_lats = xr.DataArray(chunk_gdf['centroid_lat'].values, dims=['lake'])\n",
    "        lake_lons_da = xr.DataArray(lake_lons, dims=['lake'])\n",
    "        \n",
    "        # Interpolate\n",
    "        lake_temps = ds_daily_celsius.interp(\n",
    "            latitude=lake_lats,\n",
    "            longitude=lake_lons_da,\n",
    "            method='linear'\n",
    "        )\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df_chunk = lake_temps.to_dataframe(name='temp_c').reset_index()\n",
    "        df_chunk['id'] = chunk_gdf['id'].values[df_chunk['lake']]\n",
    "        df_chunk['era5_date'] = df_chunk['time'].dt.strftime('%Y-%m-%d')\n",
    "        df_chunk['era5_doy'] = df_chunk['time'].dt.dayofyear\n",
    "        df_chunk['year'] = df_chunk['time'].dt.year\n",
    "        df_chunk = df_chunk[['id', 'era5_date', 'era5_doy', 'temp_c', 'year']]\n",
    "        \n",
    "        # Export by year\n",
    "        for year in YEARS:\n",
    "            df_year = df_chunk[df_chunk['year'] == year].copy()\n",
    "            df_year = df_year.drop(columns=['year']).sort_values(['id', 'era5_doy'])\n",
    "            output_file = f'gs://{BUCKET}/{OUTPUT_PATH}/{year}/chunk_{chunk_id:02d}/era5_data.csv'\n",
    "            df_year.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(\"Done\")\n",
    "\n",
    "    era5_time = time.time() - era5_start\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ERA5 COMPLETE!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Time: {era5_time/60:.1f} minutes\")\n",
    "    print(f\"Files: {len(chunks_info) * len(YEARS)} CSV files\")\n",
    "else:\n",
    "    print(\"Skipping ERA5 export (RUN_ERA5_EXPORT = False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in chunk_00.geojson:\n",
      "['id', 'chunk_id', 'lake_area_m2', 'lake_area_km2', 'lake_perim_m', 'circularity', 'shoreline_dev', 'convexity', 'centroid_lon', 'centroid_lat', 'geometry']\n",
      "\n",
      "First few rows:\n",
      "       id  chunk_id  lake_area_m2  lake_area_km2  lake_perim_m  circularity  \\\n",
      "0  278001         0  2.819212e+04       0.028192    829.785235     0.514525   \n",
      "1  279125         0  7.143037e+04       0.071430   1458.696235     0.421855   \n",
      "2  280053         0  7.732235e+06       7.732235  16148.322690     0.372615   \n",
      "3  280110         0  4.257919e+06       4.257919  12113.186764     0.364662   \n",
      "4  280158         0  4.343958e+04       0.043440   1009.096047     0.536081   \n",
      "\n",
      "   shoreline_dev  convexity  centroid_lon  centroid_lat  \\\n",
      "0       1.394109   0.910838   -157.027101     70.805772   \n",
      "1       1.539638   0.890586   -157.035125     70.766392   \n",
      "2       1.638211   0.961461   -156.978301     70.761569   \n",
      "3       1.655979   0.886562   -157.020026     70.753066   \n",
      "4       1.365793   0.933048   -156.916509     70.739525   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON ((-113496.978 2315713.906, -113466.686...  \n",
      "1  POLYGON ((-114036.449 2311515.901, -113995.991...  \n",
      "2  POLYGON ((-112481.809 2313173.679, -112397.577...  \n",
      "3  POLYGON ((-113375.541 2311858.953, -113345.202...  \n",
      "4  POLYGON ((-109740.925 2308327.57, -109680.159 ...  \n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "CHUNKS_PATH = 'gs://wustl-eeps-geospatial/thermokarst_lakes/processed/chunks'\n",
    "\n",
    "# Check chunk_00\n",
    "chunk_gdf = gpd.read_file(f'{CHUNKS_PATH}/chunk_00.geojson')\n",
    "\n",
    "print(\"Columns in chunk_00.geojson:\")\n",
    "print(chunk_gdf.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(chunk_gdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## S1/S2 exports (via GEE)\n",
    "\n",
    "**WARNING:** This will prep ~210 export tasks (21 chunks × 5 years × 2 datasets each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing Chunk 0\n",
      "============================================================\n",
      "  1659 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Preparing 2022... Done\n",
      "  Preparing 2023... Done\n",
      "  Chunk 0 complete (0.8s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 1\n",
      "============================================================\n",
      "  1402 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Preparing 2022... Done\n",
      "  Preparing 2023... Done\n",
      "  Chunk 1 complete (1.3s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 2\n",
      "============================================================\n",
      "  2172 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Preparing 2022... Done\n",
      "  Preparing 2023... Done\n",
      "  Chunk 2 complete (3.5s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 3\n",
      "============================================================\n",
      "  1002 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Preparing 2022... Done\n",
      "  Preparing 2023... Done\n",
      "  Chunk 3 complete (1.0s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 4\n",
      "============================================================\n",
      "  2243 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Preparing 2022... Done\n",
      "  Preparing 2023... Done\n",
      "  Chunk 4 complete (3.0s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 5\n",
      "============================================================\n",
      "  408 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Preparing 2022... Done\n",
      "  Preparing 2023... Done\n",
      "  Chunk 5 complete (0.7s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 6\n",
      "============================================================\n",
      "  1726 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Preparing 2022... Done\n",
      "  Preparing 2023... Done\n",
      "  Chunk 6 complete (3.9s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 7\n",
      "============================================================\n",
      "  1704 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Preparing 2022... Done\n",
      "  Preparing 2023... Done\n",
      "  Chunk 7 complete (1.6s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 8\n",
      "============================================================\n",
      "  753 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Preparing 2022... Done\n",
      "  Preparing 2023... Done\n",
      "  Chunk 8 complete (0.7s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 9\n",
      "============================================================\n",
      "  460 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Preparing 2022... Done\n",
      "  Preparing 2023... Done\n",
      "  Chunk 9 complete (0.8s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 10\n",
      "============================================================\n",
      "  1440 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Preparing 2022... Done\n",
      "  Preparing 2023... Done\n",
      "  Chunk 10 complete (1.4s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 11\n",
      "============================================================\n",
      "  1392 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Preparing 2022... Done\n",
      "  Preparing 2023... Done\n",
      "  Chunk 11 complete (1.6s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 12\n",
      "============================================================\n",
      "  1834 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Preparing 2022... Done\n",
      "  Preparing 2023... Done\n",
      "  Chunk 12 complete (3.4s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 13\n",
      "============================================================\n",
      "  2001 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Preparing 2022... Done\n",
      "  Preparing 2023... Done\n",
      "  Chunk 13 complete (2.9s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 14\n",
      "============================================================\n",
      "  2095 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Preparing 2022... Done\n",
      "  Preparing 2023... Done\n",
      "  Chunk 14 complete (2.4s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 15\n",
      "============================================================\n",
      "  1152 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Preparing 2022... Done\n",
      "  Preparing 2023... Done\n",
      "  Chunk 15 complete (1.1s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 16\n",
      "============================================================\n",
      "  2019 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Preparing 2022... Done\n",
      "  Preparing 2023... Done\n",
      "  Chunk 16 complete (6.0s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 17\n",
      "============================================================\n",
      "  897 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Preparing 2022... Done\n",
      "  Preparing 2023... Done\n",
      "  Chunk 17 complete (1.0s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 18\n",
      "============================================================\n",
      "  1493 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Preparing 2022... Done\n",
      "  Preparing 2023... Done\n",
      "  Chunk 18 complete (1.2s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 19\n",
      "============================================================\n",
      "  1987 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Preparing 2022... Done\n",
      "  Preparing 2023... Done\n",
      "  Chunk 19 complete (7.9s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 20\n",
      "============================================================\n",
      "  1269 lakes (geometries pre-computed)\n",
      "  Preparing 2019... Done\n",
      "  Preparing 2020... Done\n",
      "  Preparing 2021... Done\n",
      "  Preparing 2022... Done\n",
      "  Preparing 2023... Done\n",
      "  Chunk 20 complete (1.8s)\n",
      "\n",
      "============================================================\n",
      "ALL EXPORTS PREPARED: 210 tasks\n",
      "============================================================\n",
      "Total preparation time: 0.8 minutes\n",
      "\n",
      "Ready to start 210 S1/S2 exports.\n",
      "ERA5 already complete (63 files).\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Prep S1/S2 exports\n",
    "# ERA5 already exported above\n",
    "# ============================================================\n",
    "\n",
    "all_exports = []\n",
    "total_start = time.time()\n",
    "\n",
    "for chunk_id in range(n_chunks):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing Chunk {chunk_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    chunk_start = time.time()\n",
    "    \n",
    "    # Load chunk with pre-computed geometries\n",
    "    interior_fc, landscape_fc, chunk_bounds, chunk_gdf = load_chunk_from_bucket(chunk_id)\n",
    "    \n",
    "    print(f\"  {len(chunk_gdf)} lakes (geometries pre-computed)\")\n",
    "    \n",
    "    for year in YEARS:\n",
    "        print(f\"  Preparing {year}...\", end=' ')\n",
    "        \n",
    "        # Process S1 and S2 with pre-computed FCs\n",
    "        s1_features = process_sentinel1(interior_fc, landscape_fc, year, chunk_bounds)\n",
    "        s2_features = process_sentinel2(interior_fc, year, chunk_bounds)\n",
    "        \n",
    "        # Create export tasks\n",
    "        s1_task = ee.batch.Export.table.toCloudStorage(\n",
    "            collection=s1_features,\n",
    "            description=f'S1_chunk{chunk_id:02d}_{year}',\n",
    "            bucket=BUCKET,\n",
    "            fileNamePrefix=f'{OUTPUT_PATH}/{year}/chunk_{chunk_id:02d}/s1_data',\n",
    "            fileFormat='CSV'\n",
    "        )\n",
    "        \n",
    "        s2_task = ee.batch.Export.table.toCloudStorage(\n",
    "            collection=s2_features,\n",
    "            description=f'S2_chunk{chunk_id:02d}_{year}',\n",
    "            bucket=BUCKET,\n",
    "            fileNamePrefix=f'{OUTPUT_PATH}/{year}/chunk_{chunk_id:02d}/s2_data',\n",
    "            fileFormat='CSV'\n",
    "        )\n",
    "        \n",
    "        all_exports.append({\n",
    "            'chunk_id': chunk_id,\n",
    "            'year': year,\n",
    "            'type': 'S1',\n",
    "            'task': s1_task\n",
    "        })\n",
    "        all_exports.append({\n",
    "            'chunk_id': chunk_id,\n",
    "            'year': year,\n",
    "            'type': 'S2',\n",
    "            'task': s2_task\n",
    "        })\n",
    "        \n",
    "        print(\"Done\")\n",
    "    \n",
    "    chunk_time = time.time() - chunk_start\n",
    "    print(f\"  Chunk {chunk_id} complete ({chunk_time:.1f}s)\")\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ALL EXPORTS PREPARED: {len(all_exports)} tasks\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total preparation time: {total_time/60:.1f} minutes\")\n",
    "print(f\"\\nReady to start {len(all_exports)} S1/S2 exports.\")\n",
    "print(f\"ERA5 already complete (63 files).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 210 tasks...\n",
      "  Started 10 tasks...\n",
      "  Started 20 tasks...\n",
      "  Started 30 tasks...\n",
      "  Started 40 tasks...\n",
      "  Started 50 tasks...\n",
      "  Started 60 tasks...\n"
     ]
    }
   ],
   "source": [
    "# Start all exports\n",
    "print(f\"Starting {len(all_exports)} tasks...\")\n",
    "started = 0\n",
    "\n",
    "for i, export in enumerate(all_exports):\n",
    "    try:\n",
    "        export['task'].start()\n",
    "        started += 1\n",
    "        \n",
    "        if started % 10 == 0:\n",
    "            print(f\"  Started {started} tasks...\")\n",
    "            time.sleep(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting task {i} ({export['type']} chunk {export['chunk_id']} {export['year']}): {e}\")\n",
    "\n",
    "print(f\"\\nStarted {started}/{len(all_exports)} tasks\")\n",
    "print(\"Monitor at: https://code.earthengine.google.com/tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Monitor GEE Export Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check status of GEE exports\n",
    "def check_export_status():\n",
    "    status_summary = {\n",
    "        'READY': 0,\n",
    "        'RUNNING': 0,\n",
    "        'COMPLETED': 0,\n",
    "        'FAILED': 0,\n",
    "        'CANCELLED': 0\n",
    "    }\n",
    "    \n",
    "    for exp in all_exports:\n",
    "        status = exp['task'].status()['state']\n",
    "        status_summary[status] = status_summary.get(status, 0) + 1\n",
    "    \n",
    "    print(f\"Export Status Summary:\")\n",
    "    print(f\"  Total tasks: {len(all_exports)}\")\n",
    "    for state, count in status_summary.items():\n",
    "        if count > 0:\n",
    "            print(f\"    {state}: {count}\")\n",
    "    \n",
    "    return status_summary\n",
    "\n",
    "# Run this cell periodically to check progress\n",
    "check_export_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook exports:\n",
    "\n",
    "**Sentinel-1**: \n",
    "- Lake interior: VV, VH backscatter (dB), VV-VH ratio, RGB features (normalized 0-255)\n",
    "- Landscape ring: VV, VH backscatter (dB), VV-VH ratio, RGB features (normalized 0-255)\n",
    "- Both ASCENDING and DESCENDING orbits\n",
    "- Angle mask applied (25-50 degrees)\n",
    "\n",
    "**Sentinel-2**: \n",
    "- s2_ndsi_mean: Mean NDSI value across lake pixels (continuous, -1 to 1)\n",
    "- s2_ndsi_std: Standard deviation of NDSI across lake pixels (measure of ice/water mixing)\n",
    "- s2_ice_fraction: Fraction of lake pixels with NDSI > 0.4 (continuous, 0 to 1)\n",
    "- s2_pixel_count: Number of valid (non-cloud) pixels used in calculation\n",
    "- s2_cloud_pct: Percentage of lake masked by clouds (0-100, for QC filtering)\n",
    "- Dual cloud masking: QA60 + s2cloudless\n",
    "\n",
    "**ERA5**: \n",
    "- Daily mean 2m air temperature at lake centroids (Celsius)\n",
    "\n",
    "**For:**\n",
    "- 21 spatial chunks\n",
    "- 5 years (2019, 2020, 2021, 2022, 2023)\n",
    "- ~31,000 North Slope lakes (>0.02 km²)\n",
    "\n",
    "**Output structure:**\n",
    "```\n",
    "gs://wustl-eeps-geospatial/thermokarst_lakes/exports/\n",
    "├── 2019/\n",
    "│   ├── chunk_00/\n",
    "│   │   ├── s1_data.csv\n",
    "│   │   ├── s2_data.csv\n",
    "│   │   └── era5_data.csv\n",
    "│   ├── chunk_01/\n",
    "│   └── ...\n",
    "├── 2020/\n",
    "├── 2021/\n",
    "├── 2022/\n",
    "└── 2023/\n",
    "```\n",
    "\n",
    "**Next step:** Combine CSVs and run ice detection algorithm (Notebook 03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "gee",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "GEE Geospatial (Local)",
   "language": "python",
   "name": "gee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

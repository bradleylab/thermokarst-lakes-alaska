{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Year Lake Ice Phenology Data Export\n",
    "## Part 2: GEE Processing and Export (2019-2021)\n",
    "\n",
    "**Goal:** Export S1 + S2 + ERA5 data for North Slope lakes across 3 years\n",
    "\n",
    "**Strategy:**\n",
    "- Process chunks independently (spatial parallelization)\n",
    "- Export one year at a time per chunk\n",
    "- Use efficient spatial filtering (only process S2 images that overlap lakes)\n",
    "- Total exports: ~21 chunks × 3 years = ~63 exports\n",
    "\n",
    "**Data sources:**\n",
    "- Sentinel-1 GRD (SAR)\n",
    "- Sentinel-2 SR Harmonized (optical, for NDSI)\n",
    "- ERA5-Land (temperature)\n",
    "\n",
    "**Years:** 2019, 2020, 2021 (match ALPOD temporal coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n",
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n",
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n",
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n",
      "Earth Engine initialized: GEE Initialized\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import xarray as xr\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Initialize Earth Engine\n",
    "ee.Initialize()\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"Earth Engine initialized: {ee.String('GEE Initialized').getInfo()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Years: [2019, 2020, 2021]\n",
      "  Chunks path: gs://wustl-eeps-geospatial/thermokarst_lakes/processed/chunks\n",
      "  Output: gs://wustl-eeps-geospatial/thermokarst_lakes/exports\n",
      "  S2 scene cloud threshold: 30%\n",
      "  S2 pixel cloud probability threshold: 40%\n",
      "  S2 time window: ±3 days\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BUCKET = 'wustl-eeps-geospatial'\n",
    "BASE_PATH = 'thermokarst_lakes'\n",
    "CHUNKS_PATH = f'gs://{BUCKET}/{BASE_PATH}/processed/chunks'\n",
    "OUTPUT_PATH = f'{BASE_PATH}/exports'  # No gs:// prefix for GEE exports\n",
    "\n",
    "# Years to process (match ALPOD coverage)\n",
    "YEARS = [2019, 2020, 2021]\n",
    "\n",
    "# Processing parameters\n",
    "SCALE = 10  # Sentinel-1 resolution\n",
    "S2_NDSI_THRESHOLD = 0.4  # NDSI > 0.4 = ice\n",
    "S2_CLOUD_THRESHOLD = 30  # Maximum cloud cover for S2 images (scene-level)\n",
    "S2_CLOUD_PROB_THRESHOLD = 40  # s2cloudless probability threshold (pixel-level)\n",
    "S2_TIME_WINDOW = 3  # Days before/after S1 acquisition to look for S2\n",
    "\n",
    "# Projection\n",
    "ALASKA_ALBERS = 'EPSG:3338'\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Years: {YEARS}\")\n",
    "print(f\"  Chunks path: {CHUNKS_PATH}\")\n",
    "print(f\"  Output: gs://{BUCKET}/{OUTPUT_PATH}\")\n",
    "print(f\"  S2 scene cloud threshold: {S2_CLOUD_THRESHOLD}%\")\n",
    "print(f\"  S2 pixel cloud probability threshold: {S2_CLOUD_PROB_THRESHOLD}%\")\n",
    "print(f\"  S2 time window: ±{S2_TIME_WINDOW} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload ALPOD data as GEE Asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping ALPOD upload (already done)\n",
      "Asset location: projects/eeps-geospatial/assets/ALPOD_full\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# UPLOAD ALPOD TO GEE ASSET (ONE-TIME ONLY)\n",
    "# =============================================================================\n",
    "# Set to False after asset is uploaded\n",
    "UPLOAD_ALPOD_ASSET = False\n",
    "\n",
    "if UPLOAD_ALPOD_ASSET:\n",
    "    print(\"Uploading ALPOD to GEE Asset...\")\n",
    "    print(\"(Direct from GCS )\\n\")\n",
    "    \n",
    "    !/opt/conda/bin/earthengine upload table \\\n",
    "        --asset_id=projects/eeps-geospatial/assets/ALPOD_full \\\n",
    "        gs://wustl-eeps-geospatial/thermokarst_lakes/ALPODlakes/ALPODlakes.shp\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Check progress: https://code.earthengine.google.com/tasks\")\n",
    "    print(\"Look for an 'Ingestion' task\")\n",
    "    print(\"\\n After upload completes, set UPLOAD_ALPOD_ASSET = False\")\n",
    "else:\n",
    "    print(\"Skipping ALPOD upload (already done)\")\n",
    "    print(\"Asset location: projects/eeps-geospatial/assets/ALPOD_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunk_from_bucket(chunk_id):\n",
    "    \"\"\"\n",
    "    Load a chunk and return ee.FeatureCollection from GEE Asset\n",
    "    Only sends lake IDs to GEE, not geometries (avoids payload limits)\n",
    "    \"\"\"\n",
    "    chunk_file = f'{CHUNKS_PATH}/chunk_{chunk_id:02d}.geojson'\n",
    "    \n",
    "    # Load locally to get ALPOD IDs and metadata\n",
    "    chunk_gdf = gpd.read_file(chunk_file)\n",
    "    chunk_gdf_wgs84 = chunk_gdf.to_crs('EPSG:4326')\n",
    "    \n",
    "    # Get ALPOD IDs\n",
    "    alpod_ids = chunk_gdf['id'].tolist()\n",
    "    \n",
    "    # Load from GEE Asset and filter to these lakes\n",
    "    all_alpod = ee.FeatureCollection('projects/eeps-geospatial/assets/ALPOD_full')\n",
    "    chunk_fc = all_alpod.filter(ee.Filter.inList('id', alpod_ids))\n",
    "    \n",
    "    # Add lake_id field (use ALPOD id)\n",
    "    chunk_fc = chunk_fc.map(lambda f: f.set('lake_id', f.get('id')))\n",
    "    \n",
    "    return chunk_fc, chunk_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lake_geometry_metrics(lakes_fc, region_bounds):\n",
    "    \"\"\"\n",
    "    Add lake interior and landscape ring geometries\n",
    "    Uses efficient spatial filtering - only masks lakes within 100m of each target lake\n",
    "    \"\"\"\n",
    "    # Load ALPOD from GEE Asset\n",
    "    all_alpod = ee.FeatureCollection('projects/eeps-geospatial/assets/ALPOD_full')\n",
    "    \n",
    "    def add_geometries(lake):\n",
    "        lake_geom = lake.geometry()\n",
    "        lake_id = lake.get('lake_id')\n",
    "        \n",
    "        # Lake interior: 10m inward buffer\n",
    "        lake_interior = lake_geom.buffer(-10)\n",
    "        \n",
    "        # Landscape ring: 100m outward buffer\n",
    "        ring_outer = lake_geom.buffer(100)\n",
    "        \n",
    "        # Only find lakes that intersect THIS lake's 100m buffer\n",
    "        nearby_lakes = all_alpod.filterBounds(ring_outer)\n",
    "        nearby_dissolved = nearby_lakes.geometry().dissolve(maxError=10)\n",
    "        \n",
    "        # Subtract only the nearby lakes\n",
    "        landscape_ring = ring_outer.difference(nearby_dissolved)\n",
    "        \n",
    "        return lake.set({\n",
    "            'lake_id': lake_id,\n",
    "            'lake_interior': lake_interior,\n",
    "            'landscape_ring': landscape_ring\n",
    "        })\n",
    "    \n",
    "    return lakes_fc.map(add_geometries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sentinel-1 Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentinel1(lakes_fc, year, region_bounds):\n",
    "    \"\"\"\n",
    "    Extract S1 features for both lake interiors and landscape rings.\n",
    "    \n",
    "    For each zone (lake and landscape), exports:\n",
    "    - VV, VH backscatter (raw dB)\n",
    "    - VV-VH ratio (dB)\n",
    "    - RGB scaled features (normalized 0-255)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load S1 collection\n",
    "    s1 = (ee.ImageCollection('COPERNICUS/S1_GRD')\n",
    "          .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "          .filterBounds(region_bounds)\n",
    "          .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
    "          .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
    "          .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n",
    "          .filter(ee.Filter.eq('resolution_meters', 10)))\n",
    "    \n",
    "    def apply_angle_mask(img):\n",
    "        \"\"\"Mask pixels outside 25-50 degree incidence angle range\"\"\"\n",
    "        angle = img.select('angle')\n",
    "        angle_mask = angle.gt(25).And(angle.lt(50))\n",
    "        return img.select(['VV', 'VH']).updateMask(angle_mask).copyProperties(img, img.propertyNames())\n",
    "    \n",
    "    s1 = s1.map(apply_angle_mask)\n",
    "    \n",
    "    def extract_s1_features(img):\n",
    "        \"\"\"Extract lake and landscape statistics from each S1 image\"\"\"\n",
    "        \n",
    "        vv_img = img.select('VV')\n",
    "        vh_img = img.select('VH')\n",
    "        \n",
    "        # Compute VV-VH at image level (before reduction)\n",
    "        vv_vh_img = vv_img.subtract(vh_img).rename('VV_VH')\n",
    "        \n",
    "        # Compute RGB bands (scaled/normalized features)\n",
    "        r_band = vv_img.unitScale(-20, -5).multiply(255).rename('R')\n",
    "        g_band = vh_img.unitScale(-28, -12).multiply(255).rename('G')\n",
    "        b_band = vv_vh_img.unitScale(8, 18).multiply(255).rename('B')\n",
    "        \n",
    "        # Stack all bands for efficient extraction\n",
    "        all_bands = vv_img.addBands(vh_img).addBands(vv_vh_img).addBands(r_band).addBands(g_band).addBands(b_band)\n",
    "        \n",
    "        date = img.date()\n",
    "        orbit = img.get('orbitProperties_pass')\n",
    "        \n",
    "        # Sample lake interiors\n",
    "        lake_samples = all_bands.reduceRegions(\n",
    "            collection=lakes_fc.select(['lake_id', 'lake_interior']),\n",
    "            reducer=ee.Reducer.mean(),\n",
    "            scale=10\n",
    "        )\n",
    "        \n",
    "        # Sample landscape rings\n",
    "        land_samples = all_bands.reduceRegions(\n",
    "            collection=lakes_fc.select(['lake_id', 'landscape_ring']),\n",
    "            reducer=ee.Reducer.mean(),\n",
    "            scale=10\n",
    "        )\n",
    "        \n",
    "        def create_s1_feature(lake_feat):\n",
    "            lake_id = lake_feat.get('lake_id')\n",
    "            \n",
    "            # Find matching landscape sample\n",
    "            land_feat = land_samples.filter(\n",
    "                ee.Filter.eq('lake_id', lake_id)\n",
    "            ).first()\n",
    "            \n",
    "            # Lake values - VV-VH already computed at image level\n",
    "            lake_vv = lake_feat.get('VV')\n",
    "            lake_vh = lake_feat.get('VH')\n",
    "            lake_vv_vh = lake_feat.get('VV_VH')\n",
    "            \n",
    "            # Landscape values\n",
    "            land_vv = land_feat.get('VV')\n",
    "            land_vh = land_feat.get('VH')\n",
    "            land_vv_vh = land_feat.get('VV_VH')\n",
    "            \n",
    "            # RGB values (already computed by reducer)\n",
    "            lake_r = lake_feat.get('R')\n",
    "            lake_g = lake_feat.get('G')\n",
    "            lake_b = lake_feat.get('B')\n",
    "            land_r = land_feat.get('R')\n",
    "            land_g = land_feat.get('G')\n",
    "            land_b = land_feat.get('B')\n",
    "            \n",
    "            return ee.Feature(None, {\n",
    "                'lake_id': lake_id,\n",
    "                's1_date': date.format('YYYY-MM-dd'),\n",
    "                's1_doy': date.getRelative('day', 'year'),\n",
    "                's1_orbit': orbit,\n",
    "                # Lake features\n",
    "                'lake_vv_db': lake_vv,\n",
    "                'lake_vh_db': lake_vh,\n",
    "                'lake_vv_vh_db': lake_vv_vh,\n",
    "                'lake_r': lake_r,\n",
    "                'lake_g': lake_g,\n",
    "                'lake_b': lake_b,\n",
    "                # Landscape features\n",
    "                'land_vv_db': land_vv,\n",
    "                'land_vh_db': land_vh,\n",
    "                'land_vv_vh_db': land_vv_vh,\n",
    "                'land_r': land_r,\n",
    "                'land_g': land_g,\n",
    "                'land_b': land_b\n",
    "            })\n",
    "        \n",
    "        return lake_samples.map(create_s1_feature)\n",
    "    \n",
    "    s1_features = s1.map(extract_s1_features).flatten()\n",
    "    \n",
    "    # Filter out observations where lake had no valid pixels\n",
    "    s1_features = s1_features.filter(ee.Filter.notNull(['lake_vv_db']))\n",
    "    \n",
    "    return s1_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sentinel-2 Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ndsi(img):\n",
    "    \"\"\"\n",
    "    Compute Normalized Difference Snow Index (NDSI)\n",
    "    NDSI = (Green - SWIR1) / (Green + SWIR1)\n",
    "    \"\"\"\n",
    "    green = img.select('B3')\n",
    "    swir1 = img.select('B11')\n",
    "    \n",
    "    ndsi = green.subtract(swir1).divide(green.add(swir1)).rename('ndsi')\n",
    "    \n",
    "    return img.addBands(ndsi)\n",
    "\n",
    "def mask_s2_clouds(img):\n",
    "    \"\"\"\n",
    "    Mask clouds using QA60 band (basic cloud mask)\n",
    "    \"\"\"\n",
    "    qa = img.select('QA60')\n",
    "    \n",
    "    # Bits 10 and 11 are clouds and cirrus\n",
    "    cloud_bit_mask = 1 << 10\n",
    "    cirrus_bit_mask = 1 << 11\n",
    "    \n",
    "    # Both should be zero (clear conditions)\n",
    "    mask = qa.bitwiseAnd(cloud_bit_mask).eq(0).And(\n",
    "           qa.bitwiseAnd(cirrus_bit_mask).eq(0))\n",
    "    \n",
    "    return img.updateMask(mask)\n",
    "\n",
    "def add_s2cloudless_mask(img):\n",
    "    s2_cloudless = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "    cloud_prob_collection = s2_cloudless.filter(\n",
    "        ee.Filter.eq('system:index', img.get('system:index'))\n",
    "    )\n",
    "    \n",
    "    has_cloud_data = cloud_prob_collection.size().gt(0)\n",
    "    \n",
    "    def apply_s2cloudless_mask():\n",
    "        cloud_prob = cloud_prob_collection.first().select('probability')\n",
    "        is_clear = cloud_prob.lt(S2_CLOUD_PROB_THRESHOLD)\n",
    "        return img.updateMask(is_clear)\n",
    "    \n",
    "    def use_qa60_only():\n",
    "        return img  # Already has QA60 mask\n",
    "    \n",
    "    return ee.Image(ee.Algorithms.If(\n",
    "        has_cloud_data,\n",
    "        apply_s2cloudless_mask(),\n",
    "        use_qa60_only()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel-2 Processing\n",
    "\n",
    "def process_sentinel2(lakes_fc, year, region_bounds):\n",
    "    \"\"\"\n",
    "    Extract S2 features - FIXED VERSION\n",
    "    \n",
    "    Exports:\n",
    "    - s2_ice_fraction: fraction of lake pixels with NDSI > threshold (0-1 continuous)\n",
    "    - s2_ndsi_mean: mean NDSI value across lake pixels (continuous)\n",
    "    - s2_cloud_pct: percentage of lake pixels masked by clouds (for QC)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load S2 collection with scene-level cloud filter\n",
    "    s2 = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "          .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "          .filterBounds(region_bounds)\n",
    "          .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', S2_CLOUD_THRESHOLD)))\n",
    "    \n",
    "    # Load s2cloudless for pixel-level cloud masking\n",
    "    s2_cloudless = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "                    .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "                    .filterBounds(region_bounds))\n",
    "    \n",
    "    def add_cloud_and_ndsi_bands(img):\n",
    "        \"\"\"Add NDSI, binary ice mask, and cloud probability bands\"\"\"\n",
    "        \n",
    "        # Get matching cloud probability image\n",
    "        img_date = img.date()\n",
    "        cloud_prob_img = s2_cloudless.filterDate(\n",
    "            img_date, img_date.advance(1, 'day')\n",
    "        ).first()\n",
    "        \n",
    "        # Cloud probability (0-100)\n",
    "        cloud_prob = ee.Image(ee.Algorithms.If(\n",
    "            cloud_prob_img,\n",
    "            ee.Image(cloud_prob_img).select('probability'),\n",
    "            ee.Image.constant(0).rename('probability')\n",
    "        ))\n",
    "        \n",
    "        # QA60 cloud mask\n",
    "        qa = img.select('QA60')\n",
    "        cloud_bit_mask = 1 << 10\n",
    "        cirrus_bit_mask = 1 << 11\n",
    "        qa_clear = qa.bitwiseAnd(cloud_bit_mask).eq(0).And(\n",
    "                   qa.bitwiseAnd(cirrus_bit_mask).eq(0))\n",
    "        \n",
    "        # Combined cloud mask: QA60 clear AND s2cloudless < threshold\n",
    "        s2cloudless_clear = cloud_prob.lt(S2_CLOUD_PROB_THRESHOLD)\n",
    "        combined_clear = qa_clear.And(s2cloudless_clear)\n",
    "        \n",
    "        # NDSI with cloud mask applied\n",
    "        ndsi = img.normalizedDifference(['B3', 'B11']).rename('ndsi')\n",
    "        ndsi_masked = ndsi.updateMask(combined_clear)\n",
    "        \n",
    "        # Binary ice mask (1 where NDSI > threshold, 0 otherwise)\n",
    "        # This gets averaged to give ice_fraction\n",
    "        ice_binary = ndsi_masked.gt(S2_NDSI_THRESHOLD).rename('ice_binary')\n",
    "        \n",
    "        # Cloud mask as 0/1 for computing cloud percentage\n",
    "        cloud_mask = combined_clear.Not().rename('is_cloud')\n",
    "        \n",
    "        return img.addBands(ndsi_masked).addBands(ice_binary).addBands(cloud_mask)\n",
    "    \n",
    "    s2 = s2.map(add_cloud_and_ndsi_bands)\n",
    "    \n",
    "    def extract_s2_features(img):\n",
    "        \"\"\"Extract lake-level statistics from each S2 image\"\"\"\n",
    "        \n",
    "        date = img.date()\n",
    "        \n",
    "        # Reduce NDSI over lake interiors (mean of continuous values)\n",
    "        ndsi_stats = img.select('ndsi').reduceRegions(\n",
    "            collection=lakes_fc.select(['lake_id', 'lake_interior']),\n",
    "            reducer=ee.Reducer.mean().setOutputs(['ndsi_mean']),\n",
    "            scale=20\n",
    "        )\n",
    "        \n",
    "        # Reduce ice_binary over lake interiors (mean = fraction of ice pixels)\n",
    "        ice_stats = img.select('ice_binary').reduceRegions(\n",
    "            collection=lakes_fc.select(['lake_id', 'lake_interior']),\n",
    "            reducer=ee.Reducer.mean().setOutputs(['ice_fraction']),\n",
    "            scale=20\n",
    "        )\n",
    "        \n",
    "        # Reduce cloud mask to get cloud percentage\n",
    "        cloud_stats = img.select('is_cloud').reduceRegions(\n",
    "            collection=lakes_fc.select(['lake_id', 'lake_interior']),\n",
    "            reducer=ee.Reducer.mean().setOutputs(['cloud_fraction']),\n",
    "            scale=20\n",
    "        )\n",
    "        \n",
    "        def create_s2_feature(ndsi_feat):\n",
    "            lake_id = ndsi_feat.get('lake_id')\n",
    "            \n",
    "            # Get matching ice and cloud stats\n",
    "            ice_feat = ice_stats.filter(ee.Filter.eq('lake_id', lake_id)).first()\n",
    "            cloud_feat = cloud_stats.filter(ee.Filter.eq('lake_id', lake_id)).first()\n",
    "            \n",
    "            # Get values with null handling\n",
    "            ndsi_mean = ndsi_feat.get('ndsi_mean')\n",
    "            ice_fraction = ice_feat.get('ice_fraction')\n",
    "            cloud_fraction = cloud_feat.get('cloud_fraction')\n",
    "            \n",
    "            # Convert cloud fraction to percentage (0-100)\n",
    "            cloud_pct = ee.Algorithms.If(\n",
    "                cloud_fraction,\n",
    "                ee.Number(cloud_fraction).multiply(100),\n",
    "                ee.Number(-1)  # Flag for missing data\n",
    "            )\n",
    "            \n",
    "            # Handle nulls: if ndsi_mean is null, set ice_fraction to null too\n",
    "            # This happens when all pixels are masked by clouds\n",
    "            ice_fraction_safe = ee.Algorithms.If(\n",
    "                ndsi_mean,\n",
    "                ice_fraction,\n",
    "                None\n",
    "            )\n",
    "            \n",
    "            return ee.Feature(None, {\n",
    "                'lake_id': lake_id,\n",
    "                's2_date': date.format('YYYY-MM-dd'),\n",
    "                's2_doy': date.getRelative('day', 'year'),\n",
    "                's2_ndsi_mean': ndsi_mean,\n",
    "                's2_ice_fraction': ice_fraction_safe,\n",
    "                's2_cloud_pct': cloud_pct\n",
    "            })\n",
    "        \n",
    "        return ndsi_stats.map(create_s2_feature)\n",
    "    \n",
    "    s2_features = s2.map(extract_s2_features).flatten()\n",
    "    \n",
    "    # Filter out features where all data is null (completely cloudy)\n",
    "    # This reduces export size and avoids null-heavy CSVs\n",
    "    s2_features = s2_features.filter(ee.Filter.notNull(['s2_ndsi_mean']))\n",
    "    \n",
    "    return s2_features\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# USAGE IN NOTEBOOK 02:\n",
    "# =============================================================================\n",
    "# Replace the existing process_sentinel2 function with this one.\n",
    "# \n",
    "# The exported CSV will now have columns:\n",
    "#   - lake_id\n",
    "#   - s2_date\n",
    "#   - s2_doy  \n",
    "#   - s2_ndsi_mean     (continuous, -1 to 1)\n",
    "#   - s2_ice_fraction  (continuous, 0 to 1 = fraction of pixels with NDSI > threshold)\n",
    "#   - s2_cloud_pct     (0-100, percentage of lake pixels that were cloudy)\n",
    "#\n",
    "# In notebook 03, you can then create training labels like:\n",
    "#   - HIGH CONFIDENCE ICE:   ice_fraction > 0.8 AND cloud_pct < 20\n",
    "#   - HIGH CONFIDENCE WATER: ice_fraction < 0.2 AND cloud_pct < 20\n",
    "#   - PARTIAL/AMBIGUOUS:     everything else\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ERA5 Temperature Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_era5_temperature(lakes_fc, year):\n",
    "    \"\"\"\n",
    "    Process ERA5-Land temperature data - OPTIMIZED VERSION\n",
    "    Pre-computes daily means once, then samples all lakes in batch\n",
    "    Much faster than computing daily mean separately for each lake\n",
    "    \"\"\"\n",
    "    print(f\"  Loading ERA5 hourly data for {year}...\")\n",
    "    \n",
    "    # Load ERA5-Land hourly data\n",
    "    era5_hourly = (ee.ImageCollection('ECMWF/ERA5_LAND/HOURLY')\n",
    "                   .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "                   .select('temperature_2m'))\n",
    "    \n",
    "    # Convert to Celsius\n",
    "    def to_celsius(img):\n",
    "        temp_c = img.subtract(273.15).rename('temp_c')\n",
    "        return temp_c.copyProperties(img, ['system:time_start'])\n",
    "    \n",
    "    era5_hourly = era5_hourly.map(to_celsius)\n",
    "    \n",
    "    print(f\"  Pre-computing daily means...\")\n",
    "    \n",
    "    # Determine number of days in year (handle leap years)\n",
    "    is_leap = ee.Number(year).mod(4).eq(0).And(\n",
    "        ee.Number(year).mod(100).neq(0).Or(\n",
    "            ee.Number(year).mod(400).eq(0)\n",
    "        )\n",
    "    )\n",
    "    n_days = ee.Number(ee.Algorithms.If(is_leap, 366, 365))\n",
    "    \n",
    "    # Pre-compute daily means for entire year (365 or 366 images)\n",
    "    def compute_daily_mean(day):\n",
    "        day = ee.Number(day)\n",
    "        date = ee.Date.fromYMD(year, 1, 1).advance(day.subtract(1), 'day')\n",
    "        next_date = date.advance(1, 'day')\n",
    "        \n",
    "        # Get all hourly images for this day\n",
    "        daily_collection = era5_hourly.filterDate(date, next_date)\n",
    "        \n",
    "        # Check if we have data\n",
    "        has_data = daily_collection.size().gt(0)\n",
    "        \n",
    "        # Compute mean if data exists, otherwise use missing flag\n",
    "        daily_mean = ee.Image(ee.Algorithms.If(\n",
    "            has_data,\n",
    "            daily_collection.mean(),\n",
    "            ee.Image.constant(-9999).rename('temp_c')\n",
    "        ))\n",
    "        \n",
    "        return daily_mean.set({\n",
    "            'system:time_start': date.millis(),\n",
    "            'doy': day,\n",
    "            'date': date.format('YYYY-MM-dd')\n",
    "        })\n",
    "    \n",
    "    days = ee.List.sequence(1, n_days)\n",
    "    era5_daily = ee.ImageCollection.fromImages(days.map(compute_daily_mean))\n",
    "    \n",
    "    print(f\"  Sampling all lakes from daily means...\")\n",
    "    \n",
    "    # Now sample ALL lakes from each daily mean (batch operation)\n",
    "    def sample_all_lakes(daily_img):\n",
    "        doy = daily_img.get('doy')\n",
    "        date = daily_img.get('date')\n",
    "        \n",
    "        # Sample ALL lakes at once using reduceRegions\n",
    "        samples = daily_img.reduceRegions(\n",
    "            collection=lakes_fc,\n",
    "            reducer=ee.Reducer.first(),  # Get pixel value at centroid\n",
    "            scale=11000  # ERA5-Land resolution\n",
    "        )\n",
    "        \n",
    "        # Add date info to each sampled feature\n",
    "        def add_date_info(feat):\n",
    "            # Get the temperature value (from 'first' property created by reducer)\n",
    "            # Use ee.Algorithms.If to provide default if missing\n",
    "            temp_value = ee.Algorithms.If(\n",
    "                feat.propertyNames().contains('first'),\n",
    "                feat.get('first'),\n",
    "                -9999\n",
    "            )\n",
    "            \n",
    "            return feat.set({\n",
    "                'era5_date': date,\n",
    "                'era5_doy': doy,\n",
    "                'temp_c': temp_value\n",
    "            })\n",
    "        \n",
    "        return samples.map(add_date_info)\n",
    "    \n",
    "    # Process all daily images\n",
    "    era5_features = era5_daily.map(sample_all_lakes).flatten()\n",
    "    \n",
    "    return era5_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Export Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_chunk_year(chunk_id, year, lakes_fc, region_bounds):\n",
    "    \"\"\"\n",
    "    Process and export S1/S2/ERA5 data for one chunk and one year\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing Chunk {chunk_id}, Year {year}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Add lake geometries\n",
    "    lakes_with_geom = add_lake_geometry_metrics(lakes_fc, region_bounds)\n",
    "    print(\"Lakes processed with geometries\")\n",
    "    \n",
    "    # Process S1\n",
    "    print(\"\\nProcessing Sentinel-1...\")\n",
    "    s1_features = process_sentinel1(lakes_with_geom, year, region_bounds)\n",
    "    print(\"  S1 processing complete\")\n",
    "    \n",
    "    # Process S2\n",
    "    print(\"\\nProcessing Sentinel-2...\")\n",
    "    s2_features = process_sentinel2(lakes_with_geom, year, region_bounds)\n",
    "    print(\"  S2 processing complete\")\n",
    "    \n",
    "    # Process ERA5\n",
    "    print(\"\\nProcessing ERA5 temperature...\")\n",
    "    era5_features = process_era5_temperature(lakes_fc, year)\n",
    "    print(\"  ERA5 processing complete\")\n",
    "    \n",
    "    # Export all three\n",
    "    exports = []\n",
    "    \n",
    "    # S1 export\n",
    "    s1_task = ee.batch.Export.table.toCloudStorage(\n",
    "        collection=s1_features,\n",
    "        description=f'S1_chunk{chunk_id:02d}_{year}',\n",
    "        bucket=BUCKET,\n",
    "        fileNamePrefix=f'{OUTPUT_PATH}/{year}/chunk_{chunk_id:02d}/s1_data',\n",
    "        fileFormat='CSV'\n",
    "    )\n",
    "    \n",
    "    # S2 export\n",
    "    s2_task = ee.batch.Export.table.toCloudStorage(\n",
    "        collection=s2_features,\n",
    "        description=f'S2_chunk{chunk_id:02d}_{year}',\n",
    "        bucket=BUCKET,\n",
    "        fileNamePrefix=f'{OUTPUT_PATH}/{year}/chunk_{chunk_id:02d}/s2_data',\n",
    "        fileFormat='CSV'\n",
    "    )\n",
    "    \n",
    "    # ERA5 export\n",
    "    era5_task = ee.batch.Export.table.toCloudStorage(\n",
    "        collection=era5_features,\n",
    "        description=f'ERA5_chunk{chunk_id:02d}_{year}',\n",
    "        bucket=BUCKET,\n",
    "        fileNamePrefix=f'{OUTPUT_PATH}/{year}/chunk_{chunk_id:02d}/era5_data',\n",
    "        fileFormat='CSV'\n",
    "    )\n",
    "    \n",
    "    # Return all three tasks\n",
    "    exports = [\n",
    "        {'task': s1_task, 'type': 'S1', 'count': 'N/A'},\n",
    "        {'task': s2_task, 'type': 'S2', 'count': 'N/A'},\n",
    "        {'task': era5_task, 'type': 'ERA5', 'count': 'N/A'}\n",
    "    ]\n",
    "    \n",
    "    return exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Main Export Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks to process: 21\n",
      "Years to process: [2019, 2020, 2021]\n",
      "Total exports: 189 (chunks × years × 3 datasets)\n",
      "\n",
      "Chunk statistics:\n",
      "    chunk_id  n_lakes    lat_min    lat_max     lon_min     lon_max\n",
      "0          0     1659  69.003753  70.805772 -157.460685 -156.323643\n",
      "1          1     1402  69.003327  70.498668 -150.588202 -149.501159\n",
      "2          2     2172  69.000561  70.506201 -149.575529 -148.460811\n",
      "3          3     1002  69.002549  70.662131 -161.092265 -159.724530\n",
      "4          4     2243  70.139174  71.157997 -155.626734 -154.362639\n",
      "5          5      408  69.061209  70.123854 -144.857630 -141.021667\n",
      "6          6     1726  69.886316  70.909059 -153.572246 -152.259761\n",
      "7          7     1704  69.002644  70.811790 -158.909885 -157.856992\n",
      "8          8      753  69.019065  70.186629 -147.158786 -144.887360\n",
      "9          9      460  69.027426  70.075681 -163.578774 -162.004458\n",
      "10        10     1440  69.013772  70.328129 -153.976923 -152.575637\n",
      "11        11     1392  69.000974  70.218418 -155.661592 -154.104769\n",
      "12        12     1834  69.001176  70.475984 -151.662586 -150.520819\n",
      "13        13     2001  69.468849  70.951464 -156.328499 -155.325830\n",
      "14        14     2095  69.242415  70.875454 -154.562909 -153.490827\n",
      "15        15     1152  69.001200  70.319093 -148.591307 -147.158186\n",
      "16        16     2019  69.000146  70.582641 -152.675190 -151.585260\n",
      "17        17      897  69.008946  70.299875 -162.343774 -160.999169\n",
      "18        18     1493  69.020858  70.841981 -159.930147 -158.880509\n",
      "19        19     1987  69.118177  71.063040 -158.031454 -157.038279\n",
      "20        20     1269  70.430225  71.360948 -157.142680 -155.634957\n"
     ]
    }
   ],
   "source": [
    "# Load chunk statistics to know how many chunks we have\n",
    "chunk_stats = pd.read_csv(f'gs://{BUCKET}/{BASE_PATH}/processed/chunk_statistics.csv')\n",
    "n_chunks = len(chunk_stats)\n",
    "\n",
    "print(f\"Total chunks to process: {n_chunks}\")\n",
    "print(f\"Years to process: {YEARS}\")\n",
    "print(f\"Total exports: {n_chunks * len(YEARS) * 3} (chunks × years × 3 datasets)\")\n",
    "print(\"\\nChunk statistics:\")\n",
    "print(chunk_stats[['chunk_id', 'n_lakes', 'lat_min', 'lat_max', 'lon_min', 'lon_max']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "TEST RUN: Chunk 0, Year 2019\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: PROJ: proj_create_from_database: Open of /opt/conda/envs/gee/share/proj failed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing Chunk 0, Year 2019\n",
      "============================================================\n",
      "Lakes processed with geometries\n",
      "\n",
      "Processing Sentinel-1...\n",
      "  S1 processing complete\n",
      "\n",
      "Processing Sentinel-2...\n",
      "  S2 processing complete\n",
      "\n",
      "Processing ERA5 temperature...\n",
      "  Loading ERA5 hourly data for 2019...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "  ERA5 processing complete\n",
      "\n",
      "============================================================\n",
      "TEST EXPORTS PREPARED (NOT STARTED)\n",
      "============================================================\n",
      "  S1: N/A observations ready\n",
      "  S2: N/A observations ready\n",
      "  ERA5: N/A observations ready\n",
      "\n",
      "To start exports, run the cells below.\n"
     ]
    }
   ],
   "source": [
    "# Test with one chunk and one year first\n",
    "TEST_CHUNK = 0\n",
    "TEST_YEAR = 2019\n",
    "\n",
    "print(f\"\\n{'#'*60}\")\n",
    "print(f\"TEST RUN: Chunk {TEST_CHUNK}, Year {TEST_YEAR}\")\n",
    "print(f\"{'#'*60}\")\n",
    "\n",
    "# Load chunk\n",
    "test_fc, test_gdf = load_chunk_from_bucket(TEST_CHUNK)\n",
    "test_gdf_wgs84 = test_gdf.to_crs('EPSG:4326') \n",
    "test_bounds = ee.Geometry.Rectangle(test_gdf_wgs84.total_bounds.tolist())  # wgs84\n",
    "\n",
    "# Process and export\n",
    "test_exports = export_chunk_year(TEST_CHUNK, TEST_YEAR, test_fc, test_bounds)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TEST EXPORTS PREPARED (NOT STARTED)\")\n",
    "print(f\"{'='*60}\")\n",
    "for exp in test_exports:\n",
    "    print(f\"  {exp['type']}: {exp['count']} observations ready\")\n",
    "print(\"\\nTo start exports, run the cells below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏭️  Skipping test exports (SKIP_TEST = True)\n",
      "Test exports already completed. Proceed to full export below.\n"
     ]
    }
   ],
   "source": [
    "# TEST EXPORTS\n",
    "# Use this to test the export on one chunk. Can skip it in future runs once working\n",
    "SKIP_TEST = True  #True to skip this cell. \n",
    "\n",
    "if not SKIP_TEST:\n",
    "    print(\"Starting test exports...\")\n",
    "    for exp in test_exports:\n",
    "        exp['task'].start()\n",
    "        print(f\"  Started: {exp['task'].status()['description']}\")\n",
    "    \n",
    "    print(\"\\nTest exports started! Monitor at: https://code.earthengine.google.com/tasks\")\n",
    "    print(\"\\nOnce test completes successfully, proceed to full export below.\")\n",
    "else:\n",
    "    print(\"⏭️  Skipping test exports (SKIP_TEST = True)\")\n",
    "    print(\"Test exports already completed. Proceed to full export below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Full Export (All Chunks, All Years)\n",
    "\n",
    "**WARNING:** This will prep ~189 export tasks (21 chunks × 3 years × 3 datasets each)\n",
    "\n",
    "Only run after test export completes successfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing Chunk 0\n",
      "============================================================\n",
      "  1659 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019... ✓\n",
      "  Preparing 2020... ✓\n",
      "  Preparing 2021... ✓\n",
      "  Chunk 0 complete (1.2s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 1\n",
      "============================================================\n",
      "  1402 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019... ✓\n",
      "  Preparing 2020... ✓\n",
      "  Preparing 2021... ✓\n",
      "  Chunk 1 complete (1.7s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 2\n",
      "============================================================\n",
      "  2172 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019... ✓\n",
      "  Preparing 2020... ✓\n",
      "  Preparing 2021... ✓\n",
      "  Chunk 2 complete (4.0s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 3\n",
      "============================================================\n",
      "  1002 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019... ✓\n",
      "  Preparing 2020... ✓\n",
      "  Preparing 2021... ✓\n",
      "  Chunk 3 complete (1.3s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 4\n",
      "============================================================\n",
      "  2243 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019... ✓\n",
      "  Preparing 2020... ✓\n",
      "  Preparing 2021... ✓\n",
      "  Chunk 4 complete (3.5s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 5\n",
      "============================================================\n",
      "  408 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019... ✓\n",
      "  Preparing 2020... ✓\n",
      "  Preparing 2021... ✓\n",
      "  Chunk 5 complete (0.7s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 6\n",
      "============================================================\n",
      "  1726 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019... ✓\n",
      "  Preparing 2020... ✓\n",
      "  Preparing 2021... ✓\n",
      "  Chunk 6 complete (4.4s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 7\n",
      "============================================================\n",
      "  1704 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019... ✓\n",
      "  Preparing 2020... ✓\n",
      "  Preparing 2021... ✓\n",
      "  Chunk 7 complete (1.9s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 8\n",
      "============================================================\n",
      "  753 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019... ✓\n",
      "  Preparing 2020... ✓\n",
      "  Preparing 2021... ✓\n",
      "  Chunk 8 complete (0.9s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 9\n",
      "============================================================\n",
      "  460 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019... ✓\n",
      "  Preparing 2020... ✓\n",
      "  Preparing 2021... ✓\n",
      "  Chunk 9 complete (1.1s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 10\n",
      "============================================================\n",
      "  1440 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019... ✓\n",
      "  Preparing 2020... ✓\n",
      "  Preparing 2021... ✓\n",
      "  Chunk 10 complete (1.7s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 11\n",
      "============================================================\n",
      "  1392 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019... ✓\n",
      "  Preparing 2020... ✓\n",
      "  Preparing 2021... ✓\n",
      "  Chunk 11 complete (1.8s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 12\n",
      "============================================================\n",
      "  1834 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019... ✓\n",
      "  Preparing 2020... ✓\n",
      "  Preparing 2021... ✓\n",
      "  Chunk 12 complete (3.9s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 13\n",
      "============================================================\n",
      "  2001 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019... ✓\n",
      "  Preparing 2020... ✓\n",
      "  Preparing 2021... ✓\n",
      "  Chunk 13 complete (3.7s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 14\n",
      "============================================================\n",
      "  2095 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019... ✓\n",
      "  Preparing 2020... ✓\n",
      "  Preparing 2021... ✓\n",
      "  Chunk 14 complete (3.4s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 15\n",
      "============================================================\n",
      "  1152 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019... ✓\n",
      "  Preparing 2020... ✓\n",
      "  Preparing 2021... ✓\n",
      "  Chunk 15 complete (1.5s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 16\n",
      "============================================================\n",
      "  2019 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019... ✓\n",
      "  Preparing 2020... ✓\n",
      "  Preparing 2021... ✓\n",
      "  Chunk 16 complete (6.2s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 17\n",
      "============================================================\n",
      "  897 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019... ✓\n",
      "  Preparing 2020... ✓\n",
      "  Preparing 2021... ✓\n",
      "  Chunk 17 complete (1.5s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 18\n",
      "============================================================\n",
      "  1493 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019... ✓\n",
      "  Preparing 2020... ✓\n",
      "  Preparing 2021... ✓\n",
      "  Chunk 18 complete (1.7s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 19\n",
      "============================================================\n",
      "  1987 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019... ✓\n",
      "  Preparing 2020... ✓\n",
      "  Preparing 2021... ✓\n",
      "  Chunk 19 complete (8.3s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 20\n",
      "============================================================\n",
      "  1269 lakes\n",
      "  Adding geometries...\n",
      "  Preparing 2019... ✓\n",
      "  Preparing 2020... ✓\n",
      "  Preparing 2021... ✓\n",
      "  Chunk 20 complete (2.2s)\n",
      "\n",
      "============================================================\n",
      "ALL EXPORTS PREPARED: 126 tasks\n",
      "============================================================\n",
      "Total preparation time: 0.9 minutes\n",
      "\n",
      "Ready to start. Run next cell to begin 126 exports.\n"
     ]
    }
   ],
   "source": [
    "# Prep S1/S2/ERA5 exports\n",
    "all_exports = []\n",
    "total_start = time.time()\n",
    "for chunk_id in range(n_chunks):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing Chunk {chunk_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    chunk_start = time.time()\n",
    "    \n",
    "    # Load chunk (now uses GEE Asset - no payload issues!)\n",
    "    chunk_fc, chunk_gdf = load_chunk_from_bucket(chunk_id)\n",
    "    chunk_bounds = chunk_fc.geometry().bounds()\n",
    "    \n",
    "    print(f\"  {len(chunk_gdf)} lakes\")\n",
    "    \n",
    "    # Add lake geometries\n",
    "    print(\"  Adding geometries...\")\n",
    "    fc_with_geom = add_lake_geometry_metrics(chunk_fc, chunk_bounds)\n",
    "    \n",
    "    for year in YEARS:\n",
    "        print(f\"  Preparing {year}...\", end=' ')\n",
    "        \n",
    "        # Process sensors\n",
    "        s1_features = process_sentinel1(fc_with_geom, year, chunk_bounds)\n",
    "        s2_features = process_sentinel2(fc_with_geom, year, chunk_bounds)\n",
    "        era5_features = process_era5_temperature(chunk_fc, year)\n",
    "        \n",
    "        # Create export tasks\n",
    "        s1_task = ee.batch.Export.table.toCloudStorage(\n",
    "            collection=s1_features,\n",
    "            description=f'S1_chunk{chunk_id:02d}_{year}',\n",
    "            bucket=BUCKET,\n",
    "            fileNamePrefix=f'{OUTPUT_PATH}/{year}/chunk_{chunk_id:02d}/s1_data',\n",
    "            fileFormat='CSV'\n",
    "        )\n",
    "        \n",
    "        s2_task = ee.batch.Export.table.toCloudStorage(\n",
    "            collection=s2_features,\n",
    "            description=f'S2_chunk{chunk_id:02d}_{year}',\n",
    "            bucket=BUCKET,\n",
    "            fileNamePrefix=f'{OUTPUT_PATH}/{year}/chunk_{chunk_id:02d}/s2_data',\n",
    "            fileFormat='CSV'\n",
    "        )\n",
    "        \n",
    "        era5_task = ee.batch.Export.table.toCloudStorage(\n",
    "            collection=era5_features,\n",
    "            description=f'ERA5_chunk{chunk_id:02d}_{year}',\n",
    "            bucket=BUCKET,\n",
    "            fileNamePrefix=f'{OUTPUT_PATH}/{year}/chunk_{chunk_id:02d}/era5_data',\n",
    "            fileFormat='CSV'\n",
    "        )\n",
    "        \n",
    "        all_exports.append({\n",
    "            'chunk_id': chunk_id,\n",
    "            'year': year,\n",
    "            'type': 'S1',\n",
    "            'task': s1_task\n",
    "        })\n",
    "        all_exports.append({\n",
    "            'chunk_id': chunk_id,\n",
    "            'year': year,\n",
    "            'type': 'S2',\n",
    "            'task': s2_task\n",
    "        })\n",
    "        all_exports.append({\n",
    "            'chunk_id': chunk_id,\n",
    "            'year': year,\n",
    "            'type': 'ERA5',\n",
    "            'task': era5_task\n",
    "        })\n",
    "        \n",
    "        print(\"Done\")\n",
    "    \n",
    "    chunk_time = time.time() - chunk_start\n",
    "    print(f\"  Chunk {chunk_id} complete ({chunk_time:.1f}s)\")\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ALL EXPORTS PREPARED: {len(all_exports)} tasks\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total preparation time: {total_time/60:.1f} minutes\")\n",
    "print(f\"\\nReady to start. Run next cell to begin {len(all_exports)} exports.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Started 10 tasks...\n",
      "  Started 20 tasks...\n",
      "  Started 30 tasks...\n",
      "  Started 40 tasks...\n",
      "  Started 50 tasks...\n",
      "  Started 60 tasks...\n",
      "  Started 70 tasks...\n",
      "  Started 80 tasks...\n",
      "  Started 90 tasks...\n",
      "  Started 100 tasks...\n",
      "  Started 110 tasks...\n",
      "  Started 120 tasks...\n",
      "\n",
      "Started 126 tasks\n",
      "Monitor at: https://code.earthengine.google.com/tasks\n"
     ]
    }
   ],
   "source": [
    "# Start all exports\n",
    "started = 0\n",
    "for i, export in enumerate(all_exports):\n",
    "    try:\n",
    "        task = export['task']\n",
    "        if task.status()['state'] == 'UNSUBMITTED':\n",
    "            task.start()\n",
    "            started += 1\n",
    "            \n",
    "            # Small delay every 10 tasks to avoid rate limiting\n",
    "            if started % 10 == 0:\n",
    "                print(f\"  Started {started} tasks...\")\n",
    "                time.sleep(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting task {i} ({export['type']} chunk {export['chunk_id']} {export['year']}): {e}\")\n",
    "\n",
    "print(f\"\\nStarted {started} tasks\")\n",
    "print(\"Monitor at: https://code.earthengine.google.com/tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Monitor GEE Export Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export Status Summary:\n",
      "  Total tasks: 126\n",
      "    READY: 125\n",
      "    RUNNING: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'READY': 125, 'RUNNING': 1, 'COMPLETED': 0, 'FAILED': 0, 'CANCELLED': 0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check status of GEE exports\n",
    "def check_export_status():\n",
    "    status_summary = {\n",
    "        'READY': 0,\n",
    "        'RUNNING': 0,\n",
    "        'COMPLETED': 0,\n",
    "        'FAILED': 0,\n",
    "        'CANCELLED': 0\n",
    "    }\n",
    "    \n",
    "    for exp in all_exports:\n",
    "        status = exp['task'].status()['state']\n",
    "        status_summary[status] = status_summary.get(status, 0) + 1\n",
    "    \n",
    "    print(f\"Export Status Summary:\")\n",
    "    print(f\"  Total tasks: {len(all_exports)}\")\n",
    "    for state, count in status_summary.items():\n",
    "        if count > 0:\n",
    "            print(f\"    {state}: {count}\")\n",
    "    \n",
    "    return status_summary\n",
    "\n",
    "# Run this cell periodically to check progress\n",
    "check_export_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook exports:\n",
    "\n",
    "**Sentinel-1**: \n",
    "- Lake interior: VV, VH backscatter (dB), VV-VH ratio, RGB features (normalized 0-255)\n",
    "- Landscape ring: VV, VH backscatter (dB), VV-VH ratio, RGB features (normalized 0-255)\n",
    "- Both ASCENDING and DESCENDING orbits\n",
    "- Angle mask applied (25-50 degrees)\n",
    "\n",
    "**Sentinel-2**: \n",
    "- s2_ndsi_mean: Mean NDSI value across lake pixels (continuous, -1 to 1)\n",
    "- s2_ice_fraction: Fraction of lake pixels with NDSI > 0.4 (continuous, 0 to 1)\n",
    "- s2_cloud_pct: Percentage of lake masked by clouds (0-100, for QC filtering)\n",
    "- Dual cloud masking: QA60 + s2cloudless\n",
    "\n",
    "**ERA5**: \n",
    "- Daily mean 2m air temperature at lake centroids (Celsius)\n",
    "\n",
    "**For:**\n",
    "- 21 spatial chunks\n",
    "- 3 years (2019, 2020, 2021)\n",
    "- ~31,000 North Slope lakes (>0.02 km²)\n",
    "\n",
    "**Output structure:**\n",
    "```\n",
    "gs://wustl-eeps-geospatial/thermokarst_lakes/exports/\n",
    "├── 2019/\n",
    "│   ├── chunk_00/\n",
    "│   │   ├── s1_data.csv\n",
    "│   │   ├── s2_data.csv\n",
    "│   │   └── era5_data.csv\n",
    "│   ├── chunk_01/\n",
    "│   └── ...\n",
    "├── 2020/\n",
    "└── 2021/\n",
    "```\n",
    "\n",
    "**Next step:** Combine CSVs and run ice detection algorithm (Notebook 03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "gee",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "GEE Geospatial (Local)",
   "language": "python",
   "name": "gee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

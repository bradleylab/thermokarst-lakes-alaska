{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Year Lake Ice Phenology Data Export\n",
    "## Part 2: GEE Processing and Export (2019-2021)\n",
    "\n",
    "**Goal:** Export S1 + S2 + ERA5 data for North Slope lakes across 3 years\n",
    "\n",
    "**Strategy:**\n",
    "- Process chunks independently (spatial parallelization)\n",
    "- Export one year at a time per chunk\n",
    "- Use efficient spatial filtering (only process S2 images that overlap lakes)\n",
    "- Total exports: ~19 chunks × 3 years = ~57 exports\n",
    "\n",
    "**Data sources:**\n",
    "- Sentinel-1 GRD (SAR)\n",
    "- Sentinel-2 SR Harmonized (optical, for NDSI)\n",
    "- ERA5-Land (temperature)\n",
    "\n",
    "**Years:** 2019, 2020, 2021 (match ALPOD temporal coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n",
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n",
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n",
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n",
      "Earth Engine initialized: GEE Initialized\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Initialize Earth Engine\n",
    "ee.Initialize()\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"Earth Engine initialized: {ee.String('GEE Initialized').getInfo()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Years: [2019, 2020, 2021]\n",
      "  Chunks path: gs://wustl-eeps-geospatial/thermokarst_lakes/processed/chunks\n",
      "  Output: gs://wustl-eeps-geospatial/thermokarst_lakes/exports\n",
      "  S2 scene cloud threshold: 30%\n",
      "  S2 pixel cloud probability threshold: 40%\n",
      "  S2 time window: ±3 days\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BUCKET = 'wustl-eeps-geospatial'\n",
    "BASE_PATH = 'thermokarst_lakes'\n",
    "CHUNKS_PATH = f'gs://{BUCKET}/{BASE_PATH}/processed/chunks'\n",
    "OUTPUT_PATH = f'{BASE_PATH}/exports'  # No gs:// prefix for GEE exports\n",
    "\n",
    "# Years to process (match ALPOD coverage)\n",
    "YEARS = [2019, 2020, 2021]\n",
    "\n",
    "# Processing parameters\n",
    "SCALE = 10  # Sentinel-1 resolution\n",
    "S2_CLOUD_THRESHOLD = 30  # Maximum cloud cover for S2 images (scene-level)\n",
    "S2_CLOUD_PROB_THRESHOLD = 40  # s2cloudless probability threshold (pixel-level)\n",
    "S2_TIME_WINDOW = 3  # Days before/after S1 acquisition to look for S2\n",
    "\n",
    "# Projection\n",
    "ALASKA_ALBERS = 'EPSG:3338'\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Years: {YEARS}\")\n",
    "print(f\"  Chunks path: {CHUNKS_PATH}\")\n",
    "print(f\"  Output: gs://{BUCKET}/{OUTPUT_PATH}\")\n",
    "print(f\"  S2 scene cloud threshold: {S2_CLOUD_THRESHOLD}%\")\n",
    "print(f\"  S2 pixel cloud probability threshold: {S2_CLOUD_PROB_THRESHOLD}%\")\n",
    "print(f\"  S2 time window: ±{S2_TIME_WINDOW} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunk_from_bucket(chunk_id):\n",
    "    \"\"\"\n",
    "    Load a chunk GeoJSON from bucket and convert to ee.FeatureCollection\n",
    "    Downloads to local temp file first to avoid GCS streaming issues\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import tempfile\n",
    "    \n",
    "    chunk_file = f'{CHUNKS_PATH}/chunk_{chunk_id:02d}.geojson'\n",
    "    \n",
    "    # Download to local temp file\n",
    "    local_path = f'/tmp/chunk_{chunk_id:02d}.geojson'\n",
    "    \n",
    "    # Use gsutil to download (reliable in Vertex AI)\n",
    "    os.system(f'gsutil -q cp {chunk_file} {local_path}')\n",
    "    \n",
    "    # Load from local file\n",
    "    gdf = gpd.read_file(local_path)\n",
    "    \n",
    "    print(f\"  Loaded chunk {chunk_id}: {len(gdf)} lakes\")\n",
    "    print(f\"    Bounds: {gdf.total_bounds}\")\n",
    "    \n",
    "    # Keep only essential properties to reduce payload size\n",
    "    essential_cols = ['geometry']\n",
    "    \n",
    "    if 'lake_area_km2' in gdf.columns:\n",
    "        essential_cols.append('lake_area_km2')\n",
    "    \n",
    "    # Create simple ID if needed\n",
    "    gdf['lake_id'] = range(len(gdf))\n",
    "    essential_cols.append('lake_id')\n",
    "    \n",
    "    gdf_simplified = gdf[essential_cols].copy()\n",
    "    \n",
    "    # Simplify geometries to reduce size (5m tolerance)\n",
    "    print(f\"  Simplifying geometries...\")\n",
    "    gdf_simplified['geometry'] = gdf_simplified.geometry.simplify(\n",
    "        tolerance=5, preserve_topology=True\n",
    "    )\n",
    "    \n",
    "    # Convert to GeoJSON dict\n",
    "    geojson = gdf_simplified.__geo_interface__\n",
    "    \n",
    "    print(f\"  Creating EE FeatureCollection...\")\n",
    "    fc = ee.FeatureCollection(geojson)\n",
    "    print(f\"  FeatureCollection created successfully\")\n",
    "    \n",
    "    return fc, gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lake_geometry_metrics(fc):\n",
    "    \"\"\"\n",
    "    More efficient version: Pre-compute union of all lakes once\n",
    "    \"\"\"\n",
    "    # PRE-COMPUTE: Union of all lake geometries (done once per chunk)\n",
    "    all_lakes_union = fc.geometry().dissolve()\n",
    "    \n",
    "    def buffer_interior(feat):\n",
    "        geom = feat.geometry()\n",
    "        area = geom.area()\n",
    "        \n",
    "        buffered = geom.buffer(-10)\n",
    "        buffered = ee.Algorithms.If(\n",
    "            area.lt(10000),\n",
    "            geom,\n",
    "            buffered\n",
    "        )\n",
    "        \n",
    "        return feat.set({'lake_interior': buffered})\n",
    "    \n",
    "    def add_landscape_ring(feat):\n",
    "        geom = feat.geometry()\n",
    "        \n",
    "        # Create 100m ring\n",
    "        outer = geom.buffer(100)\n",
    "        ring = outer.difference(geom)\n",
    "        \n",
    "        # Remove ALL lakes from ring (including this one and all neighbors)\n",
    "        # This ensures we only sample terrestrial landscape\n",
    "        clean_ring = ring.difference(all_lakes_union)\n",
    "        \n",
    "        return feat.set({'landscape_ring': clean_ring})\n",
    "    \n",
    "    fc_with_interior = fc.map(buffer_interior)\n",
    "    fc_with_landscape = fc_with_interior.map(add_landscape_ring)\n",
    "    \n",
    "    return fc_with_landscape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sentinel-1 Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentinel1(lakes_fc, year, region_bounds):\n",
    "    \"\"\"\n",
    "    Process Sentinel-1 SAR data for a given year and region\n",
    "    \"\"\"\n",
    "    # Filter S1 collection\n",
    "    s1 = (ee.ImageCollection('COPERNICUS/S1_GRD')\n",
    "          .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "          .filterBounds(region_bounds)\n",
    "          .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
    "          .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n",
    "          .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
    "          .filter(ee.Filter.eq('orbitProperties_pass', 'DESCENDING'))\n",
    "          .select(['VV', 'VH']))\n",
    "    \n",
    "    def extract_s1_features(img):\n",
    "        \"\"\"\n",
    "        Extract S1 backscatter for each lake\n",
    "        \"\"\"\n",
    "        date = img.date()\n",
    "        \n",
    "        def lake_s1_stats(lake):\n",
    "            # Get lake interior geometry\n",
    "            lake_geom = ee.Geometry(lake.get('lake_interior'))\n",
    "            \n",
    "            # Extract VV and VH\n",
    "            stats = img.reduceRegion(\n",
    "                reducer=ee.Reducer.mean(),\n",
    "                geometry=lake_geom,\n",
    "                scale=SCALE,\n",
    "                maxPixels=1e9\n",
    "            )\n",
    "            \n",
    "            return lake.set({\n",
    "                's1_date': date.format('YYYY-MM-dd'),\n",
    "                's1_doy': date.getRelative('day', 'year'),\n",
    "                'vv_db': stats.get('VV'),\n",
    "                'vh_db': stats.get('VH')\n",
    "            })\n",
    "        \n",
    "        return lakes_fc.map(lake_s1_stats)\n",
    "    \n",
    "    # Process all S1 images\n",
    "    s1_features = s1.map(extract_s1_features).flatten()\n",
    "    \n",
    "    return s1_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sentinel-2 Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ndsi(img):\n",
    "    \"\"\"\n",
    "    Compute Normalized Difference Snow Index (NDSI)\n",
    "    NDSI = (Green - SWIR1) / (Green + SWIR1)\n",
    "    \"\"\"\n",
    "    green = img.select('B3')\n",
    "    swir1 = img.select('B11')\n",
    "    \n",
    "    ndsi = green.subtract(swir1).divide(green.add(swir1)).rename('ndsi')\n",
    "    \n",
    "    return img.addBands(ndsi)\n",
    "\n",
    "def mask_s2_clouds(img):\n",
    "    \"\"\"\n",
    "    Mask clouds using QA60 band (basic cloud mask)\n",
    "    \"\"\"\n",
    "    qa = img.select('QA60')\n",
    "    \n",
    "    # Bits 10 and 11 are clouds and cirrus\n",
    "    cloud_bit_mask = 1 << 10\n",
    "    cirrus_bit_mask = 1 << 11\n",
    "    \n",
    "    # Both should be zero (clear conditions)\n",
    "    mask = qa.bitwiseAnd(cloud_bit_mask).eq(0).And(\n",
    "           qa.bitwiseAnd(cirrus_bit_mask).eq(0))\n",
    "    \n",
    "    return img.updateMask(mask)\n",
    "\n",
    "def add_s2cloudless_mask(img):\n",
    "    \"\"\"\n",
    "    Add s2cloudless probability-based cloud mask\n",
    "    More accurate than QA60, especially for distinguishing ice from clouds\n",
    "    \"\"\"\n",
    "    # Get the s2cloudless collection\n",
    "    s2_cloudless = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "    \n",
    "    # Find the matching cloud probability image\n",
    "    # Match by system:index (unique ID that links S2 and s2cloudless)\n",
    "    cloud_prob = (s2_cloudless\n",
    "                  .filter(ee.Filter.eq('system:index', img.get('system:index')))\n",
    "                  .first()\n",
    "                  .select('probability'))\n",
    "    \n",
    "    # Mask pixels with cloud probability > threshold\n",
    "    is_clear = cloud_prob.lt(S2_CLOUD_PROB_THRESHOLD)\n",
    "    \n",
    "    return img.updateMask(is_clear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentinel2(lakes_fc, year, region_bounds):\n",
    "    \"\"\"\n",
    "    Process Sentinel-2 optical data for ice detection\n",
    "    Uses both QA60 and s2cloudless for robust cloud masking\n",
    "    Only process S2 images that actually overlap with lakes (spatial filtering)\n",
    "    \"\"\"\n",
    "    # Get S2 collection for the year\n",
    "    s2 = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "          .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "          .filterBounds(region_bounds)\n",
    "          .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', S2_CLOUD_THRESHOLD))\n",
    "          .map(mask_s2_clouds)        # Apply QA60 mask first (fast)\n",
    "          .map(add_s2cloudless_mask)  # Then s2cloudless (accurate for ice/cloud distinction)\n",
    "          .map(compute_ndsi))\n",
    "    \n",
    "    def extract_s2_for_date(s2_img):\n",
    "        \"\"\"\n",
    "        For each S2 image, extract NDSI for lakes within the image footprint\n",
    "        \"\"\"\n",
    "        s2_date = s2_img.date()\n",
    "        s2_bounds = s2_img.geometry()\n",
    "        cloud_pct = s2_img.get('CLOUDY_PIXEL_PERCENTAGE')\n",
    "        \n",
    "        # CRITICAL: Only process lakes that overlap this S2 image\n",
    "        lakes_in_image = lakes_fc.filterBounds(s2_bounds)\n",
    "        \n",
    "        def lake_s2_stats(lake):\n",
    "            lake_geom = ee.Geometry(lake.get('lake_interior'))\n",
    "            \n",
    "            # Compute ice fraction (NDSI > 0.4 indicates ice)\n",
    "            ndsi = s2_img.select('ndsi')\n",
    "            ice_mask = ndsi.gt(0.4)\n",
    "            \n",
    "            # Get statistics\n",
    "            stats = ice_mask.reduceRegion(\n",
    "                reducer=ee.Reducer.mean(),\n",
    "                geometry=lake_geom,\n",
    "                scale=20,  # S2 SWIR resolution\n",
    "                maxPixels=1e9\n",
    "            )\n",
    "            \n",
    "            ice_fraction = stats.get('ndsi')\n",
    "            \n",
    "            return lake.set({\n",
    "                's2_date': s2_date.format('YYYY-MM-dd'),\n",
    "                's2_doy': s2_date.getRelative('day', 'year'),\n",
    "                's2_ice_fraction': ice_fraction,\n",
    "                's2_cloud_pct': cloud_pct\n",
    "            })\n",
    "        \n",
    "        return lakes_in_image.map(lake_s2_stats)\n",
    "    \n",
    "    # Process all S2 images\n",
    "    s2_features = s2.map(extract_s2_for_date).flatten()\n",
    "    \n",
    "    return s2_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ERA5 Temperature Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_era5_temperature(lakes_fc, year):\n",
    "    \"\"\"\n",
    "    Process ERA5-Land temperature data - OPTIMIZED VERSION\n",
    "    Pre-computes daily means once, then samples all lakes in batch\n",
    "    Much faster than computing daily mean separately for each lake\n",
    "    \"\"\"\n",
    "    print(f\"  Loading ERA5 hourly data for {year}...\")\n",
    "    \n",
    "    # Load ERA5-Land hourly data\n",
    "    era5_hourly = (ee.ImageCollection('ECMWF/ERA5_LAND/HOURLY')\n",
    "                   .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "                   .select('temperature_2m'))\n",
    "    \n",
    "    # Convert to Celsius\n",
    "    def to_celsius(img):\n",
    "        temp_c = img.subtract(273.15).rename('temp_c')\n",
    "        return temp_c.copyProperties(img, ['system:time_start'])\n",
    "    \n",
    "    era5_hourly = era5_hourly.map(to_celsius)\n",
    "    \n",
    "    print(f\"  Pre-computing daily means...\")\n",
    "    \n",
    "    # Determine number of days in year (handle leap years)\n",
    "    is_leap = ee.Number(year).mod(4).eq(0).And(\n",
    "        ee.Number(year).mod(100).neq(0).Or(\n",
    "            ee.Number(year).mod(400).eq(0)\n",
    "        )\n",
    "    )\n",
    "    n_days = ee.Number(ee.Algorithms.If(is_leap, 366, 365))\n",
    "    \n",
    "    # Pre-compute daily means for entire year (365 or 366 images)\n",
    "    def compute_daily_mean(day):\n",
    "        day = ee.Number(day)\n",
    "        date = ee.Date.fromYMD(year, 1, 1).advance(day.subtract(1), 'day')\n",
    "        next_date = date.advance(1, 'day')\n",
    "        \n",
    "        # Get all hourly images for this day\n",
    "        daily_collection = era5_hourly.filterDate(date, next_date)\n",
    "        \n",
    "        # Check if we have data\n",
    "        has_data = daily_collection.size().gt(0)\n",
    "        \n",
    "        # Compute mean if data exists, otherwise use missing flag\n",
    "        daily_mean = ee.Image(ee.Algorithms.If(\n",
    "            has_data,\n",
    "            daily_collection.mean(),\n",
    "            ee.Image.constant(-9999).rename('temp_c')\n",
    "        ))\n",
    "        \n",
    "        return daily_mean.set({\n",
    "            'system:time_start': date.millis(),\n",
    "            'doy': day,\n",
    "            'date': date.format('YYYY-MM-dd')\n",
    "        })\n",
    "    \n",
    "    days = ee.List.sequence(1, n_days)\n",
    "    era5_daily = ee.ImageCollection.fromImages(days.map(compute_daily_mean))\n",
    "    \n",
    "    print(f\"  Sampling all lakes from daily means...\")\n",
    "    \n",
    "    # Now sample ALL lakes from each daily mean (batch operation)\n",
    "    def sample_all_lakes(daily_img):\n",
    "        doy = daily_img.get('doy')\n",
    "        date = daily_img.get('date')\n",
    "        \n",
    "        # Sample ALL lakes at once using reduceRegions\n",
    "        samples = daily_img.reduceRegions(\n",
    "            collection=lakes_fc,\n",
    "            reducer=ee.Reducer.first(),  # Get pixel value at centroid\n",
    "            scale=11000  # ERA5-Land resolution\n",
    "        )\n",
    "        \n",
    "        # Add date info to each sampled feature\n",
    "        def add_date_info(feat):\n",
    "            # Get the temperature value (from 'first' property created by reducer)\n",
    "            # Use ee.Algorithms.If to provide default if missing\n",
    "            temp_value = ee.Algorithms.If(\n",
    "                feat.propertyNames().contains('first'),\n",
    "                feat.get('first'),\n",
    "                -9999\n",
    "            )\n",
    "            \n",
    "            return feat.set({\n",
    "                'era5_date': date,\n",
    "                'era5_doy': doy,\n",
    "                'temp_c': temp_value\n",
    "            })\n",
    "        \n",
    "        return samples.map(add_date_info)\n",
    "    \n",
    "    # Process all daily images\n",
    "    era5_features = era5_daily.map(sample_all_lakes).flatten()\n",
    "    \n",
    "    return era5_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Export Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_chunk_year(chunk_id, year, lakes_fc, region_bounds):\n",
    "    \"\"\"\n",
    "    Process and export all data for one chunk and one year\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing Chunk {chunk_id}, Year {year}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Add lake geometries\n",
    "    lakes_with_geom = add_lake_geometry_metrics(lakes_fc)\n",
    "    n_lakes = lakes_with_geom.size().getInfo()\n",
    "    print(f\"Lakes in chunk: {n_lakes}\")\n",
    "    \n",
    "    # Process S1\n",
    "    print(\"\\nProcessing Sentinel-1...\")\n",
    "    s1_features = process_sentinel1(lakes_with_geom, year, region_bounds)\n",
    "    s1_count = s1_features.size().getInfo()\n",
    "    print(f\"  S1 observations: {s1_count}\")\n",
    "    \n",
    "    # Process S2\n",
    "    print(\"\\nProcessing Sentinel-2...\")\n",
    "    s2_features = process_sentinel2(lakes_with_geom, year, region_bounds)\n",
    "    s2_count = s2_features.size().getInfo()\n",
    "    print(f\"  S2 observations: {s2_count}\")\n",
    "    \n",
    "    # Process ERA5\n",
    "    print(\"\\nProcessing ERA5 temperature...\")\n",
    "    era5_features = process_era5_temperature(lakes_with_geom, year)\n",
    "    era5_count = era5_features.size().getInfo()\n",
    "    print(f\"  ERA5 observations: {era5_count}\")\n",
    "    \n",
    "    # Export each dataset separately\n",
    "    exports = []\n",
    "    \n",
    "    # S1 export\n",
    "    s1_task = ee.batch.Export.table.toCloudStorage(\n",
    "        collection=s1_features,\n",
    "        description=f'S1_chunk{chunk_id:02d}_{year}',\n",
    "        bucket=BUCKET,\n",
    "        fileNamePrefix=f'{OUTPUT_PATH}/{year}/chunk_{chunk_id:02d}/s1_data',\n",
    "        fileFormat='CSV'\n",
    "    )\n",
    "    \n",
    "    # S2 export\n",
    "    s2_task = ee.batch.Export.table.toCloudStorage(\n",
    "        collection=s2_features,\n",
    "        description=f'S2_chunk{chunk_id:02d}_{year}',\n",
    "        bucket=BUCKET,\n",
    "        fileNamePrefix=f'{OUTPUT_PATH}/{year}/chunk_{chunk_id:02d}/s2_data',\n",
    "        fileFormat='CSV'\n",
    "    )\n",
    "    \n",
    "    # ERA5 export\n",
    "    era5_task = ee.batch.Export.table.toCloudStorage(\n",
    "        collection=era5_features,\n",
    "        description=f'ERA5_chunk{chunk_id:02d}_{year}',\n",
    "        bucket=BUCKET,\n",
    "        fileNamePrefix=f'{OUTPUT_PATH}/{year}/chunk_{chunk_id:02d}/era5_data',\n",
    "        fileFormat='CSV'\n",
    "    )\n",
    "    \n",
    "    exports = [\n",
    "        {'task': s1_task, 'type': 'S1', 'count': s1_count},\n",
    "        {'task': s2_task, 'type': 'S2', 'count': s2_count},\n",
    "        {'task': era5_task, 'type': 'ERA5', 'count': era5_count}\n",
    "    ]\n",
    "    \n",
    "    return exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Main Export Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks to process: 13\n",
      "Years to process: [2019, 2020, 2021]\n",
      "Total exports: 117 (chunks × years × 3 datasets)\n",
      "\n",
      "Chunk statistics:\n",
      "    chunk_id  n_lakes    lat_min    lat_max     lon_min     lon_max\n",
      "0          0     2083  69.160813  70.902079 -153.130795 -151.727647\n",
      "1          1     1452  69.006262  70.827977 -159.368810 -157.856992\n",
      "2          2     1327  69.040671  70.501113 -150.347535 -148.888885\n",
      "3          3     2518  69.006075  71.117631 -155.556146 -154.195153\n",
      "4          4      780  69.027426  70.294397 -163.578774 -160.999169\n",
      "5          5      223  69.061209  70.121377 -144.849551 -141.047786\n",
      "6          6     1987  69.022434  71.334582 -157.932951 -156.619012\n",
      "7          7      910  69.000834  70.388435 -149.010972 -147.408194\n",
      "8          8     2044  69.016535  70.903127 -154.464362 -153.026821\n",
      "9          9     1383  69.000146  70.478816 -151.904233 -150.237160\n",
      "10        10      903  69.021021  70.839688 -161.140899 -159.325726\n",
      "11        11     2148  69.298632  71.360948 -156.626150 -155.381872\n",
      "12        12      452  69.036249  70.184548 -147.406392 -145.008440\n"
     ]
    }
   ],
   "source": [
    "# Load chunk statistics to know how many chunks we have\n",
    "chunk_stats = pd.read_csv(f'gs://{BUCKET}/{BASE_PATH}/processed/chunk_statistics.csv')\n",
    "n_chunks = len(chunk_stats)\n",
    "\n",
    "print(f\"Total chunks to process: {n_chunks}\")\n",
    "print(f\"Years to process: {YEARS}\")\n",
    "print(f\"Total exports: {n_chunks * len(YEARS) * 3} (chunks × years × 3 datasets)\")\n",
    "print(\"\\nChunk statistics:\")\n",
    "print(chunk_stats[['chunk_id', 'n_lakes', 'lat_min', 'lat_max', 'lon_min', 'lon_max']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "TEST RUN: Chunk 0, Year 2019\n",
      "############################################################\n",
      "  Loaded chunk 0: 2083 lakes\n",
      "    Bounds: [-153.14164553   69.15976659 -151.71727682   70.91525432]\n",
      "  Simplifying geometries...\n",
      "  Creating EE FeatureCollection...\n",
      "  FeatureCollection created successfully\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 0, Year 2019\n",
      "============================================================\n",
      "Lakes in chunk: 2083\n",
      "\n",
      "Processing Sentinel-1...\n",
      "  S1 observations: 245794\n",
      "\n",
      "Processing Sentinel-2...\n",
      "  S2 observations: 178567\n",
      "\n",
      "Processing ERA5 temperature...\n",
      "  Loading ERA5 hourly data for 2019...\n",
      "  Pre-computing daily means...\n",
      "  Sampling all lakes from daily means...\n",
      "  ERA5 observations: 760295\n",
      "\n",
      "============================================================\n",
      "TEST EXPORTS PREPARED (NOT STARTED)\n",
      "============================================================\n",
      "  S1: 245794 observations ready\n",
      "  S2: 178567 observations ready\n",
      "  ERA5: 760295 observations ready\n",
      "\n",
      "To start exports, run the cells below.\n"
     ]
    }
   ],
   "source": [
    "# Test with one chunk and one year first\n",
    "TEST_CHUNK = 0\n",
    "TEST_YEAR = 2019\n",
    "\n",
    "print(f\"\\n{'#'*60}\")\n",
    "print(f\"TEST RUN: Chunk {TEST_CHUNK}, Year {TEST_YEAR}\")\n",
    "print(f\"{'#'*60}\")\n",
    "\n",
    "# Load chunk\n",
    "test_fc, test_gdf = load_chunk_from_bucket(TEST_CHUNK)\n",
    "test_bounds = ee.Geometry.Rectangle(test_gdf.total_bounds.tolist())\n",
    "\n",
    "# Process and export\n",
    "test_exports = export_chunk_year(TEST_CHUNK, TEST_YEAR, test_fc, test_bounds)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TEST EXPORTS PREPARED (NOT STARTED)\")\n",
    "print(f\"{'='*60}\")\n",
    "for exp in test_exports:\n",
    "    print(f\"  {exp['type']}: {exp['count']} observations ready\")\n",
    "print(\"\\nTo start exports, run the cells below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏭️  Skipping test exports (SKIP_TEST = True)\n",
      "Test exports already completed. Proceed to full export below.\n"
     ]
    }
   ],
   "source": [
    "# TEST EXPORTS\n",
    "# Use this to test the export on one chunk. Can skip it in future runs once working\n",
    "SKIP_TEST = True  #True to skip this cell. \n",
    "\n",
    "if not SKIP_TEST:\n",
    "    print(\"Starting test exports...\")\n",
    "    for exp in test_exports:\n",
    "        exp['task'].start()\n",
    "        print(f\"  Started: {exp['task'].status()['description']}\")\n",
    "    \n",
    "    print(\"\\nTest exports started! Monitor at: https://code.earthengine.google.com/tasks\")\n",
    "    print(\"\\nOnce test completes successfully, proceed to full export below.\")\n",
    "else:\n",
    "    print(\"⏭️  Skipping test exports (SKIP_TEST = True)\")\n",
    "    print(\"Test exports already completed. Proceed to full export below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Full Export (All Chunks, All Years)\n",
    "\n",
    "**WARNING:** This will start ~57 export tasks (19 chunks × 3 years × 3 datasets each)\n",
    "\n",
    "Only run after test export completes successfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare all exports (run once test is good)\n",
    "all_exports = []\n",
    "\n",
    "for year in YEARS:\n",
    "    for chunk_id in range(n_chunks):\n",
    "        print(f\"\\nPreparing Chunk {chunk_id}, Year {year}...\")\n",
    "        \n",
    "        # Load chunk\n",
    "        chunk_fc, chunk_gdf = load_chunk_from_bucket(chunk_id)\n",
    "        chunk_bounds = ee.Geometry.Rectangle(chunk_gdf.total_bounds.tolist())\n",
    "        \n",
    "        # Prepare exports\n",
    "        exports = export_chunk_year(chunk_id, year, chunk_fc, chunk_bounds)\n",
    "        \n",
    "        for exp in exports:\n",
    "            all_exports.append({\n",
    "                'chunk_id': chunk_id,\n",
    "                'year': year,\n",
    "                'type': exp['type'],\n",
    "                'task': exp['task'],\n",
    "                'count': exp['count']\n",
    "            })\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ALL EXPORTS PREPARED: {len(all_exports)} tasks\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"\\nReady to start. Run next cell to begin all exports.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START ALL EXPORTS\n",
    "# Only run after careful review!\n",
    "\n",
    "print(f\"Starting {len(all_exports)} export tasks...\")\n",
    "print(\"This may take a few minutes to submit all tasks.\\n\")\n",
    "\n",
    "for i, exp in enumerate(all_exports):\n",
    "    exp['task'].start()\n",
    "    \n",
    "    if (i+1) % 10 == 0:\n",
    "        print(f\"  Started {i+1}/{len(all_exports)} tasks...\")\n",
    "        time.sleep(2)  # Brief pause to avoid overwhelming GEE\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ALL {len(all_exports)} EXPORTS STARTED!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"\\nMonitor progress at: https://code.earthengine.google.com/tasks\")\n",
    "print(f\"\\nOutputs will be in: gs://{BUCKET}/{OUTPUT_PATH}/YEAR/chunk_XX/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Monitor Export Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check status of all exports\n",
    "def check_export_status():\n",
    "    status_summary = {\n",
    "        'READY': 0,\n",
    "        'RUNNING': 0,\n",
    "        'COMPLETED': 0,\n",
    "        'FAILED': 0,\n",
    "        'CANCELLED': 0\n",
    "    }\n",
    "    \n",
    "    for exp in all_exports:\n",
    "        status = exp['task'].status()['state']\n",
    "        status_summary[status] = status_summary.get(status, 0) + 1\n",
    "    \n",
    "    print(f\"Export Status Summary:\")\n",
    "    print(f\"  Total tasks: {len(all_exports)}\")\n",
    "    for state, count in status_summary.items():\n",
    "        if count > 0:\n",
    "            print(f\"    {state}: {count}\")\n",
    "    \n",
    "    return status_summary\n",
    "\n",
    "# Run this cell periodically to check progress\n",
    "check_export_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook exports:\n",
    "- **Sentinel-1**: VV/VH backscatter for each lake interior\n",
    "- **Sentinel-2**: NDSI-based ice fraction (with cloud filtering)\n",
    "- **ERA5**: Daily mean temperature at lake locations\n",
    "\n",
    "**For:**\n",
    "- ~19 spatial chunks\n",
    "- 3 years (2019, 2020, 2021)\n",
    "- ~28,000 North Slope lakes\n",
    "\n",
    "**Output structure:**\n",
    "```\n",
    "gs://wustl-eeps-geospatial/thermokarst_lakes/exports/\n",
    "├── 2019/\n",
    "│   ├── chunk_00/\n",
    "│   │   ├── s1_data.csv\n",
    "│   │   ├── s2_data.csv\n",
    "│   │   └── era5_data.csv\n",
    "│   ├── chunk_01/\n",
    "│   └── ...\n",
    "├── 2020/\n",
    "└── 2021/\n",
    "```\n",
    "\n",
    "**Next step:** Combine CSVs and run ice detection algorithm (Notebook 03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "gee",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "GEE Geospatial (Local)",
   "language": "python",
   "name": "gee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Year Lake Ice Phenology Data Export\n",
    "## Part 2: GEE Processing and Export (2019-2021)\n",
    "\n",
    "**Goal:** Export S1 + S2 + ERA5 data for North Slope lakes across 3 years\n",
    "\n",
    "**Strategy:**\n",
    "- Process chunks independently (spatial parallelization)\n",
    "- Export one year at a time per chunk\n",
    "- Use efficient spatial filtering (only process S2 images that overlap lakes)\n",
    "- Total exports: ~19 chunks × 3 years = ~57 exports\n",
    "\n",
    "**Data sources:**\n",
    "- Sentinel-1 GRD (SAR)\n",
    "- Sentinel-2 SR Harmonized (optical, for NDSI)\n",
    "- ERA5-Land (temperature)\n",
    "\n",
    "**Years:** 2019, 2020, 2021 (match ALPOD temporal coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gee/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n",
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n",
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n",
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n",
      "Earth Engine initialized: GEE Initialized\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import xarray as xr\n",
    "\n",
    "# Initialize Earth Engine\n",
    "ee.Initialize()\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"Earth Engine initialized: {ee.String('GEE Initialized').getInfo()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Years: [2019, 2020, 2021]\n",
      "  Chunks path: gs://wustl-eeps-geospatial/thermokarst_lakes/processed/chunks\n",
      "  Output: gs://wustl-eeps-geospatial/thermokarst_lakes/exports\n",
      "  S2 scene cloud threshold: 30%\n",
      "  S2 pixel cloud probability threshold: 40%\n",
      "  S2 time window: ±3 days\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BUCKET = 'wustl-eeps-geospatial'\n",
    "BASE_PATH = 'thermokarst_lakes'\n",
    "CHUNKS_PATH = f'gs://{BUCKET}/{BASE_PATH}/processed/chunks'\n",
    "OUTPUT_PATH = f'{BASE_PATH}/exports'  # No gs:// prefix for GEE exports\n",
    "\n",
    "# Years to process (match ALPOD coverage)\n",
    "YEARS = [2019, 2020, 2021]\n",
    "\n",
    "# Processing parameters\n",
    "SCALE = 10  # Sentinel-1 resolution\n",
    "S2_NDSI_THRESHOLD = 0.4  # NDSI > 0.4 = ice\n",
    "S2_CLOUD_THRESHOLD = 30  # Maximum cloud cover for S2 images (scene-level)\n",
    "S2_CLOUD_PROB_THRESHOLD = 40  # s2cloudless probability threshold (pixel-level)\n",
    "S2_TIME_WINDOW = 3  # Days before/after S1 acquisition to look for S2\n",
    "\n",
    "# Projection\n",
    "ALASKA_ALBERS = 'EPSG:3338'\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Years: {YEARS}\")\n",
    "print(f\"  Chunks path: {CHUNKS_PATH}\")\n",
    "print(f\"  Output: gs://{BUCKET}/{OUTPUT_PATH}\")\n",
    "print(f\"  S2 scene cloud threshold: {S2_CLOUD_THRESHOLD}%\")\n",
    "print(f\"  S2 pixel cloud probability threshold: {S2_CLOUD_PROB_THRESHOLD}%\")\n",
    "print(f\"  S2 time window: ±{S2_TIME_WINDOW} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upload ALPOD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ALPOD shapefile from GCS...\n",
      "Loaded 801,895 lakes\n",
      "Fixing invalid geometries...\n",
      "  Found 209 invalid geometries\n",
      "  After cleaning: 801,894 lakes\n",
      "Converting to GEE FeatureCollection...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# UPLOAD ALPOD TO GEE ASSET (ONE-TIME ONLY)\n",
    "# =============================================================================\n",
    "# Set to False after asset is uploaded\n",
    "UPLOAD_ALPOD_ASSET = True\n",
    "\n",
    "if UPLOAD_ALPOD_ASSET:\n",
    "    print(\"Loading ALPOD shapefile from GCS...\")\n",
    "    alpod_gdf = gpd.read_file('gs://wustl-eeps-geospatial/thermokarst_lakes/ALPODlakes/ALPODlakes.shp')\n",
    "    print(f\"Loaded {len(alpod_gdf):,} lakes\")\n",
    "\n",
    "    # Fix invalid geometries\n",
    "    print(\"Fixing invalid geometries...\")\n",
    "    invalid_count = (~alpod_gdf.is_valid).sum()\n",
    "    print(f\"  Found {invalid_count} invalid geometries\")\n",
    "    \n",
    "    alpod_gdf['geometry'] = alpod_gdf.geometry.buffer(0)  # Standard fix for invalid geometries\n",
    "    \n",
    "    # Remove any null/empty geometries\n",
    "    alpod_gdf = alpod_gdf[~alpod_gdf.geometry.is_empty & alpod_gdf.geometry.notna()]\n",
    "    print(f\"  After cleaning: {len(alpod_gdf):,} lakes\")\n",
    "\n",
    "    print(\"Converting to GEE FeatureCollection...\")\n",
    "    alpod_fc = ee.FeatureCollection(alpod_gdf.__geo_interface__)\n",
    "    print(\"Conversion complete\")\n",
    "\n",
    "    print(\"Starting asset upload...\")\n",
    "    task = ee.batch.Export.table.toAsset(\n",
    "        collection=alpod_fc,\n",
    "        description='ALPOD_full_upload',\n",
    "        assetId='projects/eeps-geospatial/assets/ALPOD_full'\n",
    "    )\n",
    "    task.start()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ASSET UPLOAD STARTED!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Check progress: https://code.earthengine.google.com/tasks\")\n",
    "    print(\"This may take 30-60 minutes for 800k lakes...\")\n",
    "    print(\"\\n⚠️  After upload completes, set UPLOAD_ALPOD_ASSET = False\")\n",
    "else:\n",
    "    print(\"Skipping ALPOD upload (already done)\")\n",
    "    print(\"Asset location: projects/eeps-geospatial/assets/ALPOD_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunk_from_bucket(chunk_id):\n",
    "    \"\"\"\n",
    "    Load a chunk GeoJSON from bucket and convert to ee.FeatureCollection\n",
    "    Downloads to local temp file first to avoid GCS streaming issues\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    chunk_file = f'{CHUNKS_PATH}/chunk_{chunk_id:02d}.geojson'\n",
    "    \n",
    "    # Download to local temp file\n",
    "    local_path = f'/tmp/chunk_{chunk_id:02d}.geojson'\n",
    "    \n",
    "    # Use gsutil to download (reliable in Vertex AI)\n",
    "    os.system(f'gsutil -q cp {chunk_file} {local_path}')\n",
    "    \n",
    "    # Load from local file\n",
    "    gdf = gpd.read_file(local_path)\n",
    "    \n",
    "    print(f\"  Loaded chunk {chunk_id}: {len(gdf)} lakes\")\n",
    "    print(f\"    Bounds: {gdf.total_bounds}\")\n",
    "    \n",
    "    # Keep only essential properties to reduce payload size\n",
    "    essential_cols = ['geometry']\n",
    "    \n",
    "    if 'lake_area_km2' in gdf.columns:\n",
    "        essential_cols.append('lake_area_km2')\n",
    "    \n",
    "    # Create simple ID if needed\n",
    "    gdf['lake_id'] = range(len(gdf))\n",
    "    essential_cols.append('lake_id')\n",
    "    \n",
    "    gdf_simplified = gdf[essential_cols].copy()\n",
    "    \n",
    "    # Simplify geometries to reduce size (5m tolerance)\n",
    "    print(f\"  Simplifying geometries...\")\n",
    "    gdf_simplified['geometry'] = gdf_simplified.geometry.simplify(\n",
    "        tolerance=5, preserve_topology=True\n",
    "    )\n",
    "    \n",
    "    # Convert to GeoJSON dict\n",
    "    geojson = gdf_simplified.__geo_interface__\n",
    "    \n",
    "    print(f\"  Creating EE FeatureCollection...\")\n",
    "    fc = ee.FeatureCollection(geojson)\n",
    "    print(f\"  FeatureCollection created successfully\")\n",
    "    \n",
    "    return fc, gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lake_geometry_metrics(lakes_fc, region_bounds):\n",
    "    \"\"\"\n",
    "    Add lake interior and landscape ring geometries\n",
    "    Uses GEE Asset for ALPOD (avoids payload bloat)\n",
    "    \"\"\"\n",
    "    # Load ALPOD from GEE Asset (tiny reference, not 800k geometries!)\n",
    "    all_alpod = ee.FeatureCollection('projects/eeps-geospatial/assets/ALPOD_full')\n",
    "    \n",
    "    # Filter to region (with buffer for edge lakes)\n",
    "    search_bounds = region_bounds.buffer(500)\n",
    "    all_alpod_nearby = all_alpod.filterBounds(search_bounds)\n",
    "    \n",
    "    # Dissolve all nearby lakes for landscape masking\n",
    "    all_lakes_dissolved = all_alpod_nearby.geometry().dissolve(maxError=10)\n",
    "    \n",
    "    def add_geometries(lake):\n",
    "        lake_geom = lake.geometry()\n",
    "        lake_id = lake.get('lake_id')\n",
    "        \n",
    "        # Lake interior: 10m inward buffer\n",
    "        lake_interior = lake_geom.buffer(-10)\n",
    "        \n",
    "        # Landscape ring: 100m outward buffer, minus ALL lakes\n",
    "        landscape_ring = lake_geom.buffer(100).difference(all_lakes_dissolved)\n",
    "        \n",
    "        return lake.set({\n",
    "            'lake_id': lake_id,\n",
    "            'lake_interior': lake_interior,\n",
    "            'landscape_ring': landscape_ring\n",
    "        })\n",
    "    \n",
    "    return lakes_fc.map(add_geometries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sentinel-1 Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentinel1(lakes_fc, year, region_bounds):\n",
    "    \"\"\"\n",
    "    Extract S1 features - simplified to avoid payload bloat\n",
    "    \"\"\"\n",
    "    s1 = (ee.ImageCollection('COPERNICUS/S1_GRD')\n",
    "          .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "          .filterBounds(region_bounds)\n",
    "          .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
    "          .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n",
    "          .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
    "          .filter(ee.Filter.eq('orbitProperties_pass', 'DESCENDING'))\n",
    "          .select(['VV', 'VH']))\n",
    "    \n",
    "    def extract_s1_features(img):\n",
    "        # Sample lake interiors\n",
    "        lake_samples = img.reduceRegions(\n",
    "            collection=lakes_fc.select(['lake_id', 'lake_interior']),\n",
    "            reducer=ee.Reducer.mean(),\n",
    "            scale=10\n",
    "        )\n",
    "        \n",
    "        # Sample landscape rings\n",
    "        landscape_samples = img.reduceRegions(\n",
    "            collection=lakes_fc.select(['lake_id', 'landscape_ring']),\n",
    "            reducer=ee.Reducer.mean(),\n",
    "            scale=10\n",
    "        )\n",
    "        \n",
    "        # Join and create output features\n",
    "        def create_s1_feature(lake_feat):\n",
    "            lake_id = lake_feat.get('lake_id')\n",
    "            \n",
    "            # Find matching landscape sample\n",
    "            land_feat = landscape_samples.filter(\n",
    "                ee.Filter.eq('lake_id', lake_id)\n",
    "            ).first()\n",
    "            \n",
    "            # Lake values\n",
    "            vv_lake = ee.Number(lake_feat.get('VV'))\n",
    "            vh_lake = ee.Number(lake_feat.get('VH'))\n",
    "            \n",
    "            # Landscape values\n",
    "            vv_land = ee.Number(land_feat.get('VV'))\n",
    "            vh_land = ee.Number(land_feat.get('VH'))\n",
    "            \n",
    "            # Derived features\n",
    "            vv_vh_ratio = vv_lake.subtract(vh_lake)\n",
    "            lake_R = vv_lake.unitScale(-20, -5).multiply(255)\n",
    "            lake_G = vh_lake.unitScale(-28, -12).multiply(255)\n",
    "            lake_B = vv_vh_ratio.unitScale(8, 18).multiply(255)\n",
    "            land_R = vv_land.unitScale(-20, -5).multiply(255)\n",
    "            land_G = vh_land.unitScale(-28, -12).multiply(255)\n",
    "            land_vv_vh = vv_land.subtract(vh_land)\n",
    "            land_B = land_vv_vh.unitScale(8, 18).multiply(255)\n",
    "            \n",
    "            # Get date from image\n",
    "            date = img.date()\n",
    "            \n",
    "            # Return simple feature with NO geometry and NO complex references\n",
    "            return ee.Feature(None, {\n",
    "                'lake_id': lake_id,\n",
    "                's1_date': date.format('YYYY-MM-dd'),\n",
    "                's1_doy': date.getRelative('day', 'year'),\n",
    "                'vv_db': vv_lake,\n",
    "                'vh_db': vh_lake,\n",
    "                'vv_vh_ratio': vv_vh_ratio,\n",
    "                'lake_R': lake_R,\n",
    "                'lake_G': lake_G,\n",
    "                'lake_B': lake_B,\n",
    "                'land_R': land_R,\n",
    "                'land_G': land_G,\n",
    "                'land_B': land_B,\n",
    "                'vv_land_db': vv_land,\n",
    "                'vh_land_db': vh_land\n",
    "            })\n",
    "        \n",
    "        return lake_samples.map(create_s1_feature)\n",
    "    \n",
    "    s1_features = s1.map(extract_s1_features).flatten()\n",
    "    return s1_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sentinel-2 Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ndsi(img):\n",
    "    \"\"\"\n",
    "    Compute Normalized Difference Snow Index (NDSI)\n",
    "    NDSI = (Green - SWIR1) / (Green + SWIR1)\n",
    "    \"\"\"\n",
    "    green = img.select('B3')\n",
    "    swir1 = img.select('B11')\n",
    "    \n",
    "    ndsi = green.subtract(swir1).divide(green.add(swir1)).rename('ndsi')\n",
    "    \n",
    "    return img.addBands(ndsi)\n",
    "\n",
    "def mask_s2_clouds(img):\n",
    "    \"\"\"\n",
    "    Mask clouds using QA60 band (basic cloud mask)\n",
    "    \"\"\"\n",
    "    qa = img.select('QA60')\n",
    "    \n",
    "    # Bits 10 and 11 are clouds and cirrus\n",
    "    cloud_bit_mask = 1 << 10\n",
    "    cirrus_bit_mask = 1 << 11\n",
    "    \n",
    "    # Both should be zero (clear conditions)\n",
    "    mask = qa.bitwiseAnd(cloud_bit_mask).eq(0).And(\n",
    "           qa.bitwiseAnd(cirrus_bit_mask).eq(0))\n",
    "    \n",
    "    return img.updateMask(mask)\n",
    "\n",
    "def add_s2cloudless_mask(img):\n",
    "    s2_cloudless = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "    cloud_prob_collection = s2_cloudless.filter(\n",
    "        ee.Filter.eq('system:index', img.get('system:index'))\n",
    "    )\n",
    "    \n",
    "    has_cloud_data = cloud_prob_collection.size().gt(0)\n",
    "    \n",
    "    def apply_s2cloudless_mask():\n",
    "        cloud_prob = cloud_prob_collection.first().select('probability')\n",
    "        is_clear = cloud_prob.lt(S2_CLOUD_PROB_THRESHOLD)\n",
    "        return img.updateMask(is_clear)\n",
    "    \n",
    "    def use_qa60_only():\n",
    "        return img  # Already has QA60 mask\n",
    "    \n",
    "    return ee.Image(ee.Algorithms.If(\n",
    "        has_cloud_data,\n",
    "        apply_s2cloudless_mask(),\n",
    "        use_qa60_only()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentinel2(lakes_fc, year, region_bounds):\n",
    "    \"\"\"\n",
    "    Extract S2 features - simplified to avoid payload bloat\n",
    "    \"\"\"\n",
    "    s2 = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "          .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "          .filterBounds(region_bounds)\n",
    "          .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', S2_CLOUD_THRESHOLD)))\n",
    "    \n",
    "    def mask_s2_clouds(img):\n",
    "        qa = img.select('QA60')\n",
    "        cloud_bit_mask = 1 << 10\n",
    "        cirrus_bit_mask = 1 << 11\n",
    "        mask = qa.bitwiseAnd(cloud_bit_mask).eq(0).And(\n",
    "               qa.bitwiseAnd(cirrus_bit_mask).eq(0))\n",
    "        return img.updateMask(mask)\n",
    "    \n",
    "    s2 = s2.map(mask_s2_clouds).map(add_s2cloudless_mask)\n",
    "    \n",
    "    def add_ndsi(img):\n",
    "        ndsi = img.normalizedDifference(['B3', 'B11']).rename('ndsi')\n",
    "        return img.addBands(ndsi)\n",
    "    \n",
    "    s2 = s2.map(add_ndsi)\n",
    "    \n",
    "    def extract_s2_features(img):\n",
    "        # Sample lake interiors only\n",
    "        samples = img.select('ndsi').reduceRegions(\n",
    "            collection=lakes_fc.select(['lake_id', 'lake_interior']),\n",
    "            reducer=ee.Reducer.mean(),\n",
    "            scale=20\n",
    "        )\n",
    "        \n",
    "        def create_s2_feature(feat):\n",
    "            ndsi_mean = ee.Number(feat.get('mean'))\n",
    "            ice_fraction = ee.Algorithms.If(\n",
    "                ndsi_mean.gte(S2_NDSI_THRESHOLD),\n",
    "                1.0,\n",
    "                0.0\n",
    "            )\n",
    "            \n",
    "            date = img.date()\n",
    "            \n",
    "            return ee.Feature(None, {\n",
    "                'lake_id': feat.get('lake_id'),\n",
    "                's2_date': date.format('YYYY-MM-dd'),\n",
    "                's2_doy': date.getRelative('day', 'year'),\n",
    "                's2_ice_fraction': ice_fraction\n",
    "            })\n",
    "        \n",
    "        return samples.map(create_s2_feature)\n",
    "    \n",
    "    s2_features = s2.map(extract_s2_features).flatten()\n",
    "    return s2_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ERA5 Temperature Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_era5_temperature(lakes_fc, year):\n",
    "    \"\"\"\n",
    "    Process ERA5-Land temperature data - OPTIMIZED VERSION\n",
    "    Pre-computes daily means once, then samples all lakes in batch\n",
    "    Much faster than computing daily mean separately for each lake\n",
    "    \"\"\"\n",
    "    print(f\"  Loading ERA5 hourly data for {year}...\")\n",
    "    \n",
    "    # Load ERA5-Land hourly data\n",
    "    era5_hourly = (ee.ImageCollection('ECMWF/ERA5_LAND/HOURLY')\n",
    "                   .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "                   .select('temperature_2m'))\n",
    "    \n",
    "    # Convert to Celsius\n",
    "    def to_celsius(img):\n",
    "        temp_c = img.subtract(273.15).rename('temp_c')\n",
    "        return temp_c.copyProperties(img, ['system:time_start'])\n",
    "    \n",
    "    era5_hourly = era5_hourly.map(to_celsius)\n",
    "    \n",
    "    print(f\"  Pre-computing daily means...\")\n",
    "    \n",
    "    # Determine number of days in year (handle leap years)\n",
    "    is_leap = ee.Number(year).mod(4).eq(0).And(\n",
    "        ee.Number(year).mod(100).neq(0).Or(\n",
    "            ee.Number(year).mod(400).eq(0)\n",
    "        )\n",
    "    )\n",
    "    n_days = ee.Number(ee.Algorithms.If(is_leap, 366, 365))\n",
    "    \n",
    "    # Pre-compute daily means for entire year (365 or 366 images)\n",
    "    def compute_daily_mean(day):\n",
    "        day = ee.Number(day)\n",
    "        date = ee.Date.fromYMD(year, 1, 1).advance(day.subtract(1), 'day')\n",
    "        next_date = date.advance(1, 'day')\n",
    "        \n",
    "        # Get all hourly images for this day\n",
    "        daily_collection = era5_hourly.filterDate(date, next_date)\n",
    "        \n",
    "        # Check if we have data\n",
    "        has_data = daily_collection.size().gt(0)\n",
    "        \n",
    "        # Compute mean if data exists, otherwise use missing flag\n",
    "        daily_mean = ee.Image(ee.Algorithms.If(\n",
    "            has_data,\n",
    "            daily_collection.mean(),\n",
    "            ee.Image.constant(-9999).rename('temp_c')\n",
    "        ))\n",
    "        \n",
    "        return daily_mean.set({\n",
    "            'system:time_start': date.millis(),\n",
    "            'doy': day,\n",
    "            'date': date.format('YYYY-MM-dd')\n",
    "        })\n",
    "    \n",
    "    days = ee.List.sequence(1, n_days)\n",
    "    era5_daily = ee.ImageCollection.fromImages(days.map(compute_daily_mean))\n",
    "    \n",
    "    print(f\"  Sampling all lakes from daily means...\")\n",
    "    \n",
    "    # Now sample ALL lakes from each daily mean (batch operation)\n",
    "    def sample_all_lakes(daily_img):\n",
    "        doy = daily_img.get('doy')\n",
    "        date = daily_img.get('date')\n",
    "        \n",
    "        # Sample ALL lakes at once using reduceRegions\n",
    "        samples = daily_img.reduceRegions(\n",
    "            collection=lakes_fc,\n",
    "            reducer=ee.Reducer.first(),  # Get pixel value at centroid\n",
    "            scale=11000  # ERA5-Land resolution\n",
    "        )\n",
    "        \n",
    "        # Add date info to each sampled feature\n",
    "        def add_date_info(feat):\n",
    "            # Get the temperature value (from 'first' property created by reducer)\n",
    "            # Use ee.Algorithms.If to provide default if missing\n",
    "            temp_value = ee.Algorithms.If(\n",
    "                feat.propertyNames().contains('first'),\n",
    "                feat.get('first'),\n",
    "                -9999\n",
    "            )\n",
    "            \n",
    "            return feat.set({\n",
    "                'era5_date': date,\n",
    "                'era5_doy': doy,\n",
    "                'temp_c': temp_value\n",
    "            })\n",
    "        \n",
    "        return samples.map(add_date_info)\n",
    "    \n",
    "    # Process all daily images\n",
    "    era5_features = era5_daily.map(sample_all_lakes).flatten()\n",
    "    \n",
    "    return era5_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Export Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_chunk_year(chunk_id, year, lakes_fc, region_bounds):\n",
    "    \"\"\"\n",
    "    Process and export S1/S2 data for one chunk and one year\n",
    "    NOTE: ERA5 is handled separately (too large for GEE export)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing Chunk {chunk_id}, Year {year}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Add lake geometries\n",
    "    lakes_with_geom = add_lake_geometry_metrics(lakes_fc, region_bounds)\n",
    "    print(\"Lakes processed with geometries\")\n",
    "    \n",
    "    # Process S1\n",
    "    print(\"\\nProcessing Sentinel-1...\")\n",
    "    s1_features = process_sentinel1(lakes_with_geom, year, region_bounds)\n",
    "    print(\"  S1 processing complete\")\n",
    "    \n",
    "    # Process S2\n",
    "    print(\"\\nProcessing Sentinel-2...\")\n",
    "    s2_features = process_sentinel2(lakes_with_geom, year, region_bounds)\n",
    "    print(\"  S2 processing complete\")\n",
    "    \n",
    "    # ERA5 will be downloaded separately\n",
    "    print(\"\\nERA5 temperature...\")\n",
    "    print(\"  (Will be downloaded separately - more efficient than GEE export)\")\n",
    "    \n",
    "    # Export S1 and S2 only\n",
    "    exports = []\n",
    "    \n",
    "    # S1 export\n",
    "    s1_task = ee.batch.Export.table.toCloudStorage(\n",
    "        collection=s1_features,\n",
    "        description=f'S1_chunk{chunk_id:02d}_{year}',\n",
    "        bucket=BUCKET,\n",
    "        fileNamePrefix=f'{OUTPUT_PATH}/{year}/chunk_{chunk_id:02d}/s1_data',\n",
    "        fileFormat='CSV'\n",
    "    )\n",
    "    \n",
    "    # S2 export\n",
    "    s2_task = ee.batch.Export.table.toCloudStorage(\n",
    "        collection=s2_features,\n",
    "        description=f'S2_chunk{chunk_id:02d}_{year}',\n",
    "        bucket=BUCKET,\n",
    "        fileNamePrefix=f'{OUTPUT_PATH}/{year}/chunk_{chunk_id:02d}/s2_data',\n",
    "        fileFormat='CSV'\n",
    "    )\n",
    "    \n",
    "    # Return only S1 and S2 tasks (no ERA5)\n",
    "    exports = [\n",
    "        {'task': s1_task, 'type': 'S1', 'count': 'N/A'},\n",
    "        {'task': s2_task, 'type': 'S2', 'count': 'N/A'}\n",
    "    ]\n",
    "    \n",
    "    return exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Main Export Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks to process: 13\n",
      "Years to process: [2019, 2020, 2021]\n",
      "Total exports: 117 (chunks × years × 3 datasets)\n",
      "\n",
      "Chunk statistics:\n",
      "    chunk_id  n_lakes    lat_min    lat_max     lon_min     lon_max\n",
      "0          0     2083  69.160813  70.902079 -153.130795 -151.727647\n",
      "1          1     1452  69.006262  70.827977 -159.368810 -157.856992\n",
      "2          2     1327  69.040671  70.501113 -150.347535 -148.888885\n",
      "3          3     2518  69.006075  71.117631 -155.556146 -154.195153\n",
      "4          4      780  69.027426  70.294397 -163.578774 -160.999169\n",
      "5          5      223  69.061209  70.121377 -144.849551 -141.047786\n",
      "6          6     1987  69.022434  71.334582 -157.932951 -156.619012\n",
      "7          7      910  69.000834  70.388435 -149.010972 -147.408194\n",
      "8          8     2044  69.016535  70.903127 -154.464362 -153.026821\n",
      "9          9     1383  69.000146  70.478816 -151.904233 -150.237160\n",
      "10        10      903  69.021021  70.839688 -161.140899 -159.325726\n",
      "11        11     2148  69.298632  71.360948 -156.626150 -155.381872\n",
      "12        12      452  69.036249  70.184548 -147.406392 -145.008440\n"
     ]
    }
   ],
   "source": [
    "# Load chunk statistics to know how many chunks we have\n",
    "chunk_stats = pd.read_csv(f'gs://{BUCKET}/{BASE_PATH}/processed/chunk_statistics.csv')\n",
    "n_chunks = len(chunk_stats)\n",
    "\n",
    "print(f\"Total chunks to process: {n_chunks}\")\n",
    "print(f\"Years to process: {YEARS}\")\n",
    "print(f\"Total exports: {n_chunks * len(YEARS) * 3} (chunks × years × 3 datasets)\")\n",
    "print(\"\\nChunk statistics:\")\n",
    "print(chunk_stats[['chunk_id', 'n_lakes', 'lat_min', 'lat_max', 'lon_min', 'lon_max']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "TEST RUN: Chunk 0, Year 2019\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: PROJ: proj_create_from_database: Open of /opt/conda/envs/gee/share/proj failed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded chunk 0: 2083 lakes\n",
      "    Bounds: [  32045.19937452 2133331.56155297   87259.03944725 2325197.90976039]\n",
      "  Simplifying geometries...\n",
      "  Creating EE FeatureCollection...\n",
      "  FeatureCollection created successfully\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 0, Year 2019\n",
      "============================================================\n",
      "  Loading full ALPOD dataset for landscape masking...\n",
      "  CRS Strategy: All geometric operations in Alaska Albers (EPSG:3338)\n",
      "    ALPOD loaded: 801895 lakes, CRS: EPSG:3338\n",
      "    Chunk bounds (WGS84): [-153.15, -151.71] × [69.15, 70.93]\n",
      "    Converting ALPOD from EPSG:3338 to EPSG:4326 for bbox filtering...\n",
      "    Found 12900 total lakes in chunk region (all sizes)\n",
      "    Converting filtered lakes back to Alaska Albers for geometric operations...\n",
      "    Simplified geometries (5m tolerance in Alaska Albers)\n",
      "    Created EE FeatureCollection with 12900 lakes\n",
      "    EE will respect embedded CRS (Alaska Albers EPSG:3338)\n",
      "  Creating lake interior buffers and landscape rings...\n",
      "    Using EPSG:3338 (Alaska Albers) for all geometric operations\n",
      "  Geometric transformations complete\n",
      "    All buffers computed in Alaska Albers (EPSG:3338)\n",
      "    Buffer distances: -10m interior, +100m landscape ring\n",
      "Lakes processed with geometries\n",
      "\n",
      "Processing Sentinel-1...\n",
      "  S1 processing complete\n",
      "\n",
      "Processing Sentinel-2...\n",
      "  S2 processing complete\n",
      "\n",
      "ERA5 temperature...\n",
      "  (Will be downloaded separately - more efficient than GEE export)\n",
      "\n",
      "============================================================\n",
      "TEST EXPORTS PREPARED (NOT STARTED)\n",
      "============================================================\n",
      "  S1: N/A observations ready\n",
      "  S2: N/A observations ready\n",
      "\n",
      "To start exports, run the cells below.\n"
     ]
    }
   ],
   "source": [
    "# Test with one chunk and one year first\n",
    "TEST_CHUNK = 0\n",
    "TEST_YEAR = 2019\n",
    "\n",
    "print(f\"\\n{'#'*60}\")\n",
    "print(f\"TEST RUN: Chunk {TEST_CHUNK}, Year {TEST_YEAR}\")\n",
    "print(f\"{'#'*60}\")\n",
    "\n",
    "# Load chunk\n",
    "test_fc, test_gdf = load_chunk_from_bucket(TEST_CHUNK)\n",
    "test_gdf_wgs84 = test_gdf.to_crs('EPSG:4326') \n",
    "test_bounds = ee.Geometry.Rectangle(test_gdf_wgs84.total_bounds.tolist())  # wgs84\n",
    "\n",
    "# Process and export\n",
    "test_exports = export_chunk_year(TEST_CHUNK, TEST_YEAR, test_fc, test_bounds)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TEST EXPORTS PREPARED (NOT STARTED)\")\n",
    "print(f\"{'='*60}\")\n",
    "for exp in test_exports:\n",
    "    print(f\"  {exp['type']}: {exp['count']} observations ready\")\n",
    "print(\"\\nTo start exports, run the cells below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏭️  Skipping test exports (SKIP_TEST = True)\n",
      "Test exports already completed. Proceed to full export below.\n"
     ]
    }
   ],
   "source": [
    "# TEST EXPORTS\n",
    "# Use this to test the export on one chunk. Can skip it in future runs once working\n",
    "SKIP_TEST = True  #True to skip this cell. \n",
    "\n",
    "if not SKIP_TEST:\n",
    "    print(\"Starting test exports...\")\n",
    "    for exp in test_exports:\n",
    "        exp['task'].start()\n",
    "        print(f\"  Started: {exp['task'].status()['description']}\")\n",
    "    \n",
    "    print(\"\\nTest exports started! Monitor at: https://code.earthengine.google.com/tasks\")\n",
    "    print(\"\\nOnce test completes successfully, proceed to full export below.\")\n",
    "else:\n",
    "    print(\"⏭️  Skipping test exports (SKIP_TEST = True)\")\n",
    "    print(\"Test exports already completed. Proceed to full export below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Full Export (All Chunks, All Years)\n",
    "\n",
    "**WARNING:** This will prep ~78 export tasks (19 chunks × 3 years × 2 datasets each)\n",
    "\n",
    "Only run after test export completes successfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing Chunk 0 geometries (first time)\n",
      "============================================================\n",
      "  Loaded chunk 0: 2083 lakes\n",
      "    Bounds: [  32045.19937452 2133331.56155297   87259.03944725 2325197.90976039]\n",
      "  Simplifying geometries...\n",
      "  Creating EE FeatureCollection...\n",
      "  FeatureCollection created successfully\n",
      "  Adding lake geometry metrics (interior buffers + landscape rings)...\n",
      "  Loading full ALPOD dataset for landscape masking...\n",
      "  CRS Strategy: All geometric operations in Alaska Albers (EPSG:3338)\n",
      "    ALPOD loaded: 801895 lakes, CRS: EPSG:3338\n",
      "    Chunk bounds (WGS84): [-153.15, -151.71] × [69.15, 70.93]\n",
      "    Converting ALPOD from EPSG:3338 to EPSG:4326 for bbox filtering...\n",
      "    Found 12900 total lakes in chunk region (all sizes)\n",
      "    Converting filtered lakes back to Alaska Albers for geometric operations...\n",
      "    Simplified geometries (5m tolerance in Alaska Albers)\n",
      "    Created EE FeatureCollection with 12900 lakes\n",
      "    EE will respect embedded CRS (Alaska Albers EPSG:3338)\n",
      "  Creating lake interior buffers and landscape rings...\n",
      "    Using EPSG:3338 (Alaska Albers) for all geometric operations\n",
      "  Geometric transformations complete\n",
      "    All buffers computed in Alaska Albers (EPSG:3338)\n",
      "    Buffer distances: -10m interior, +100m landscape ring\n",
      "  ✓ Geometries computed in 84.3s\n",
      "  ✓ Chunk 0 geometry processing complete (88.4s)\n",
      "\n",
      "Processing Chunk 0, Year 2019...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2019 complete (215.4s)\n",
      "\n",
      "Processing Chunk 0, Year 2020...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2020 complete (214.3s)\n",
      "\n",
      "Processing Chunk 0, Year 2021...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2021 complete (212.3s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 1 geometries (first time)\n",
      "============================================================\n",
      "  Loaded chunk 1: 1452 lakes\n",
      "    Bounds: [-210626.60719623 2121103.48394851 -147893.30048031 2323753.71065851]\n",
      "  Simplifying geometries...\n",
      "  Creating EE FeatureCollection...\n",
      "  FeatureCollection created successfully\n",
      "  Adding lake geometry metrics (interior buffers + landscape rings)...\n",
      "  Loading full ALPOD dataset for landscape masking...\n",
      "  CRS Strategy: All geometric operations in Alaska Albers (EPSG:3338)\n",
      "    ALPOD loaded: 801895 lakes, CRS: EPSG:3338\n",
      "    Chunk bounds (WGS84): [-159.38, -157.84] × [69.00, 70.84]\n",
      "    Converting ALPOD from EPSG:3338 to EPSG:4326 for bbox filtering...\n",
      "    Found 14053 total lakes in chunk region (all sizes)\n",
      "    Converting filtered lakes back to Alaska Albers for geometric operations...\n",
      "    Simplified geometries (5m tolerance in Alaska Albers)\n",
      "    Created EE FeatureCollection with 14053 lakes\n",
      "    EE will respect embedded CRS (Alaska Albers EPSG:3338)\n",
      "  Creating lake interior buffers and landscape rings...\n",
      "    Using EPSG:3338 (Alaska Albers) for all geometric operations\n",
      "  Geometric transformations complete\n",
      "    All buffers computed in Alaska Albers (EPSG:3338)\n",
      "    Buffer distances: -10m interior, +100m landscape ring\n",
      "  ✓ Geometries computed in 77.5s\n",
      "  ✓ Chunk 1 geometry processing complete (80.8s)\n",
      "\n",
      "Processing Chunk 1, Year 2019...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2019 complete (162.0s)\n",
      "\n",
      "Processing Chunk 1, Year 2020...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2020 complete (160.1s)\n",
      "\n",
      "Processing Chunk 1, Year 2021...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2021 complete (159.2s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 2 geometries (first time)\n",
      "============================================================\n",
      "  Loaded chunk 2: 1327 lakes\n",
      "    Bounds: [ 139025.12624896 2126636.20611104  201270.80791334 2285799.70990154]\n",
      "  Simplifying geometries...\n",
      "  Creating EE FeatureCollection...\n",
      "  FeatureCollection created successfully\n",
      "  Adding lake geometry metrics (interior buffers + landscape rings)...\n",
      "  Loading full ALPOD dataset for landscape masking...\n",
      "  CRS Strategy: All geometric operations in Alaska Albers (EPSG:3338)\n",
      "    ALPOD loaded: 801895 lakes, CRS: EPSG:3338\n",
      "    Chunk bounds (WGS84): [-150.37, -148.87] × [69.03, 70.51]\n",
      "    Converting ALPOD from EPSG:3338 to EPSG:4326 for bbox filtering...\n",
      "    Found 15427 total lakes in chunk region (all sizes)\n",
      "    Converting filtered lakes back to Alaska Albers for geometric operations...\n",
      "    Simplified geometries (5m tolerance in Alaska Albers)\n",
      "    Created EE FeatureCollection with 15427 lakes\n",
      "    EE will respect embedded CRS (Alaska Albers EPSG:3338)\n",
      "  Creating lake interior buffers and landscape rings...\n",
      "    Using EPSG:3338 (Alaska Albers) for all geometric operations\n",
      "  Geometric transformations complete\n",
      "    All buffers computed in Alaska Albers (EPSG:3338)\n",
      "    Buffer distances: -10m interior, +100m landscape ring\n",
      "  ✓ Geometries computed in 75.0s\n",
      "  ✓ Chunk 2 geometry processing complete (78.1s)\n",
      "\n",
      "Processing Chunk 2, Year 2019...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2019 complete (158.0s)\n",
      "\n",
      "Processing Chunk 2, Year 2020...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2020 complete (159.1s)\n",
      "\n",
      "Processing Chunk 2, Year 2021...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2021 complete (157.3s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 3 geometries (first time)\n",
      "============================================================\n",
      "  Loaded chunk 3: 2518 lakes\n",
      "    Bounds: [ -62844.35829245 2115621.23962721   -7235.26906028 2347513.47113718]\n",
      "  Simplifying geometries...\n",
      "  Creating EE FeatureCollection...\n",
      "  FeatureCollection created successfully\n",
      "  Adding lake geometry metrics (interior buffers + landscape rings)...\n",
      "  Loading full ALPOD dataset for landscape masking...\n",
      "  CRS Strategy: All geometric operations in Alaska Albers (EPSG:3338)\n",
      "    ALPOD loaded: 801895 lakes, CRS: EPSG:3338\n",
      "    Chunk bounds (WGS84): [-155.57, -154.18] × [68.99, 71.13]\n",
      "    Converting ALPOD from EPSG:3338 to EPSG:4326 for bbox filtering...\n",
      "    Found 14588 total lakes in chunk region (all sizes)\n",
      "    Converting filtered lakes back to Alaska Albers for geometric operations...\n",
      "    Simplified geometries (5m tolerance in Alaska Albers)\n",
      "    Created EE FeatureCollection with 14588 lakes\n",
      "    EE will respect embedded CRS (Alaska Albers EPSG:3338)\n",
      "  Creating lake interior buffers and landscape rings...\n",
      "    Using EPSG:3338 (Alaska Albers) for all geometric operations\n",
      "  Geometric transformations complete\n",
      "    All buffers computed in Alaska Albers (EPSG:3338)\n",
      "    Buffer distances: -10m interior, +100m landscape ring\n",
      "  ✓ Geometries computed in 90.9s\n",
      "  ✓ Chunk 3 geometry processing complete (96.2s)\n",
      "\n",
      "Processing Chunk 3, Year 2019...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2019 complete (258.4s)\n",
      "\n",
      "Processing Chunk 3, Year 2020...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2020 complete (260.1s)\n",
      "\n",
      "Processing Chunk 3, Year 2021...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2021 complete (260.8s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 4 geometries (first time)\n",
      "============================================================\n",
      "  Loaded chunk 4: 780 lakes\n",
      "    Bounds: [-385038.31975939 2141903.3610496  -273646.65335548 2275209.73021333]\n",
      "  Simplifying geometries...\n",
      "  Creating EE FeatureCollection...\n",
      "  FeatureCollection created successfully\n",
      "  Adding lake geometry metrics (interior buffers + landscape rings)...\n",
      "  Loading full ALPOD dataset for landscape masking...\n",
      "  CRS Strategy: All geometric operations in Alaska Albers (EPSG:3338)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n",
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ALPOD loaded: 801895 lakes, CRS: EPSG:3338\n",
      "    Chunk bounds (WGS84): [-163.60, -160.98] × [69.01, 70.31]\n",
      "    Converting ALPOD from EPSG:3338 to EPSG:4326 for bbox filtering...\n",
      "    Found 5423 total lakes in chunk region (all sizes)\n",
      "    Converting filtered lakes back to Alaska Albers for geometric operations...\n",
      "    Simplified geometries (5m tolerance in Alaska Albers)\n",
      "    Created EE FeatureCollection with 5423 lakes\n",
      "    EE will respect embedded CRS (Alaska Albers EPSG:3338)\n",
      "  Creating lake interior buffers and landscape rings...\n",
      "    Using EPSG:3338 (Alaska Albers) for all geometric operations\n",
      "  Geometric transformations complete\n",
      "    All buffers computed in Alaska Albers (EPSG:3338)\n",
      "    Buffer distances: -10m interior, +100m landscape ring\n",
      "  ✓ Geometries computed in 57.4s\n",
      "  ✓ Chunk 4 geometry processing complete (60.0s)\n",
      "\n",
      "Processing Chunk 4, Year 2019...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2019 complete (80.3s)\n",
      "\n",
      "Processing Chunk 4, Year 2020...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2020 complete (82.2s)\n",
      "\n",
      "Processing Chunk 4, Year 2021...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2021 complete (81.3s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 5 geometries (first time)\n",
      "============================================================\n",
      "  Loaded chunk 5: 223 lakes\n",
      "    Bounds: [ 355155.2352059  2148191.99594228  507800.73806888 2271169.77849827]\n",
      "  Simplifying geometries...\n",
      "  Creating EE FeatureCollection...\n",
      "  FeatureCollection created successfully\n",
      "  Adding lake geometry metrics (interior buffers + landscape rings)...\n",
      "  Loading full ALPOD dataset for landscape masking...\n",
      "  CRS Strategy: All geometric operations in Alaska Albers (EPSG:3338)\n",
      "    ALPOD loaded: 801895 lakes, CRS: EPSG:3338\n",
      "    Chunk bounds (WGS84): [-144.87, -141.03] × [69.05, 70.15]\n",
      "    Converting ALPOD from EPSG:3338 to EPSG:4326 for bbox filtering...\n",
      "    Found 2058 total lakes in chunk region (all sizes)\n",
      "    Converting filtered lakes back to Alaska Albers for geometric operations...\n",
      "    Simplified geometries (5m tolerance in Alaska Albers)\n",
      "    Created EE FeatureCollection with 2058 lakes\n",
      "    EE will respect embedded CRS (Alaska Albers EPSG:3338)\n",
      "  Creating lake interior buffers and landscape rings...\n",
      "    Using EPSG:3338 (Alaska Albers) for all geometric operations\n",
      "  Geometric transformations complete\n",
      "    All buffers computed in Alaska Albers (EPSG:3338)\n",
      "    Buffer distances: -10m interior, +100m landscape ring\n",
      "  ✓ Geometries computed in 49.1s\n",
      "  ✓ Chunk 5 geometry processing complete (50.9s)\n",
      "\n",
      "Processing Chunk 5, Year 2019...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2019 complete (19.3s)\n",
      "\n",
      "Processing Chunk 5, Year 2020...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2020 complete (19.2s)\n",
      "\n",
      "Processing Chunk 5, Year 2021...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2021 complete (19.7s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 6 geometries (first time)\n",
      "============================================================\n",
      "  Loaded chunk 6: 1987 lakes\n",
      "    Bounds: [-153255.88771585 2119186.29186161  -95055.74639857 2373312.58665049]\n",
      "  Simplifying geometries...\n",
      "  Creating EE FeatureCollection...\n",
      "  FeatureCollection created successfully\n",
      "  Adding lake geometry metrics (interior buffers + landscape rings)...\n",
      "  Loading full ALPOD dataset for landscape masking...\n",
      "  CRS Strategy: All geometric operations in Alaska Albers (EPSG:3338)\n",
      "    ALPOD loaded: 801895 lakes, CRS: EPSG:3338\n",
      "    Chunk bounds (WGS84): [-157.95, -156.58] × [69.01, 71.35]\n",
      "    Converting ALPOD from EPSG:3338 to EPSG:4326 for bbox filtering...\n",
      "    Found 14646 total lakes in chunk region (all sizes)\n",
      "    Converting filtered lakes back to Alaska Albers for geometric operations...\n",
      "    Simplified geometries (5m tolerance in Alaska Albers)\n",
      "    Created EE FeatureCollection with 14646 lakes\n",
      "    EE will respect embedded CRS (Alaska Albers EPSG:3338)\n",
      "  Creating lake interior buffers and landscape rings...\n",
      "    Using EPSG:3338 (Alaska Albers) for all geometric operations\n",
      "  Geometric transformations complete\n",
      "    All buffers computed in Alaska Albers (EPSG:3338)\n",
      "    Buffer distances: -10m interior, +100m landscape ring\n",
      "  ✓ Geometries computed in 83.1s\n",
      "  ✓ Chunk 6 geometry processing complete (87.2s)\n",
      "\n",
      "Processing Chunk 6, Year 2019...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2019 complete (206.1s)\n",
      "\n",
      "Processing Chunk 6, Year 2020...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2020 complete (206.7s)\n",
      "\n",
      "Processing Chunk 6, Year 2021...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2021 complete (209.1s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 7 geometries (first time)\n",
      "============================================================\n",
      "  Loaded chunk 7: 910 lakes\n",
      "    Bounds: [ 195836.58253925 2122461.07315638  262789.49297168 2275884.59105943]\n",
      "  Simplifying geometries...\n",
      "  Creating EE FeatureCollection...\n",
      "  FeatureCollection created successfully\n",
      "  Adding lake geometry metrics (interior buffers + landscape rings)...\n",
      "  Loading full ALPOD dataset for landscape masking...\n",
      "  CRS Strategy: All geometric operations in Alaska Albers (EPSG:3338)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n",
      "httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ALPOD loaded: 801895 lakes, CRS: EPSG:3338\n",
      "    Chunk bounds (WGS84): [-149.02, -147.39] × [68.99, 70.40]\n",
      "    Converting ALPOD from EPSG:3338 to EPSG:4326 for bbox filtering...\n",
      "    Found 11959 total lakes in chunk region (all sizes)\n",
      "    Converting filtered lakes back to Alaska Albers for geometric operations...\n",
      "    Simplified geometries (5m tolerance in Alaska Albers)\n",
      "    Created EE FeatureCollection with 11959 lakes\n",
      "    EE will respect embedded CRS (Alaska Albers EPSG:3338)\n",
      "  Creating lake interior buffers and landscape rings...\n",
      "    Using EPSG:3338 (Alaska Albers) for all geometric operations\n",
      "  Geometric transformations complete\n",
      "    All buffers computed in Alaska Albers (EPSG:3338)\n",
      "    Buffer distances: -10m interior, +100m landscape ring\n",
      "  ✓ Geometries computed in 66.4s\n",
      "  ✓ Chunk 7 geometry processing complete (69.1s)\n",
      "\n",
      "Processing Chunk 7, Year 2019...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2019 complete (119.0s)\n",
      "\n",
      "Processing Chunk 7, Year 2020...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2020 complete (115.6s)\n",
      "\n",
      "Processing Chunk 7, Year 2021...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2021 complete (117.2s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 8 geometries (first time)\n",
      "============================================================\n",
      "  Loaded chunk 8: 2044 lakes\n",
      "    Bounds: [ -19021.21940238 2116078.98955455   41916.49992922 2324225.00827463]\n",
      "  Simplifying geometries...\n",
      "  Creating EE FeatureCollection...\n",
      "  FeatureCollection created successfully\n",
      "  Adding lake geometry metrics (interior buffers + landscape rings)...\n",
      "  Loading full ALPOD dataset for landscape masking...\n",
      "  CRS Strategy: All geometric operations in Alaska Albers (EPSG:3338)\n",
      "    ALPOD loaded: 801895 lakes, CRS: EPSG:3338\n",
      "    Chunk bounds (WGS84): [-154.48, -152.88] × [69.00, 70.92]\n",
      "    Converting ALPOD from EPSG:3338 to EPSG:4326 for bbox filtering...\n",
      "    Found 13496 total lakes in chunk region (all sizes)\n",
      "    Converting filtered lakes back to Alaska Albers for geometric operations...\n",
      "    Simplified geometries (5m tolerance in Alaska Albers)\n",
      "    Created EE FeatureCollection with 13496 lakes\n",
      "    EE will respect embedded CRS (Alaska Albers EPSG:3338)\n",
      "  Creating lake interior buffers and landscape rings...\n",
      "    Using EPSG:3338 (Alaska Albers) for all geometric operations\n",
      "  Geometric transformations complete\n",
      "    All buffers computed in Alaska Albers (EPSG:3338)\n",
      "    Buffer distances: -10m interior, +100m landscape ring\n",
      "  ✓ Geometries computed in 87.6s\n",
      "  ✓ Chunk 8 geometry processing complete (92.6s)\n",
      "\n",
      "Processing Chunk 8, Year 2019...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2019 complete (234.1s)\n",
      "\n",
      "Processing Chunk 8, Year 2020...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2020 complete (231.4s)\n",
      "\n",
      "Processing Chunk 8, Year 2021...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2021 complete (231.0s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 9 geometries (first time)\n",
      "============================================================\n",
      "  Loaded chunk 9: 1383 lakes\n",
      "    Bounds: [  83750.41124991 2115927.58399985  150900.93208482 2281087.97837676]\n",
      "  Simplifying geometries...\n",
      "  Creating EE FeatureCollection...\n",
      "  FeatureCollection created successfully\n",
      "  Adding lake geometry metrics (interior buffers + landscape rings)...\n",
      "  Loading full ALPOD dataset for landscape masking...\n",
      "  CRS Strategy: All geometric operations in Alaska Albers (EPSG:3338)\n",
      "    ALPOD loaded: 801895 lakes, CRS: EPSG:3338\n",
      "    Chunk bounds (WGS84): [-151.92, -150.22] × [68.99, 70.49]\n",
      "    Converting ALPOD from EPSG:3338 to EPSG:4326 for bbox filtering...\n",
      "    Found 11021 total lakes in chunk region (all sizes)\n",
      "    Converting filtered lakes back to Alaska Albers for geometric operations...\n",
      "    Simplified geometries (5m tolerance in Alaska Albers)\n",
      "    Created EE FeatureCollection with 11021 lakes\n",
      "    EE will respect embedded CRS (Alaska Albers EPSG:3338)\n",
      "  Creating lake interior buffers and landscape rings...\n",
      "    Using EPSG:3338 (Alaska Albers) for all geometric operations\n",
      "  Geometric transformations complete\n",
      "    All buffers computed in Alaska Albers (EPSG:3338)\n",
      "    Buffer distances: -10m interior, +100m landscape ring\n",
      "  ✓ Geometries computed in 70.4s\n",
      "  ✓ Chunk 9 geometry processing complete (73.8s)\n",
      "\n",
      "Processing Chunk 9, Year 2019...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2019 complete (155.2s)\n",
      "\n",
      "Processing Chunk 9, Year 2020...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2020 complete (155.0s)\n",
      "\n",
      "Processing Chunk 9, Year 2021...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2021 complete (153.8s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 10 geometries (first time)\n",
      "============================================================\n",
      "  Loaded chunk 10: 903 lakes\n",
      "    Bounds: [-274858.28466469 2126342.63368411 -201240.42472078 2325136.38129859]\n",
      "  Simplifying geometries...\n",
      "  Creating EE FeatureCollection...\n",
      "  FeatureCollection created successfully\n",
      "  Adding lake geometry metrics (interior buffers + landscape rings)...\n",
      "  Loading full ALPOD dataset for landscape masking...\n",
      "  CRS Strategy: All geometric operations in Alaska Albers (EPSG:3338)\n",
      "    ALPOD loaded: 801895 lakes, CRS: EPSG:3338\n",
      "    Chunk bounds (WGS84): [-161.15, -159.31] × [69.01, 70.85]\n",
      "    Converting ALPOD from EPSG:3338 to EPSG:4326 for bbox filtering...\n",
      "    Found 9390 total lakes in chunk region (all sizes)\n",
      "    Converting filtered lakes back to Alaska Albers for geometric operations...\n",
      "    Simplified geometries (5m tolerance in Alaska Albers)\n",
      "    Created EE FeatureCollection with 9390 lakes\n",
      "    EE will respect embedded CRS (Alaska Albers EPSG:3338)\n",
      "  Creating lake interior buffers and landscape rings...\n",
      "    Using EPSG:3338 (Alaska Albers) for all geometric operations\n",
      "  Geometric transformations complete\n",
      "    All buffers computed in Alaska Albers (EPSG:3338)\n",
      "    Buffer distances: -10m interior, +100m landscape ring\n",
      "  ✓ Geometries computed in 61.6s\n",
      "  ✓ Chunk 10 geometry processing complete (64.1s)\n",
      "\n",
      "Processing Chunk 10, Year 2019...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2019 complete (94.2s)\n",
      "\n",
      "Processing Chunk 10, Year 2020...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2020 complete (94.5s)\n",
      "\n",
      "Processing Chunk 10, Year 2021...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2021 complete (93.7s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 11 geometries (first time)\n",
      "============================================================\n",
      "  Loaded chunk 11: 2148 lakes\n",
      "    Bounds: [-102944.70416456 2148479.48439202  -51511.57732555 2375493.39808125]\n",
      "  Simplifying geometries...\n",
      "  Creating EE FeatureCollection...\n",
      "  FeatureCollection created successfully\n",
      "  Adding lake geometry metrics (interior buffers + landscape rings)...\n",
      "  Loading full ALPOD dataset for landscape masking...\n",
      "  CRS Strategy: All geometric operations in Alaska Albers (EPSG:3338)\n",
      "    ALPOD loaded: 801895 lakes, CRS: EPSG:3338\n",
      "    Chunk bounds (WGS84): [-156.65, -155.37] × [69.29, 71.37]\n",
      "    Converting ALPOD from EPSG:3338 to EPSG:4326 for bbox filtering...\n",
      "    Found 11110 total lakes in chunk region (all sizes)\n",
      "    Converting filtered lakes back to Alaska Albers for geometric operations...\n",
      "    Simplified geometries (5m tolerance in Alaska Albers)\n",
      "    Created EE FeatureCollection with 11110 lakes\n",
      "    EE will respect embedded CRS (Alaska Albers EPSG:3338)\n",
      "  Creating lake interior buffers and landscape rings...\n",
      "    Using EPSG:3338 (Alaska Albers) for all geometric operations\n",
      "  Geometric transformations complete\n",
      "    All buffers computed in Alaska Albers (EPSG:3338)\n",
      "    Buffer distances: -10m interior, +100m landscape ring\n",
      "  ✓ Geometries computed in 81.1s\n",
      "  ✓ Chunk 11 geometry processing complete (85.8s)\n",
      "\n",
      "Processing Chunk 11, Year 2019...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2019 complete (221.0s)\n",
      "\n",
      "Processing Chunk 11, Year 2020...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2020 complete (224.7s)\n",
      "\n",
      "Processing Chunk 11, Year 2021...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2021 complete (220.5s)\n",
      "\n",
      "============================================================\n",
      "Processing Chunk 12 geometries (first time)\n",
      "============================================================\n",
      "  Loaded chunk 12: 452 lakes\n",
      "    Bounds: [ 253817.53535445 2134976.02925771  361518.88086615 2261608.77223793]\n",
      "  Simplifying geometries...\n",
      "  Creating EE FeatureCollection...\n",
      "  FeatureCollection created successfully\n",
      "  Adding lake geometry metrics (interior buffers + landscape rings)...\n",
      "  Loading full ALPOD dataset for landscape masking...\n",
      "  CRS Strategy: All geometric operations in Alaska Albers (EPSG:3338)\n",
      "    ALPOD loaded: 801895 lakes, CRS: EPSG:3338\n",
      "    Chunk bounds (WGS84): [-147.43, -144.89] × [69.02, 70.20]\n",
      "    Converting ALPOD from EPSG:3338 to EPSG:4326 for bbox filtering...\n",
      "    Found 6003 total lakes in chunk region (all sizes)\n",
      "    Converting filtered lakes back to Alaska Albers for geometric operations...\n",
      "    Simplified geometries (5m tolerance in Alaska Albers)\n",
      "    Created EE FeatureCollection with 6003 lakes\n",
      "    EE will respect embedded CRS (Alaska Albers EPSG:3338)\n",
      "  Creating lake interior buffers and landscape rings...\n",
      "    Using EPSG:3338 (Alaska Albers) for all geometric operations\n",
      "  Geometric transformations complete\n",
      "    All buffers computed in Alaska Albers (EPSG:3338)\n",
      "    Buffer distances: -10m interior, +100m landscape ring\n",
      "  ✓ Geometries computed in 53.4s\n",
      "  ✓ Chunk 12 geometry processing complete (55.4s)\n",
      "\n",
      "Processing Chunk 12, Year 2019...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2019 complete (50.2s)\n",
      "\n",
      "Processing Chunk 12, Year 2020...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2020 complete (50.9s)\n",
      "\n",
      "Processing Chunk 12, Year 2021...\n",
      "  Processing Sentinel-1...\n",
      "  ✓ S1 complete\n",
      "  Processing Sentinel-2...\n",
      "  ✓ S2 complete\n",
      "  (ERA5 will be downloaded separately)\n",
      "  ✓ Year 2021 complete (49.6s)\n",
      "\n",
      "============================================================\n",
      "ALL EXPORTS PREPARED: 78 tasks (S1 + S2 only)\n",
      "============================================================\n",
      "Total preparation time: 114.9 minutes\n",
      "Average time per chunk-year: 176.8 seconds\n",
      "\n",
      "Geometry cache stats:\n",
      "  Unique chunks processed: 13\n",
      "  Cache hits (years using cached geometries): 26\n",
      "\n",
      "Note: ERA5 data will be downloaded separately (see Notebook 03)\n",
      "\n",
      "Ready to start. Run next cell to begin 78 exports.\n"
     ]
    }
   ],
   "source": [
    "# Prep S1/S2 exports with geometry caching\n",
    "# ERA5 will be downloaded separately\n",
    "\n",
    "all_exports = []\n",
    "geometry_cache = {}  # Cache fc_with_geom by chunk_id\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "for chunk_id in range(n_chunks):\n",
    "    # ============================================================\n",
    "    # STEP 1: Process geometries ONCE per chunk (not per year)\n",
    "    # ============================================================\n",
    "    if chunk_id not in geometry_cache:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing Chunk {chunk_id} geometries (first time)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        chunk_start = time.time()\n",
    "        \n",
    "        # Load chunk\n",
    "        chunk_fc, chunk_gdf = load_chunk_from_bucket(chunk_id)\n",
    "        chunk_gdf_wgs84 = chunk_gdf.to_crs('EPSG:4326')\n",
    "        chunk_bounds = ee.Geometry.Rectangle(chunk_gdf_wgs84.total_bounds.tolist())\n",
    "        \n",
    "        # Add lake geometries (THE SLOW PART - only do once per chunk!)\n",
    "        print(\"  Adding lake geometry metrics (interior buffers + landscape rings)...\")\n",
    "        geom_start = time.time()\n",
    "        fc_with_geom = add_lake_geometry_metrics(chunk_fc, chunk_bounds)\n",
    "        geom_time = time.time() - geom_start\n",
    "        print(f\"  ✓ Geometries computed in {geom_time:.1f}s\")\n",
    "        \n",
    "        # Cache for reuse across years\n",
    "        geometry_cache[chunk_id] = {\n",
    "            'fc_with_geom': fc_with_geom,\n",
    "            'chunk_bounds': chunk_bounds,\n",
    "            'chunk_gdf': chunk_gdf\n",
    "        }\n",
    "        \n",
    "        chunk_time = time.time() - chunk_start\n",
    "        print(f\"  ✓ Chunk {chunk_id} geometry processing complete ({chunk_time:.1f}s)\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # STEP 2: Process each year using cached geometries\n",
    "    # ============================================================\n",
    "    for year in YEARS:\n",
    "        print(f\"\\nProcessing Chunk {chunk_id}, Year {year}...\")\n",
    "        year_start = time.time()\n",
    "        \n",
    "        # Retrieve cached geometries\n",
    "        cached = geometry_cache[chunk_id]\n",
    "        fc_with_geom = cached['fc_with_geom']\n",
    "        chunk_bounds = cached['chunk_bounds']\n",
    "        \n",
    "        # Process sensors (year-specific data)\n",
    "        print(\"  Processing Sentinel-1...\")\n",
    "        s1_features = process_sentinel1(fc_with_geom, year, chunk_bounds)\n",
    "        print(\"  ✓ S1 complete\")\n",
    "        \n",
    "        print(\"  Processing Sentinel-2...\")\n",
    "        s2_features = process_sentinel2(fc_with_geom, year, chunk_bounds)\n",
    "        print(\"  ✓ S2 complete\")\n",
    "        \n",
    "        print(\"  (ERA5 will be downloaded separately)\")\n",
    "        \n",
    "        # Create export tasks (S1 and S2 only)\n",
    "        s1_task = ee.batch.Export.table.toCloudStorage(\n",
    "            collection=s1_features,\n",
    "            description=f'S1_chunk{chunk_id:02d}_{year}',\n",
    "            bucket=BUCKET,\n",
    "            fileNamePrefix=f'{OUTPUT_PATH}/{year}/chunk_{chunk_id:02d}/s1_data',\n",
    "            fileFormat='CSV'\n",
    "        )\n",
    "        \n",
    "        s2_task = ee.batch.Export.table.toCloudStorage(\n",
    "            collection=s2_features,\n",
    "            description=f'S2_chunk{chunk_id:02d}_{year}',\n",
    "            bucket=BUCKET,\n",
    "            fileNamePrefix=f'{OUTPUT_PATH}/{year}/chunk_{chunk_id:02d}/s2_data',\n",
    "            fileFormat='CSV'\n",
    "        )\n",
    "        \n",
    "        # Store export info (S1 and S2 only)\n",
    "        all_exports.append({\n",
    "            'chunk_id': chunk_id,\n",
    "            'year': year,\n",
    "            'type': 'S1',\n",
    "            'task': s1_task,\n",
    "            'count': 'N/A'\n",
    "        })\n",
    "        all_exports.append({\n",
    "            'chunk_id': chunk_id,\n",
    "            'year': year,\n",
    "            'type': 'S2',\n",
    "            'task': s2_task,\n",
    "            'count': 'N/A'\n",
    "        })\n",
    "        \n",
    "        year_time = time.time() - year_start\n",
    "        print(f\"  ✓ Year {year} complete ({year_time:.1f}s)\")\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ALL EXPORTS PREPARED: {len(all_exports)} tasks (S1 + S2 only)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total preparation time: {total_time/60:.1f} minutes\")\n",
    "print(f\"Average time per chunk-year: {total_time/39:.1f} seconds\")\n",
    "print(f\"\\nGeometry cache stats:\")\n",
    "print(f\"  Unique chunks processed: {len(geometry_cache)}\")\n",
    "print(f\"  Cache hits (years using cached geometries): {39 - len(geometry_cache)}\")\n",
    "print(f\"\\nNote: ERA5 data will be downloaded separately (see Notebook 03)\")\n",
    "print(f\"\\nReady to start. Run next cell to begin {len(all_exports)} exports.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Monitor GEE Export Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check status of GEE exports\n",
    "def check_export_status():\n",
    "    status_summary = {\n",
    "        'READY': 0,\n",
    "        'RUNNING': 0,\n",
    "        'COMPLETED': 0,\n",
    "        'FAILED': 0,\n",
    "        'CANCELLED': 0\n",
    "    }\n",
    "    \n",
    "    for exp in all_exports:\n",
    "        status = exp['task'].status()['state']\n",
    "        status_summary[status] = status_summary.get(status, 0) + 1\n",
    "    \n",
    "    print(f\"Export Status Summary:\")\n",
    "    print(f\"  Total tasks: {len(all_exports)}\")\n",
    "    for state, count in status_summary.items():\n",
    "        if count > 0:\n",
    "            print(f\"    {state}: {count}\")\n",
    "    \n",
    "    return status_summary\n",
    "\n",
    "# Run this cell periodically to check progress\n",
    "check_export_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERA5 Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Wait for GEE Exports & Download ERA5\n",
    "# ============================================================\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WAITING FOR GEE EXPORTS TO COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total tasks: {len(all_exports)} (S1 + S2)\")\n",
    "print(\"Monitor progress at: https://code.earthengine.google.com/tasks\")\n",
    "print(\"\\nThis cell will poll GEE every 5 minutes and automatically\")\n",
    "print(\"download ERA5 data when all exports complete.\")\n",
    "print(\"\")\n",
    "\n",
    "# ============================================================\n",
    "# Poll GEE Export Status\n",
    "# ============================================================\n",
    "\n",
    "def check_export_status():\n",
    "    \"\"\"Check status of all export tasks\"\"\"\n",
    "    statuses = {'COMPLETED': 0, 'RUNNING': 0, 'READY': 0, 'FAILED': 0, 'CANCELLED': 0}\n",
    "    for exp in all_exports:\n",
    "        state = exp['task'].status()['state']\n",
    "        statuses[state] = statuses.get(state, 0) + 1\n",
    "    return statuses\n",
    "\n",
    "print(\"Checking export status...\\n\")\n",
    "\n",
    "poll_count = 0\n",
    "while True:\n",
    "    statuses = check_export_status()\n",
    "    completed = statuses['COMPLETED']\n",
    "    failed = statuses['FAILED']\n",
    "    running = statuses['RUNNING']\n",
    "    total = len(all_exports)\n",
    "    \n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] Status: {completed}/{total} complete, {running} running, {failed} failed\")\n",
    "    \n",
    "    if completed + failed == total:\n",
    "        print(\"\\n✓ All GEE tasks finished!\")\n",
    "        break\n",
    "    \n",
    "    poll_count += 1\n",
    "    if poll_count == 1:\n",
    "        print(\"(Polling every 5 minutes...)\\n\")\n",
    "    \n",
    "    time.sleep(300)\n",
    "\n",
    "# Check for failures\n",
    "failed_tasks = [exp for exp in all_exports if exp['task'].status()['state'] == 'FAILED']\n",
    "\n",
    "if failed_tasks:\n",
    "    print(f\"\\n⚠️  {len(failed_tasks)} tasks FAILED:\")\n",
    "    for exp in failed_tasks[:10]:\n",
    "        status = exp['task'].status()\n",
    "        error = status.get('error_message', 'Unknown error')\n",
    "        print(f\"  - Chunk {exp['chunk_id']}, {exp['year']}, {exp['type']}: {error}\")\n",
    "    if len(failed_tasks) > 10:\n",
    "        print(f\"  ... and {len(failed_tasks)-10} more\")\n",
    "    print(\"\\n❌ Cannot proceed to ERA5 download with failed tasks\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n✓ All S1/S2 exports succeeded!\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DOWNLOADING ERA5 TEMPERATURE DATA\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"This will take 5-10 minutes for all chunks and years...\")\n",
    "    print(\"\")\n",
    "    \n",
    "    era5_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Load chunk info\n",
    "        print(\"Step 1: Loading lake locations by chunk...\")\n",
    "        chunks_info = []\n",
    "        \n",
    "        for chunk_id in range(n_chunks):\n",
    "            chunk_file = f'{CHUNKS_PATH}/chunk_{chunk_id:02d}.geojson'\n",
    "            chunk_gdf = gpd.read_file(chunk_file)\n",
    "            chunk_gdf_wgs84 = chunk_gdf.to_crs('EPSG:4326')\n",
    "            chunk_gdf_wgs84['centroid_lon'] = chunk_gdf_wgs84.centroid.x\n",
    "            chunk_gdf_wgs84['centroid_lat'] = chunk_gdf_wgs84.centroid.y\n",
    "            chunk_gdf_wgs84['lake_id'] = range(len(chunk_gdf_wgs84))\n",
    "            \n",
    "            chunks_info.append({\n",
    "                'chunk_id': chunk_id,\n",
    "                'gdf': chunk_gdf_wgs84,\n",
    "                'n_lakes': len(chunk_gdf_wgs84)\n",
    "            })\n",
    "        \n",
    "        total_lakes = sum(c['n_lakes'] for c in chunks_info)\n",
    "        print(f\"  ✓ Loaded {total_lakes:,} lakes across {n_chunks} chunks\\n\")\n",
    "        \n",
    "        # Download ERA5\n",
    "        print(\"Step 2: Accessing ERA5-Land dataset...\")\n",
    "        ERA5_URL = 'gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3'\n",
    "        ds = xr.open_zarr(ERA5_URL, chunks='auto', consolidated=True)\n",
    "        temp_var = '2m_temperature' if '2m_temperature' in ds else 't2m'\n",
    "        print(f\"  ✓ Dataset opened\\n\")\n",
    "        \n",
    "        print(\"Step 3: Selecting North Slope region and time period...\")\n",
    "        ds_subset = ds[temp_var].sel(\n",
    "            latitude=slice(72, 69),\n",
    "            longitude=slice(-164, -140),\n",
    "            time=slice(f'{YEARS[0]}-01-01', f'{YEARS[-1]}-12-31T23:59:59')\n",
    "        )\n",
    "        print(f\"  ✓ Selected: {len(ds_subset.latitude)} lats × {len(ds_subset.longitude)} lons, {len(ds_subset.time)} hours\\n\")\n",
    "        \n",
    "        print(\"Step 4: Computing daily means (2-3 minutes)...\")\n",
    "        ds_daily = ds_subset.resample(time='1D').mean()\n",
    "        ds_daily_celsius = ds_daily - 273.15\n",
    "        print(f\"  ✓ {len(ds_daily_celsius.time)} daily means computed\\n\")\n",
    "        \n",
    "        print(\"Step 5: Processing each chunk and exporting...\")\n",
    "        \n",
    "        for chunk_info in chunks_info:\n",
    "            chunk_id = chunk_info['chunk_id']\n",
    "            chunk_gdf = chunk_info['gdf']\n",
    "            n_lakes = chunk_info['n_lakes']\n",
    "            \n",
    "            print(f\"  Chunk {chunk_id:02d} ({n_lakes} lakes)...\", end=' ')\n",
    "            \n",
    "            # Interpolate\n",
    "            lake_lats = xr.DataArray(chunk_gdf['centroid_lat'].values, dims=['lake'])\n",
    "            lake_lons = xr.DataArray(chunk_gdf['centroid_lon'].values, dims=['lake'])\n",
    "            lake_temps = ds_daily_celsius.interp(latitude=lake_lats, longitude=lake_lons, method='linear')\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df_chunk = lake_temps.to_dataframe(name='temp_c').reset_index()\n",
    "            df_chunk['lake_id'] = df_chunk['lake']\n",
    "            df_chunk['era5_date'] = df_chunk['time'].dt.strftime('%Y-%m-%d')\n",
    "            df_chunk['era5_doy'] = df_chunk['time'].dt.dayofyear\n",
    "            df_chunk['year'] = df_chunk['time'].dt.year\n",
    "            df_chunk = df_chunk[['lake_id', 'era5_date', 'era5_doy', 'temp_c', 'year']]\n",
    "            \n",
    "            # Export by year\n",
    "            for year in YEARS:\n",
    "                df_year = df_chunk[df_chunk['year'] == year].copy()\n",
    "                df_year = df_year.drop(columns=['year']).sort_values(['lake_id', 'era5_doy'])\n",
    "                output_file = f'{OUTPUT_PATH}/{year}/chunk_{chunk_id:02d}/era5_data.csv'\n",
    "                df_year.to_csv(output_file, index=False)\n",
    "            \n",
    "            print(f\"✓\")\n",
    "        \n",
    "        era5_time = time.time() - era5_start\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"ERA5 DOWNLOAD COMPLETE!\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Time: {era5_time/60:.1f} minutes\")\n",
    "        print(f\"Files: {n_chunks * len(YEARS)} CSV files created\")\n",
    "        \n",
    "        # Validation\n",
    "        print(f\"\\nValidation check:\")\n",
    "        test_file = f'{OUTPUT_PATH}/{YEARS[0]}/chunk_00/era5_data.csv'\n",
    "        df_test = pd.read_csv(test_file)\n",
    "        print(f\"  Chunk 00, {YEARS[0]}: {len(df_test):,} rows\")\n",
    "        print(f\"  Lakes: {df_test['lake_id'].nunique()}, Temp range: {df_test['temp_c'].min():.1f}°C to {df_test['temp_c'].max():.1f}°C\")\n",
    "        print(f\"  ✓ Data looks good!\\n\")\n",
    "        \n",
    "        # Final summary\n",
    "        total_time = (time.time() - total_start) / 60\n",
    "        print(f\"{'='*60}\")\n",
    "        print(\"ALL EXPORTS COMPLETE!\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total time: {total_time:.1f} minutes\")\n",
    "        print(f\"  GEE exports: ~{total_time - era5_time/60:.0f} min\")\n",
    "        print(f\"  ERA5 download: {era5_time/60:.1f} min\")\n",
    "        print(f\"\\nData location: {OUTPUT_PATH}/[YEAR]/chunk_[XX]/\")\n",
    "        print(f\"  - s1_data.csv\")\n",
    "        print(f\"  - s2_data.csv\")\n",
    "        print(f\"  - era5_data.csv\")\n",
    "        print(f\"\\n✓ Ready for analysis!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ ERROR: {e}\")\n",
    "        print(\"\\nYou can download ERA5 separately using Notebook 03\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook exports:\n",
    "- **Sentinel-1**: VV/VH backscatter for each lake interior\n",
    "- **Sentinel-2**: NDSI-based ice fraction (with cloud filtering)\n",
    "- **ERA5**: Daily mean temperature at lake locations\n",
    "\n",
    "**For:**\n",
    "- ~19 spatial chunks\n",
    "- 3 years (2019, 2020, 2021)\n",
    "- ~28,000 North Slope lakes\n",
    "\n",
    "**Output structure:**\n",
    "```\n",
    "gs://wustl-eeps-geospatial/thermokarst_lakes/exports/\n",
    "├── 2019/\n",
    "│   ├── chunk_00/\n",
    "│   │   ├── s1_data.csv\n",
    "│   │   ├── s2_data.csv\n",
    "│   │   └── era5_data.csv\n",
    "│   ├── chunk_01/\n",
    "│   └── ...\n",
    "├── 2020/\n",
    "└── 2021/\n",
    "```\n",
    "\n",
    "**Next step:** Combine CSVs and run ice detection algorithm (Notebook 03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "DataSourceError",
     "evalue": "HTTP response code: 404",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataSourceError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the ALPOD shapefile\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m alpod_gdf \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgs://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mBUCKET\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mBASE_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/source/ALPODlakes.shp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Convert to GEE FeatureCollection\u001b[39;00m\n\u001b[1;32m      5\u001b[0m alpod_fc \u001b[38;5;241m=\u001b[39m ee\u001b[38;5;241m.\u001b[39mFeatureCollection(alpod_gdf\u001b[38;5;241m.\u001b[39m__geo_interface__)\n",
      "File \u001b[0;32m/opt/conda/envs/gee/lib/python3.10/site-packages/geopandas/io/file.py:316\u001b[0m, in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, columns, rows, engine, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m             filename \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyogrio\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_pyogrio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiona\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_file_like(filename):\n",
      "File \u001b[0;32m/opt/conda/envs/gee/lib/python3.10/site-packages/geopandas/io/file.py:576\u001b[0m, in \u001b[0;36m_read_file_pyogrio\u001b[0;34m(path_or_bytes, bbox, mask, rows, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    568\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minclude_fields\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore_fields\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keywords are deprecated, and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future release. You can use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keyword \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    572\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    573\u001b[0m     )\n\u001b[1;32m    574\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_fields\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyogrio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/gee/lib/python3.10/site-packages/pyogrio/geopandas.py:275\u001b[0m, in \u001b[0;36mread_dataframe\u001b[0;34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, fid_as_index, use_arrow, on_invalid, arrow_to_pandas_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_arrow:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;66;03m# For arrow, datetimes are read as is.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# For numpy IO, datetimes are read as string values to preserve timezone info\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m# as numpy does not directly support timezones.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime_as_string\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mread_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgdal_force_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfid_as_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_arrow:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpa\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/gee/lib/python3.10/site-packages/pyogrio/raw.py:198\u001b[0m, in \u001b[0;36mread\u001b[0;34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, return_fids, datetime_as_string, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Read OGR data source into numpy arrays.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03mIMPORTANT: non-linear geometry types (e.g., MultiSurface) are converted\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m dataset_kwargs \u001b[38;5;241m=\u001b[39m _preprocess_options_key_value(kwargs) \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m--> 198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mogr_read\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_vsi_path_or_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_mask_to_wkb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_fids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatetime_as_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatetime_as_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mpyogrio/_io.pyx:1293\u001b[0m, in \u001b[0;36mpyogrio._io.ogr_read\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpyogrio/_io.pyx:232\u001b[0m, in \u001b[0;36mpyogrio._io.ogr_open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDataSourceError\u001b[0m: HTTP response code: 404"
     ]
    }
   ],
   "source": [
    "# Load the ALPOD shapefile\n",
    "alpod_gdf = gpd.read_file(f'gs://{BUCKET}/{BASE_PATH}/source/ALPODlakes.shp')\n",
    "\n",
    "# Convert to GEE FeatureCollection\n",
    "alpod_fc = ee.FeatureCollection(alpod_gdf.__geo_interface__)\n",
    "\n",
    "# Export as GEE Asset\n",
    "task = ee.batch.Export.table.toAsset(\n",
    "    collection=alpod_fc,\n",
    "    description='ALPOD_full_upload',\n",
    "    assetId='projects/eeps-geospatial/assets/ALPOD_full'  # Adjust project name\n",
    ")\n",
    "task.start()\n",
    "\n",
    "print(\"Asset upload started!\")\n",
    "print(\"Check progress: https://code.earthengine.google.com/tasks\")\n",
    "print(\"This may take 30-60 minutes for 800k lakes...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "gee",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "GEE Geospatial (Local)",
   "language": "python",
   "name": "gee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
